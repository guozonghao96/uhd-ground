nohup: ignoring input
./checkpoints_new/llava-uhd-144-13b-loc-unpad-fullft
[2024-09-01 01:11:01,595] torch.distributed.run: [WARNING] 
[2024-09-01 01:11:01,595] torch.distributed.run: [WARNING] *****************************************
[2024-09-01 01:11:01,595] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-09-01 01:11:01,595] torch.distributed.run: [WARNING] *****************************************
[2024-09-01 01:11:48,080] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-01 01:11:48,080] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-01 01:11:48,080] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-01 01:11:48,081] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-01 01:11:48,081] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-01 01:11:48,082] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-01 01:11:48,091] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-01 01:11:48,106] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-01 01:11:50,170] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-01 01:11:50,170] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-01 01:11:50,171] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-01 01:11:50,171] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-01 01:11:50,171] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-01 01:11:50,172] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-01 01:11:50,172] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-09-01 01:11:50,172] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-01 01:11:50,174] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  33%|███▎      | 1/3 [00:25<00:51, 25.77s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:25<00:51, 25.80s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:25<00:51, 25.80s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:25<00:51, 25.83s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:27<00:54, 27.04s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:26<00:53, 26.91s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:26<00:53, 26.97s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:27<00:54, 27.13s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:22<01:19, 79.43s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:22<01:19, 79.49s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:22<01:19, 79.33s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:22<01:19, 79.54s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:22<01:19, 79.54s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:23<01:19, 79.61s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:22<01:19, 79.33s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:23<01:19, 79.37s/it]Loading checkpoint shards: 100%|██████████| 3/3 [03:12<00:00, 65.83s/it]Loading checkpoint shards: 100%|██████████| 3/3 [03:12<00:00, 64.25s/it]
---------init adapt_vision_model---------
Loading checkpoint shards: 100%|██████████| 3/3 [03:12<00:00, 66.00s/it]Loading checkpoint shards: 100%|██████████| 3/3 [03:12<00:00, 64.28s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [03:12<00:00, 66.02s/it]Loading checkpoint shards: 100%|██████████| 3/3 [03:12<00:00, 64.29s/it]
---------init adapt_vision_model---------
Loading checkpoint shards: 100%|██████████| 3/3 [03:12<00:00, 65.92s/it]Loading checkpoint shards: 100%|██████████| 3/3 [03:12<00:00, 64.30s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [03:13<00:00, 66.04s/it]Loading checkpoint shards: 100%|██████████| 3/3 [03:13<00:00, 64.45s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [03:12<00:00, 66.05s/it]Loading checkpoint shards: 100%|██████████| 3/3 [03:12<00:00, 64.30s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [03:12<00:00, 65.88s/it]Loading checkpoint shards: 100%|██████████| 3/3 [03:12<00:00, 64.29s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [03:12<00:00, 66.02s/it]Loading checkpoint shards: 100%|██████████| 3/3 [03:12<00:00, 64.29s/it]
---------init adapt_vision_model---------
---------init adapt_vision_model---------
---------init adapt_vision_model---------
---------init adapt_vision_model---------
---------init adapt_vision_model---------
---------init adapt_vision_model---------
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
layers.12.self_attn.q_proj.weight
layers.12.self_attn.k_proj.weight
layers.12.self_attn.v_proj.weight
layers.12.self_attn.o_proj.weight
layers.12.mlp.gate_proj.weight
layers.12.mlp.up_proj.weight
layers.12.mlp.down_proj.weight
layers.12.input_layernorm.weight
layers.12.post_attention_layernorm.weight
layers.13.self_attn.q_proj.weight
layers.13.self_attn.k_proj.weight
layers.13.self_attn.v_proj.weight
layers.13.self_attn.o_proj.weight
layers.13.mlp.gate_proj.weight
layers.13.mlp.up_proj.weight
layers.13.mlp.down_proj.weight
layers.13.input_layernorm.weight
layers.13.post_attention_layernorm.weight
layers.14.self_attn.q_proj.weight
layers.14.self_attn.k_proj.weight
layers.14.self_attn.v_proj.weight
layers.14.self_attn.o_proj.weight
layers.14.mlp.gate_proj.weight
layers.14.mlp.up_proj.weight
layers.14.mlp.down_proj.weight
layers.14.input_layernorm.weight
layers.14.post_attention_layernorm.weight
layers.15.self_attn.q_proj.weight
layers.15.self_attn.k_proj.weight
layers.15.self_attn.v_proj.weight
layers.15.self_attn.o_proj.weight
layers.15.mlp.gate_proj.weight
layers.15.mlp.up_proj.weight
layers.15.mlp.down_proj.weight
layers.15.input_layernorm.weight
layers.15.post_attention_layernorm.weight
layers.16.self_attn.q_proj.weight
layers.16.self_attn.k_proj.weight
layers.16.self_attn.v_proj.weight
layers.16.self_attn.o_proj.weight
layers.16.mlp.gate_proj.weight
layers.16.mlp.up_proj.weight
layers.16.mlp.down_proj.weight
layers.16.input_layernorm.weight
layers.16.post_attention_layernorm.weight
layers.17.self_attn.q_proj.weight
layers.17.self_attn.k_proj.weight
layers.17.self_attn.v_proj.weight
layers.17.self_attn.o_proj.weight
layers.17.mlp.gate_proj.weight
layers.17.mlp.up_proj.weight
layers.17.mlp.down_proj.weight
layers.17.input_layernorm.weight
layers.17.post_attention_layernorm.weight
layers.18.self_attn.q_proj.weight
layers.18.self_attn.k_proj.weight
layers.18.self_attn.v_proj.weight
layers.18.self_attn.o_proj.weight
layers.18.mlp.gate_proj.weight
layers.18.mlp.up_proj.weight
layers.18.mlp.down_proj.weight
layers.18.input_layernorm.weight
layers.18.post_attention_layernorm.weight
layers.19.self_attn.q_proj.weight
layers.19.self_attn.k_proj.weight
layers.19.self_attn.v_proj.weight
layers.19.self_attn.o_proj.weight
layers.19.mlp.gate_proj.weight
layers.19.mlp.up_proj.weight
layers.19.mlp.down_proj.weight
layers.19.input_layernorm.weight
layers.19.post_attention_layernorm.weight
layers.20.self_attn.q_proj.weight
layers.20.self_attn.k_proj.weight
layers.20.self_attn.v_proj.weight
layers.20.self_attn.o_proj.weight
layers.20.mlp.gate_proj.weight
layers.20.mlp.up_proj.weight
layers.20.mlp.down_proj.weight
layers.20.input_layernorm.weight
layers.20.post_attention_layernorm.weight
layers.21.self_attn.q_proj.weight
layers.21.self_attn.k_proj.weight
layers.21.self_attn.v_proj.weight
layers.21.self_attn.o_proj.weight
layers.21.mlp.gate_proj.weight
layers.21.mlp.up_proj.weight
layers.21.mlp.down_proj.weight
layers.21.input_layernorm.weight
layers.21.post_attention_layernorm.weight
layers.22.self_attn.q_proj.weight
layers.22.self_attn.k_proj.weight
layers.22.self_attn.v_proj.weight
layers.22.self_attn.o_proj.weight
layers.22.mlp.gate_proj.weight
layers.22.mlp.up_proj.weight
layers.22.mlp.down_proj.weight
layers.22.input_layernorm.weight
layers.22.post_attention_layernorm.weight
layers.23.self_attn.q_proj.weight
layers.23.self_attn.k_proj.weight
layers.23.self_attn.v_proj.weight
layers.23.self_attn.o_proj.weight
layers.23.mlp.gate_proj.weight
layers.23.mlp.up_proj.weight
layers.23.mlp.down_proj.weight
layers.23.input_layernorm.weight
layers.23.post_attention_layernorm.weight
layers.24.self_attn.q_proj.weight
layers.24.self_attn.k_proj.weight
layers.24.self_attn.v_proj.weight
layers.24.self_attn.o_proj.weight
layers.24.mlp.gate_proj.weight
layers.24.mlp.up_proj.weight
layers.24.mlp.down_proj.weight
layers.24.input_layernorm.weight
layers.24.post_attention_layernorm.weight
layers.25.self_attn.q_proj.weight
layers.25.self_attn.k_proj.weight
layers.25.self_attn.v_proj.weight
layers.25.self_attn.o_proj.weight
layers.25.mlp.gate_proj.weight
layers.25.mlp.up_proj.weight
layers.25.mlp.down_proj.weight
layers.25.input_layernorm.weight
layers.25.post_attention_layernorm.weight
layers.26.self_attn.q_proj.weight
layers.26.self_attn.k_proj.weight
layers.26.self_attn.v_proj.weight
layers.26.self_attn.o_proj.weight
layers.26.mlp.gate_proj.weight
layers.26.mlp.up_proj.weight
layers.26.mlp.down_proj.weight
layers.26.input_layernorm.weight
layers.26.post_attention_layernorm.weight
layers.27.self_attn.q_proj.weight
layers.27.self_attn.k_proj.weight
layers.27.self_attn.v_proj.weight
layers.27.self_attn.o_proj.weight
layers.27.mlp.gate_proj.weight
layers.27.mlp.up_proj.weight
layers.27.mlp.down_proj.weight
layers.27.input_layernorm.weight
layers.27.post_attention_layernorm.weight
layers.28.self_attn.q_proj.weight
layers.28.self_attn.k_proj.weight
layers.28.self_attn.v_proj.weight
layers.28.self_attn.o_proj.weight
layers.28.mlp.gate_proj.weight
layers.28.mlp.up_proj.weight
layers.28.mlp.down_proj.weight
layers.28.input_layernorm.weight
layers.28.post_attention_layernorm.weight
layers.29.self_attn.q_proj.weight
layers.29.self_attn.k_proj.weight
layers.29.self_attn.v_proj.weight
layers.29.self_attn.o_proj.weight
layers.29.mlp.gate_proj.weight
layers.29.mlp.up_proj.weight
layers.29.mlp.down_proj.weight
layers.29.input_layernorm.weight
layers.29.post_attention_layernorm.weight
layers.30.self_attn.q_proj.weight
layers.30.self_attn.k_proj.weight
layers.30.self_attn.v_proj.weight
layers.30.self_attn.o_proj.weight
layers.30.mlp.gate_proj.weight
layers.30.mlp.up_proj.weight
layers.30.mlp.down_proj.weight
layers.30.input_layernorm.weight
layers.30.post_attention_layernorm.weight
layers.31.self_attn.q_proj.weight
layers.31.self_attn.k_proj.weight
layers.31.self_attn.v_proj.weight
layers.31.self_attn.o_proj.weight
layers.31.mlp.gate_proj.weight
layers.31.mlp.up_proj.weight
layers.31.mlp.down_proj.weight
layers.31.input_layernorm.weight
layers.31.post_attention_layernorm.weight
layers.32.self_attn.q_proj.weight
layers.32.self_attn.k_proj.weight
layers.32.self_attn.v_proj.weight
layers.32.self_attn.o_proj.weight
layers.32.mlp.gate_proj.weight
layers.32.mlp.up_proj.weight
layers.32.mlp.down_proj.weight
layers.32.input_layernorm.weight
layers.32.post_attention_layernorm.weight
layers.33.self_attn.q_proj.weight
layers.33.self_attn.k_proj.weight
layers.33.self_attn.v_proj.weight
layers.33.self_attn.o_proj.weight
layers.33.mlp.gate_proj.weight
layers.33.mlp.up_proj.weight
layers.33.mlp.down_proj.weight
layers.33.input_layernorm.weight
layers.33.post_attention_layernorm.weight
layers.34.self_attn.q_proj.weight
layers.34.self_attn.k_proj.weight
layers.34.self_attn.v_proj.weight
layers.34.self_attn.o_proj.weight
layers.34.mlp.gate_proj.weight
layers.34.mlp.up_proj.weight
layers.34.mlp.down_proj.weight
layers.34.input_layernorm.weight
layers.34.post_attention_layernorm.weight
layers.35.self_attn.q_proj.weight
layers.35.self_attn.k_proj.weight
layers.35.self_attn.v_proj.weight
layers.35.self_attn.o_proj.weight
layers.35.mlp.gate_proj.weight
layers.35.mlp.up_proj.weight
layers.35.mlp.down_proj.weight
layers.35.input_layernorm.weight
layers.35.post_attention_layernorm.weight
layers.36.self_attn.q_proj.weight
layers.36.self_attn.k_proj.weight
layers.36.self_attn.v_proj.weight
layers.36.self_attn.o_proj.weight
layers.36.mlp.gate_proj.weight
layers.36.mlp.up_proj.weight
layers.36.mlp.down_proj.weight
layers.36.input_layernorm.weight
layers.36.post_attention_layernorm.weight
layers.37.self_attn.q_proj.weight
layers.37.self_attn.k_proj.weight
layers.37.self_attn.v_proj.weight
layers.37.self_attn.o_proj.weight
layers.37.mlp.gate_proj.weight
layers.37.mlp.up_proj.weight
layers.37.mlp.down_proj.weight
layers.37.input_layernorm.weight
layers.37.post_attention_layernorm.weight
layers.38.self_attn.q_proj.weight
layers.38.self_attn.k_proj.weight
layers.38.self_attn.v_proj.weight
layers.38.self_attn.o_proj.weight
layers.38.mlp.gate_proj.weight
layers.38.mlp.up_proj.weight
layers.38.mlp.down_proj.weight
layers.38.input_layernorm.weight
layers.38.post_attention_layernorm.weight
layers.39.self_attn.q_proj.weight
layers.39.self_attn.k_proj.weight
layers.39.self_attn.v_proj.weight
layers.39.self_attn.o_proj.weight
layers.39.mlp.gate_proj.weight
layers.39.mlp.up_proj.weight
layers.39.mlp.down_proj.weight
layers.39.input_layernorm.weight
layers.39.post_attention_layernorm.weight
norm.weight
vision_tower.vision_tower.vision_model.embeddings.class_embedding
vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight
vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight
vision_tower.vision_tower.vision_model.pre_layrnorm.weight
vision_tower.vision_tower.vision_model.pre_layrnorm.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias
vision_tower.vision_tower.vision_model.post_layernorm.weight
vision_tower.vision_tower.vision_model.post_layernorm.bias
mm_projector.query
mm_projector.proj
mm_projector.kv_proj.weight
mm_projector.attn.in_proj_weight
mm_projector.attn.in_proj_bias
mm_projector.attn.out_proj.weight
mm_projector.attn.out_proj.bias
mm_projector.ln_q.weight
mm_projector.ln_q.bias
mm_projector.ln_kv.weight
mm_projector.ln_kv.bias
mm_projector.ln_post.weight
mm_projector.ln_post.bias
lm_head.weight
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
layers.12.self_attn.q_proj.weight
layers.12.self_attn.k_proj.weight
layers.12.self_attn.v_proj.weight
layers.12.self_attn.o_proj.weight
layers.12.mlp.gate_proj.weight
layers.12.mlp.up_proj.weight
layers.12.mlp.down_proj.weight
layers.12.input_layernorm.weight
layers.12.post_attention_layernorm.weight
layers.13.self_attn.q_proj.weight
layers.13.self_attn.k_proj.weight
layers.13.self_attn.v_proj.weight
layers.13.self_attn.o_proj.weight
layers.13.mlp.gate_proj.weight
layers.13.mlp.up_proj.weight
layers.13.mlp.down_proj.weight
layers.13.input_layernorm.weight
layers.13.post_attention_layernorm.weight
layers.14.self_attn.q_proj.weight
layers.14.self_attn.k_proj.weight
layers.14.self_attn.v_proj.weight
layers.14.self_attn.o_proj.weight
layers.14.mlp.gate_proj.weight
layers.14.mlp.up_proj.weight
layers.14.mlp.down_proj.weight
layers.14.input_layernorm.weight
layers.14.post_attention_layernorm.weight
layers.15.self_attn.q_proj.weight
layers.15.self_attn.k_proj.weight
layers.15.self_attn.v_proj.weight
layers.15.self_attn.o_proj.weight
layers.15.mlp.gate_proj.weight
layers.15.mlp.up_proj.weight
layers.15.mlp.down_proj.weight
layers.15.input_layernorm.weight
layers.15.post_attention_layernorm.weight
layers.16.self_attn.q_proj.weight
layers.16.self_attn.k_proj.weight
layers.16.self_attn.v_proj.weight
layers.16.self_attn.o_proj.weight
layers.16.mlp.gate_proj.weight
layers.16.mlp.up_proj.weight
layers.16.mlp.down_proj.weight
layers.16.input_layernorm.weight
layers.16.post_attention_layernorm.weight
layers.17.self_attn.q_proj.weight
layers.17.self_attn.k_proj.weight
layers.17.self_attn.v_proj.weight
layers.17.self_attn.o_proj.weight
layers.17.mlp.gate_proj.weight
layers.17.mlp.up_proj.weight
layers.17.mlp.down_proj.weight
layers.17.input_layernorm.weight
layers.17.post_attention_layernorm.weight
layers.18.self_attn.q_proj.weight
layers.18.self_attn.k_proj.weight
layers.18.self_attn.v_proj.weight
layers.18.self_attn.o_proj.weight
layers.18.mlp.gate_proj.weight
layers.18.mlp.up_proj.weight
layers.18.mlp.down_proj.weight
layers.18.input_layernorm.weight
layers.18.post_attention_layernorm.weight
layers.19.self_attn.q_proj.weight
layers.19.self_attn.k_proj.weight
layers.19.self_attn.v_proj.weight
layers.19.self_attn.o_proj.weight
layers.19.mlp.gate_proj.weight
layers.19.mlp.up_proj.weight
layers.19.mlp.down_proj.weight
layers.19.input_layernorm.weight
layers.19.post_attention_layernorm.weight
layers.20.self_attn.q_proj.weight
layers.20.self_attn.k_proj.weight
layers.20.self_attn.v_proj.weight
layers.20.self_attn.o_proj.weight
layers.20.mlp.gate_proj.weight
layers.20.mlp.up_proj.weight
layers.20.mlp.down_proj.weight
layers.20.input_layernorm.weight
layers.20.post_attention_layernorm.weight
layers.21.self_attn.q_proj.weight
layers.21.self_attn.k_proj.weight
layers.21.self_attn.v_proj.weight
layers.21.self_attn.o_proj.weight
layers.21.mlp.gate_proj.weight
layers.21.mlp.up_proj.weight
layers.21.mlp.down_proj.weight
layers.21.input_layernorm.weight
layers.21.post_attention_layernorm.weight
layers.22.self_attn.q_proj.weight
layers.22.self_attn.k_proj.weight
layers.22.self_attn.v_proj.weight
layers.22.self_attn.o_proj.weight
layers.22.mlp.gate_proj.weight
layers.22.mlp.up_proj.weight
layers.22.mlp.down_proj.weight
layers.22.input_layernorm.weight
layers.22.post_attention_layernorm.weight
layers.23.self_attn.q_proj.weight
layers.23.self_attn.k_proj.weight
layers.23.self_attn.v_proj.weight
layers.23.self_attn.o_proj.weight
layers.23.mlp.gate_proj.weight
layers.23.mlp.up_proj.weight
layers.23.mlp.down_proj.weight
layers.23.input_layernorm.weight
layers.23.post_attention_layernorm.weight
layers.24.self_attn.q_proj.weight
layers.24.self_attn.k_proj.weight
layers.24.self_attn.v_proj.weight
layers.24.self_attn.o_proj.weight
layers.24.mlp.gate_proj.weight
layers.24.mlp.up_proj.weight
layers.24.mlp.down_proj.weight
layers.24.input_layernorm.weight
layers.24.post_attention_layernorm.weight
layers.25.self_attn.q_proj.weight
layers.25.self_attn.k_proj.weight
layers.25.self_attn.v_proj.weight
layers.25.self_attn.o_proj.weight
layers.25.mlp.gate_proj.weight
layers.25.mlp.up_proj.weight
layers.25.mlp.down_proj.weight
layers.25.input_layernorm.weight
layers.25.post_attention_layernorm.weight
layers.26.self_attn.q_proj.weight
layers.26.self_attn.k_proj.weight
layers.26.self_attn.v_proj.weight
layers.26.self_attn.o_proj.weight
layers.26.mlp.gate_proj.weight
layers.26.mlp.up_proj.weight
layers.26.mlp.down_proj.weight
layers.26.input_layernorm.weight
layers.26.post_attention_layernorm.weight
layers.27.self_attn.q_proj.weight
layers.27.self_attn.k_proj.weight
layers.27.self_attn.v_proj.weight
layers.27.self_attn.o_proj.weight
layers.27.mlp.gate_proj.weight
layers.27.mlp.up_proj.weight
layers.27.mlp.down_proj.weight
layers.27.input_layernorm.weight
layers.27.post_attention_layernorm.weight
layers.28.self_attn.q_proj.weight
layers.28.self_attn.k_proj.weight
layers.28.self_attn.v_proj.weight
layers.28.self_attn.o_proj.weight
layers.28.mlp.gate_proj.weight
layers.28.mlp.up_proj.weight
layers.28.mlp.down_proj.weight
layers.28.input_layernorm.weight
layers.28.post_attention_layernorm.weight
layers.29.self_attn.q_proj.weight
layers.29.self_attn.k_proj.weight
layers.29.self_attn.v_proj.weight
layers.29.self_attn.o_proj.weight
layers.29.mlp.gate_proj.weight
layers.29.mlp.up_proj.weight
layers.29.mlp.down_proj.weight
layers.29.input_layernorm.weight
layers.29.post_attention_layernorm.weight
layers.30.self_attn.q_proj.weight
layers.30.self_attn.k_proj.weight
layers.30.self_attn.v_proj.weight
layers.30.self_attn.o_proj.weight
layers.30.mlp.gate_proj.weight
layers.30.mlp.up_proj.weight
layers.30.mlp.down_proj.weight
layers.30.input_layernorm.weight
layers.30.post_attention_layernorm.weight
layers.31.self_attn.q_proj.weight
layers.31.self_attn.k_proj.weight
layers.31.self_attn.v_proj.weight
layers.31.self_attn.o_proj.weight
layers.31.mlp.gate_proj.weight
layers.31.mlp.up_proj.weight
layers.31.mlp.down_proj.weight
layers.31.input_layernorm.weight
layers.31.post_attention_layernorm.weight
layers.32.self_attn.q_proj.weight
layers.32.self_attn.k_proj.weight
layers.32.self_attn.v_proj.weight
layers.32.self_attn.o_proj.weight
layers.32.mlp.gate_proj.weight
layers.32.mlp.up_proj.weight
layers.32.mlp.down_proj.weight
layers.32.input_layernorm.weight
layers.32.post_attention_layernorm.weight
layers.33.self_attn.q_proj.weight
layers.33.self_attn.k_proj.weight
layers.33.self_attn.v_proj.weight
layers.33.self_attn.o_proj.weight
layers.33.mlp.gate_proj.weight
layers.33.mlp.up_proj.weight
layers.33.mlp.down_proj.weight
layers.33.input_layernorm.weight
layers.33.post_attention_layernorm.weight
layers.34.self_attn.q_proj.weight
layers.34.self_attn.k_proj.weight
layers.34.self_attn.v_proj.weight
layers.34.self_attn.o_proj.weight
layers.34.mlp.gate_proj.weight
layers.34.mlp.up_proj.weight
layers.34.mlp.down_proj.weight
layers.34.input_layernorm.weight
layers.34.post_attention_layernorm.weight
layers.35.self_attn.q_proj.weight
layers.35.self_attn.k_proj.weight
layers.35.self_attn.v_proj.weight
layers.35.self_attn.o_proj.weight
layers.35.mlp.gate_proj.weight
layers.35.mlp.up_proj.weight
layers.35.mlp.down_proj.weight
layers.35.input_layernorm.weight
layers.35.post_attention_layernorm.weight
layers.36.self_attn.q_proj.weight
layers.36.self_attn.k_proj.weight
layers.36.self_attn.v_proj.weight
layers.36.self_attn.o_proj.weight
layers.36.mlp.gate_proj.weight
layers.36.mlp.up_proj.weight
layers.36.mlp.down_proj.weight
layers.36.input_layernorm.weight
layers.36.post_attention_layernorm.weight
layers.37.self_attn.q_proj.weight
layers.37.self_attn.k_proj.weight
layers.37.self_attn.v_proj.weight
layers.37.self_attn.o_proj.weight
layers.37.mlp.gate_proj.weight
layers.37.mlp.up_proj.weight
layers.37.mlp.down_proj.weight
layers.37.input_layernorm.weight
layers.37.post_attention_layernorm.weight
layers.38.self_attn.q_proj.weight
layers.38.self_attn.k_proj.weight
layers.38.self_attn.v_proj.weight
layers.38.self_attn.o_proj.weight
layers.38.mlp.gate_proj.weight
layers.38.mlp.up_proj.weight
layers.38.mlp.down_proj.weight
layers.38.input_layernorm.weight
layers.38.post_attention_layernorm.weight
layers.39.self_attn.q_proj.weight
layers.39.self_attn.k_proj.weight
layers.39.self_attn.v_proj.weight
layers.39.self_attn.o_proj.weight
layers.39.mlp.gate_proj.weight
layers.39.mlp.up_proj.weight
layers.39.mlp.down_proj.weight
layers.39.input_layernorm.weight
layers.39.post_attention_layernorm.weight
norm.weight
vision_tower.vision_tower.vision_model.embeddings.class_embedding
vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight
vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight
vision_tower.vision_tower.vision_model.pre_layrnorm.weight
vision_tower.vision_tower.vision_model.pre_layrnorm.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias
vision_tower.vision_tower.vision_model.post_layernorm.weight
vision_tower.vision_tower.vision_model.post_layernorm.bias
mm_projector.query
mm_projector.proj
mm_projector.kv_proj.weight
mm_projector.attn.in_proj_weight
mm_projector.attn.in_proj_bias
mm_projector.attn.out_proj.weight
mm_projector.attn.out_proj.bias
mm_projector.ln_q.weight
mm_projector.ln_q.bias
mm_projector.ln_kv.weight
mm_projector.ln_kv.bias
mm_projector.ln_post.weight
mm_projector.ln_post.bias
lm_head.weight
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
layers.12.self_attn.q_proj.weight
layers.12.self_attn.k_proj.weight
layers.12.self_attn.v_proj.weight
layers.12.self_attn.o_proj.weight
layers.12.mlp.gate_proj.weight
layers.12.mlp.up_proj.weight
layers.12.mlp.down_proj.weight
layers.12.input_layernorm.weight
layers.12.post_attention_layernorm.weight
layers.13.self_attn.q_proj.weight
layers.13.self_attn.k_proj.weight
layers.13.self_attn.v_proj.weight
layers.13.self_attn.o_proj.weight
layers.13.mlp.gate_proj.weight
layers.13.mlp.up_proj.weight
layers.13.mlp.down_proj.weight
layers.13.input_layernorm.weight
layers.13.post_attention_layernorm.weight
layers.14.self_attn.q_proj.weight
layers.14.self_attn.k_proj.weight
layers.14.self_attn.v_proj.weight
layers.14.self_attn.o_proj.weight
layers.14.mlp.gate_proj.weight
layers.14.mlp.up_proj.weight
layers.14.mlp.down_proj.weight
layers.14.input_layernorm.weight
layers.14.post_attention_layernorm.weight
layers.15.self_attn.q_proj.weight
layers.15.self_attn.k_proj.weight
layers.15.self_attn.v_proj.weight
layers.15.self_attn.o_proj.weight
layers.15.mlp.gate_proj.weight
layers.15.mlp.up_proj.weight
layers.15.mlp.down_proj.weight
layers.15.input_layernorm.weight
layers.15.post_attention_layernorm.weight
layers.16.self_attn.q_proj.weight
layers.16.self_attn.k_proj.weight
layers.16.self_attn.v_proj.weight
layers.16.self_attn.o_proj.weight
layers.16.mlp.gate_proj.weight
layers.16.mlp.up_proj.weight
layers.16.mlp.down_proj.weight
layers.16.input_layernorm.weight
layers.16.post_attention_layernorm.weight
layers.17.self_attn.q_proj.weight
layers.17.self_attn.k_proj.weight
layers.17.self_attn.v_proj.weight
layers.17.self_attn.o_proj.weight
layers.17.mlp.gate_proj.weight
layers.17.mlp.up_proj.weight
layers.17.mlp.down_proj.weight
layers.17.input_layernorm.weight
layers.17.post_attention_layernorm.weight
layers.18.self_attn.q_proj.weight
layers.18.self_attn.k_proj.weight
layers.18.self_attn.v_proj.weight
layers.18.self_attn.o_proj.weight
layers.18.mlp.gate_proj.weight
layers.18.mlp.up_proj.weight
layers.18.mlp.down_proj.weight
layers.18.input_layernorm.weight
layers.18.post_attention_layernorm.weight
layers.19.self_attn.q_proj.weight
layers.19.self_attn.k_proj.weight
layers.19.self_attn.v_proj.weight
layers.19.self_attn.o_proj.weight
layers.19.mlp.gate_proj.weight
layers.19.mlp.up_proj.weight
layers.19.mlp.down_proj.weight
layers.19.input_layernorm.weight
layers.19.post_attention_layernorm.weight
layers.20.self_attn.q_proj.weight
layers.20.self_attn.k_proj.weight
layers.20.self_attn.v_proj.weight
layers.20.self_attn.o_proj.weight
layers.20.mlp.gate_proj.weight
layers.20.mlp.up_proj.weight
layers.20.mlp.down_proj.weight
layers.20.input_layernorm.weight
layers.20.post_attention_layernorm.weight
layers.21.self_attn.q_proj.weight
layers.21.self_attn.k_proj.weight
layers.21.self_attn.v_proj.weight
layers.21.self_attn.o_proj.weight
layers.21.mlp.gate_proj.weight
layers.21.mlp.up_proj.weight
layers.21.mlp.down_proj.weight
layers.21.input_layernorm.weight
layers.21.post_attention_layernorm.weight
layers.22.self_attn.q_proj.weight
layers.22.self_attn.k_proj.weight
layers.22.self_attn.v_proj.weight
layers.22.self_attn.o_proj.weight
layers.22.mlp.gate_proj.weight
layers.22.mlp.up_proj.weight
layers.22.mlp.down_proj.weight
layers.22.input_layernorm.weight
layers.22.post_attention_layernorm.weight
layers.23.self_attn.q_proj.weight
layers.23.self_attn.k_proj.weight
layers.23.self_attn.v_proj.weight
layers.23.self_attn.o_proj.weight
layers.23.mlp.gate_proj.weight
layers.23.mlp.up_proj.weight
layers.23.mlp.down_proj.weight
layers.23.input_layernorm.weight
layers.23.post_attention_layernorm.weight
layers.24.self_attn.q_proj.weight
layers.24.self_attn.k_proj.weight
layers.24.self_attn.v_proj.weight
layers.24.self_attn.o_proj.weight
layers.24.mlp.gate_proj.weight
layers.24.mlp.up_proj.weight
layers.24.mlp.down_proj.weight
layers.24.input_layernorm.weight
layers.24.post_attention_layernorm.weight
layers.25.self_attn.q_proj.weight
layers.25.self_attn.k_proj.weight
layers.25.self_attn.v_proj.weight
layers.25.self_attn.o_proj.weight
layers.25.mlp.gate_proj.weight
layers.25.mlp.up_proj.weight
layers.25.mlp.down_proj.weight
layers.25.input_layernorm.weight
layers.25.post_attention_layernorm.weight
layers.26.self_attn.q_proj.weight
layers.26.self_attn.k_proj.weight
layers.26.self_attn.v_proj.weight
layers.26.self_attn.o_proj.weight
layers.26.mlp.gate_proj.weight
layers.26.mlp.up_proj.weight
layers.26.mlp.down_proj.weight
layers.26.input_layernorm.weight
layers.26.post_attention_layernorm.weight
layers.27.self_attn.q_proj.weight
layers.27.self_attn.k_proj.weight
layers.27.self_attn.v_proj.weight
layers.27.self_attn.o_proj.weight
layers.27.mlp.gate_proj.weight
layers.27.mlp.up_proj.weight
layers.27.mlp.down_proj.weight
layers.27.input_layernorm.weight
layers.27.post_attention_layernorm.weight
layers.28.self_attn.q_proj.weight
layers.28.self_attn.k_proj.weight
layers.28.self_attn.v_proj.weight
layers.28.self_attn.o_proj.weight
layers.28.mlp.gate_proj.weight
layers.28.mlp.up_proj.weight
layers.28.mlp.down_proj.weight
layers.28.input_layernorm.weight
layers.28.post_attention_layernorm.weight
layers.29.self_attn.q_proj.weight
layers.29.self_attn.k_proj.weight
layers.29.self_attn.v_proj.weight
layers.29.self_attn.o_proj.weight
layers.29.mlp.gate_proj.weight
layers.29.mlp.up_proj.weight
layers.29.mlp.down_proj.weight
layers.29.input_layernorm.weight
layers.29.post_attention_layernorm.weight
layers.30.self_attn.q_proj.weight
layers.30.self_attn.k_proj.weight
layers.30.self_attn.v_proj.weight
layers.30.self_attn.o_proj.weight
layers.30.mlp.gate_proj.weight
layers.30.mlp.up_proj.weight
layers.30.mlp.down_proj.weight
layers.30.input_layernorm.weight
layers.30.post_attention_layernorm.weight
layers.31.self_attn.q_proj.weight
layers.31.self_attn.k_proj.weight
layers.31.self_attn.v_proj.weight
layers.31.self_attn.o_proj.weight
layers.31.mlp.gate_proj.weight
layers.31.mlp.up_proj.weight
layers.31.mlp.down_proj.weight
layers.31.input_layernorm.weight
layers.31.post_attention_layernorm.weight
layers.32.self_attn.q_proj.weight
layers.32.self_attn.k_proj.weight
layers.32.self_attn.v_proj.weight
layers.32.self_attn.o_proj.weight
layers.32.mlp.gate_proj.weight
layers.32.mlp.up_proj.weight
layers.32.mlp.down_proj.weight
layers.32.input_layernorm.weight
layers.32.post_attention_layernorm.weight
layers.33.self_attn.q_proj.weight
layers.33.self_attn.k_proj.weight
layers.33.self_attn.v_proj.weight
layers.33.self_attn.o_proj.weight
layers.33.mlp.gate_proj.weight
layers.33.mlp.up_proj.weight
layers.33.mlp.down_proj.weight
layers.33.input_layernorm.weight
layers.33.post_attention_layernorm.weight
layers.34.self_attn.q_proj.weight
layers.34.self_attn.k_proj.weight
layers.34.self_attn.v_proj.weight
layers.34.self_attn.o_proj.weight
layers.34.mlp.gate_proj.weight
layers.34.mlp.up_proj.weight
layers.34.mlp.down_proj.weight
layers.34.input_layernorm.weight
layers.34.post_attention_layernorm.weight
layers.35.self_attn.q_proj.weight
layers.35.self_attn.k_proj.weight
layers.35.self_attn.v_proj.weight
layers.35.self_attn.o_proj.weight
layers.35.mlp.gate_proj.weight
layers.35.mlp.up_proj.weight
layers.35.mlp.down_proj.weight
layers.35.input_layernorm.weight
layers.35.post_attention_layernorm.weight
layers.36.self_attn.q_proj.weight
layers.36.self_attn.k_proj.weight
layers.36.self_attn.v_proj.weight
layers.36.self_attn.o_proj.weight
layers.36.mlp.gate_proj.weight
layers.36.mlp.up_proj.weight
layers.36.mlp.down_proj.weight
layers.36.input_layernorm.weight
layers.36.post_attention_layernorm.weight
layers.37.self_attn.q_proj.weight
layers.37.self_attn.k_proj.weight
layers.37.self_attn.v_proj.weight
layers.37.self_attn.o_proj.weight
layers.37.mlp.gate_proj.weight
layers.37.mlp.up_proj.weight
layers.37.mlp.down_proj.weight
layers.37.input_layernorm.weight
layers.37.post_attention_layernorm.weight
layers.38.self_attn.q_proj.weight
layers.38.self_attn.k_proj.weight
layers.38.self_attn.v_proj.weight
layers.38.self_attn.o_proj.weight
layers.38.mlp.gate_proj.weight
layers.38.mlp.up_proj.weight
layers.38.mlp.down_proj.weight
layers.38.input_layernorm.weight
layers.38.post_attention_layernorm.weight
layers.39.self_attn.q_proj.weight
layers.39.self_attn.k_proj.weight
layers.39.self_attn.v_proj.weight
layers.39.self_attn.o_proj.weight
layers.39.mlp.gate_proj.weight
layers.39.mlp.up_proj.weight
layers.39.mlp.down_proj.weight
layers.39.input_layernorm.weight
layers.39.post_attention_layernorm.weight
norm.weight
vision_tower.vision_tower.vision_model.embeddings.class_embedding
vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight
vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight
vision_tower.vision_tower.vision_model.pre_layrnorm.weight
vision_tower.vision_tower.vision_model.pre_layrnorm.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias
vision_tower.vision_tower.vision_model.post_layernorm.weight
vision_tower.vision_tower.vision_model.post_layernorm.bias
mm_projector.query
mm_projector.proj
mm_projector.kv_proj.weight
mm_projector.attn.in_proj_weight
mm_projector.attn.in_proj_bias
mm_projector.attn.out_proj.weight
mm_projector.attn.out_proj.bias
mm_projector.ln_q.weight
mm_projector.ln_q.bias
mm_projector.ln_kv.weight
mm_projector.ln_kv.bias
mm_projector.ln_post.weight
mm_projector.ln_post.bias
lm_head.weight
Formatting inputs...Skip in lazy mode
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
layers.12.self_attn.q_proj.weight
layers.12.self_attn.k_proj.weight
layers.12.self_attn.v_proj.weight
layers.12.self_attn.o_proj.weight
layers.12.mlp.gate_proj.weight
layers.12.mlp.up_proj.weight
layers.12.mlp.down_proj.weight
layers.12.input_layernorm.weight
layers.12.post_attention_layernorm.weight
layers.13.self_attn.q_proj.weight
layers.13.self_attn.k_proj.weight
layers.13.self_attn.v_proj.weight
layers.13.self_attn.o_proj.weight
layers.13.mlp.gate_proj.weight
layers.13.mlp.up_proj.weight
layers.13.mlp.down_proj.weight
layers.13.input_layernorm.weight
layers.13.post_attention_layernorm.weight
layers.14.self_attn.q_proj.weight
layers.14.self_attn.k_proj.weight
layers.14.self_attn.v_proj.weight
layers.14.self_attn.o_proj.weight
layers.14.mlp.gate_proj.weight
layers.14.mlp.up_proj.weight
layers.14.mlp.down_proj.weight
layers.14.input_layernorm.weight
layers.14.post_attention_layernorm.weight
layers.15.self_attn.q_proj.weight
layers.15.self_attn.k_proj.weight
layers.15.self_attn.v_proj.weight
layers.15.self_attn.o_proj.weight
layers.15.mlp.gate_proj.weight
layers.15.mlp.up_proj.weight
layers.15.mlp.down_proj.weight
layers.15.input_layernorm.weight
layers.15.post_attention_layernorm.weight
layers.16.self_attn.q_proj.weight
layers.16.self_attn.k_proj.weight
layers.16.self_attn.v_proj.weight
layers.16.self_attn.o_proj.weight
layers.16.mlp.gate_proj.weight
layers.16.mlp.up_proj.weight
layers.16.mlp.down_proj.weight
layers.16.input_layernorm.weight
layers.16.post_attention_layernorm.weight
layers.17.self_attn.q_proj.weight
layers.17.self_attn.k_proj.weight
layers.17.self_attn.v_proj.weight
layers.17.self_attn.o_proj.weight
layers.17.mlp.gate_proj.weight
layers.17.mlp.up_proj.weight
layers.17.mlp.down_proj.weight
layers.17.input_layernorm.weight
layers.17.post_attention_layernorm.weight
layers.18.self_attn.q_proj.weight
layers.18.self_attn.k_proj.weight
layers.18.self_attn.v_proj.weight
layers.18.self_attn.o_proj.weight
layers.18.mlp.gate_proj.weight
layers.18.mlp.up_proj.weight
layers.18.mlp.down_proj.weight
layers.18.input_layernorm.weight
layers.18.post_attention_layernorm.weight
layers.19.self_attn.q_proj.weight
layers.19.self_attn.k_proj.weight
layers.19.self_attn.v_proj.weight
layers.19.self_attn.o_proj.weight
layers.19.mlp.gate_proj.weight
layers.19.mlp.up_proj.weight
layers.19.mlp.down_proj.weight
layers.19.input_layernorm.weight
layers.19.post_attention_layernorm.weight
layers.20.self_attn.q_proj.weight
layers.20.self_attn.k_proj.weight
layers.20.self_attn.v_proj.weight
layers.20.self_attn.o_proj.weight
layers.20.mlp.gate_proj.weight
layers.20.mlp.up_proj.weight
layers.20.mlp.down_proj.weight
layers.20.input_layernorm.weight
layers.20.post_attention_layernorm.weight
layers.21.self_attn.q_proj.weight
layers.21.self_attn.k_proj.weight
layers.21.self_attn.v_proj.weight
layers.21.self_attn.o_proj.weight
layers.21.mlp.gate_proj.weight
layers.21.mlp.up_proj.weight
layers.21.mlp.down_proj.weight
layers.21.input_layernorm.weight
layers.21.post_attention_layernorm.weight
layers.22.self_attn.q_proj.weight
layers.22.self_attn.k_proj.weight
layers.22.self_attn.v_proj.weight
layers.22.self_attn.o_proj.weight
layers.22.mlp.gate_proj.weight
layers.22.mlp.up_proj.weight
layers.22.mlp.down_proj.weight
layers.22.input_layernorm.weight
layers.22.post_attention_layernorm.weight
layers.23.self_attn.q_proj.weight
layers.23.self_attn.k_proj.weight
layers.23.self_attn.v_proj.weight
layers.23.self_attn.o_proj.weight
layers.23.mlp.gate_proj.weight
layers.23.mlp.up_proj.weight
layers.23.mlp.down_proj.weight
layers.23.input_layernorm.weight
layers.23.post_attention_layernorm.weight
layers.24.self_attn.q_proj.weight
layers.24.self_attn.k_proj.weight
layers.24.self_attn.v_proj.weight
layers.24.self_attn.o_proj.weight
layers.24.mlp.gate_proj.weight
layers.24.mlp.up_proj.weight
layers.24.mlp.down_proj.weight
layers.24.input_layernorm.weight
layers.24.post_attention_layernorm.weight
layers.25.self_attn.q_proj.weight
layers.25.self_attn.k_proj.weight
layers.25.self_attn.v_proj.weight
layers.25.self_attn.o_proj.weight
layers.25.mlp.gate_proj.weight
layers.25.mlp.up_proj.weight
layers.25.mlp.down_proj.weight
layers.25.input_layernorm.weight
layers.25.post_attention_layernorm.weight
layers.26.self_attn.q_proj.weight
layers.26.self_attn.k_proj.weight
layers.26.self_attn.v_proj.weight
layers.26.self_attn.o_proj.weight
layers.26.mlp.gate_proj.weight
layers.26.mlp.up_proj.weight
layers.26.mlp.down_proj.weight
layers.26.input_layernorm.weight
layers.26.post_attention_layernorm.weight
layers.27.self_attn.q_proj.weight
layers.27.self_attn.k_proj.weight
layers.27.self_attn.v_proj.weight
layers.27.self_attn.o_proj.weight
layers.27.mlp.gate_proj.weight
layers.27.mlp.up_proj.weight
layers.27.mlp.down_proj.weight
layers.27.input_layernorm.weight
layers.27.post_attention_layernorm.weight
layers.28.self_attn.q_proj.weight
layers.28.self_attn.k_proj.weight
layers.28.self_attn.v_proj.weight
layers.28.self_attn.o_proj.weight
layers.28.mlp.gate_proj.weight
layers.28.mlp.up_proj.weight
layers.28.mlp.down_proj.weight
layers.28.input_layernorm.weight
layers.28.post_attention_layernorm.weight
layers.29.self_attn.q_proj.weight
layers.29.self_attn.k_proj.weight
layers.29.self_attn.v_proj.weight
layers.29.self_attn.o_proj.weight
layers.29.mlp.gate_proj.weight
layers.29.mlp.up_proj.weight
layers.29.mlp.down_proj.weight
layers.29.input_layernorm.weight
layers.29.post_attention_layernorm.weight
layers.30.self_attn.q_proj.weight
layers.30.self_attn.k_proj.weight
layers.30.self_attn.v_proj.weight
layers.30.self_attn.o_proj.weight
layers.30.mlp.gate_proj.weight
layers.30.mlp.up_proj.weight
layers.30.mlp.down_proj.weight
layers.30.input_layernorm.weight
layers.30.post_attention_layernorm.weight
layers.31.self_attn.q_proj.weight
layers.31.self_attn.k_proj.weight
layers.31.self_attn.v_proj.weight
layers.31.self_attn.o_proj.weight
layers.31.mlp.gate_proj.weight
layers.31.mlp.up_proj.weight
layers.31.mlp.down_proj.weight
layers.31.input_layernorm.weight
layers.31.post_attention_layernorm.weight
layers.32.self_attn.q_proj.weight
layers.32.self_attn.k_proj.weight
layers.32.self_attn.v_proj.weight
layers.32.self_attn.o_proj.weight
layers.32.mlp.gate_proj.weight
layers.32.mlp.up_proj.weight
layers.32.mlp.down_proj.weight
layers.32.input_layernorm.weight
layers.32.post_attention_layernorm.weight
layers.33.self_attn.q_proj.weight
layers.33.self_attn.k_proj.weight
layers.33.self_attn.v_proj.weight
layers.33.self_attn.o_proj.weight
layers.33.mlp.gate_proj.weight
layers.33.mlp.up_proj.weight
layers.33.mlp.down_proj.weight
layers.33.input_layernorm.weight
layers.33.post_attention_layernorm.weight
layers.34.self_attn.q_proj.weight
layers.34.self_attn.k_proj.weight
layers.34.self_attn.v_proj.weight
layers.34.self_attn.o_proj.weight
layers.34.mlp.gate_proj.weight
layers.34.mlp.up_proj.weight
layers.34.mlp.down_proj.weight
layers.34.input_layernorm.weight
layers.34.post_attention_layernorm.weight
layers.35.self_attn.q_proj.weight
layers.35.self_attn.k_proj.weight
layers.35.self_attn.v_proj.weight
layers.35.self_attn.o_proj.weight
layers.35.mlp.gate_proj.weight
layers.35.mlp.up_proj.weight
layers.35.mlp.down_proj.weight
layers.35.input_layernorm.weight
layers.35.post_attention_layernorm.weight
layers.36.self_attn.q_proj.weight
layers.36.self_attn.k_proj.weight
layers.36.self_attn.v_proj.weight
layers.36.self_attn.o_proj.weight
layers.36.mlp.gate_proj.weight
layers.36.mlp.up_proj.weight
layers.36.mlp.down_proj.weight
layers.36.input_layernorm.weight
layers.36.post_attention_layernorm.weight
layers.37.self_attn.q_proj.weight
layers.37.self_attn.k_proj.weight
layers.37.self_attn.v_proj.weight
layers.37.self_attn.o_proj.weight
layers.37.mlp.gate_proj.weight
layers.37.mlp.up_proj.weight
layers.37.mlp.down_proj.weight
layers.37.input_layernorm.weight
layers.37.post_attention_layernorm.weight
layers.38.self_attn.q_proj.weight
layers.38.self_attn.k_proj.weight
layers.38.self_attn.v_proj.weight
layers.38.self_attn.o_proj.weight
layers.38.mlp.gate_proj.weight
layers.38.mlp.up_proj.weight
layers.38.mlp.down_proj.weight
layers.38.input_layernorm.weight
layers.38.post_attention_layernorm.weight
layers.39.self_attn.q_proj.weight
layers.39.self_attn.k_proj.weight
layers.39.self_attn.v_proj.weight
layers.39.self_attn.o_proj.weight
layers.39.mlp.gate_proj.weight
layers.39.mlp.up_proj.weight
layers.39.mlp.down_proj.weight
layers.39.input_layernorm.weight
layers.39.post_attention_layernorm.weight
norm.weight
vision_tower.vision_tower.vision_model.embeddings.class_embedding
vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight
vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight
vision_tower.vision_tower.vision_model.pre_layrnorm.weight
vision_tower.vision_tower.vision_model.pre_layrnorm.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias
vision_tower.vision_tower.vision_model.post_layernorm.weight
vision_tower.vision_tower.vision_model.post_layernorm.bias
mm_projector.query
mm_projector.proj
mm_projector.kv_proj.weight
mm_projector.attn.in_proj_weight
mm_projector.attn.in_proj_bias
mm_projector.attn.out_proj.weight
mm_projector.attn.out_proj.bias
mm_projector.ln_q.weight
mm_projector.ln_q.bias
mm_projector.ln_kv.weight
mm_projector.ln_kv.bias
mm_projector.ln_post.weight
mm_projector.ln_post.bias
lm_head.weight
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
layers.12.self_attn.q_proj.weight
layers.12.self_attn.k_proj.weight
layers.12.self_attn.v_proj.weight
layers.12.self_attn.o_proj.weight
layers.12.mlp.gate_proj.weight
layers.12.mlp.up_proj.weight
layers.12.mlp.down_proj.weight
layers.12.input_layernorm.weight
layers.12.post_attention_layernorm.weight
layers.13.self_attn.q_proj.weight
layers.13.self_attn.k_proj.weight
layers.13.self_attn.v_proj.weight
layers.13.self_attn.o_proj.weight
layers.13.mlp.gate_proj.weight
layers.13.mlp.up_proj.weight
layers.13.mlp.down_proj.weight
layers.13.input_layernorm.weight
layers.13.post_attention_layernorm.weight
layers.14.self_attn.q_proj.weight
layers.14.self_attn.k_proj.weight
layers.14.self_attn.v_proj.weight
layers.14.self_attn.o_proj.weight
layers.14.mlp.gate_proj.weight
layers.14.mlp.up_proj.weight
layers.14.mlp.down_proj.weight
layers.14.input_layernorm.weight
layers.14.post_attention_layernorm.weight
layers.15.self_attn.q_proj.weight
layers.15.self_attn.k_proj.weight
layers.15.self_attn.v_proj.weight
layers.15.self_attn.o_proj.weight
layers.15.mlp.gate_proj.weight
layers.15.mlp.up_proj.weight
layers.15.mlp.down_proj.weight
layers.15.input_layernorm.weight
layers.15.post_attention_layernorm.weight
layers.16.self_attn.q_proj.weight
layers.16.self_attn.k_proj.weight
layers.16.self_attn.v_proj.weight
layers.16.self_attn.o_proj.weight
layers.16.mlp.gate_proj.weight
layers.16.mlp.up_proj.weight
layers.16.mlp.down_proj.weight
layers.16.input_layernorm.weight
layers.16.post_attention_layernorm.weight
layers.17.self_attn.q_proj.weight
layers.17.self_attn.k_proj.weight
layers.17.self_attn.v_proj.weight
layers.17.self_attn.o_proj.weight
layers.17.mlp.gate_proj.weight
layers.17.mlp.up_proj.weight
layers.17.mlp.down_proj.weight
layers.17.input_layernorm.weight
layers.17.post_attention_layernorm.weight
layers.18.self_attn.q_proj.weight
layers.18.self_attn.k_proj.weight
layers.18.self_attn.v_proj.weight
layers.18.self_attn.o_proj.weight
layers.18.mlp.gate_proj.weight
layers.18.mlp.up_proj.weight
layers.18.mlp.down_proj.weight
layers.18.input_layernorm.weight
layers.18.post_attention_layernorm.weight
layers.19.self_attn.q_proj.weight
layers.19.self_attn.k_proj.weight
layers.19.self_attn.v_proj.weight
layers.19.self_attn.o_proj.weight
layers.19.mlp.gate_proj.weight
layers.19.mlp.up_proj.weight
layers.19.mlp.down_proj.weight
layers.19.input_layernorm.weight
layers.19.post_attention_layernorm.weight
layers.20.self_attn.q_proj.weight
layers.20.self_attn.k_proj.weight
layers.20.self_attn.v_proj.weight
layers.20.self_attn.o_proj.weight
layers.20.mlp.gate_proj.weight
layers.20.mlp.up_proj.weight
layers.20.mlp.down_proj.weight
layers.20.input_layernorm.weight
layers.20.post_attention_layernorm.weight
layers.21.self_attn.q_proj.weight
layers.21.self_attn.k_proj.weight
layers.21.self_attn.v_proj.weight
layers.21.self_attn.o_proj.weight
layers.21.mlp.gate_proj.weight
layers.21.mlp.up_proj.weight
layers.21.mlp.down_proj.weight
layers.21.input_layernorm.weight
layers.21.post_attention_layernorm.weight
layers.22.self_attn.q_proj.weight
layers.22.self_attn.k_proj.weight
layers.22.self_attn.v_proj.weight
layers.22.self_attn.o_proj.weight
layers.22.mlp.gate_proj.weight
layers.22.mlp.up_proj.weight
layers.22.mlp.down_proj.weight
layers.22.input_layernorm.weight
layers.22.post_attention_layernorm.weight
layers.23.self_attn.q_proj.weight
layers.23.self_attn.k_proj.weight
layers.23.self_attn.v_proj.weight
layers.23.self_attn.o_proj.weight
layers.23.mlp.gate_proj.weight
layers.23.mlp.up_proj.weight
layers.23.mlp.down_proj.weight
layers.23.input_layernorm.weight
layers.23.post_attention_layernorm.weight
layers.24.self_attn.q_proj.weight
layers.24.self_attn.k_proj.weight
layers.24.self_attn.v_proj.weight
layers.24.self_attn.o_proj.weight
layers.24.mlp.gate_proj.weight
layers.24.mlp.up_proj.weight
layers.24.mlp.down_proj.weight
layers.24.input_layernorm.weight
layers.24.post_attention_layernorm.weight
layers.25.self_attn.q_proj.weight
layers.25.self_attn.k_proj.weight
layers.25.self_attn.v_proj.weight
layers.25.self_attn.o_proj.weight
layers.25.mlp.gate_proj.weight
layers.25.mlp.up_proj.weight
layers.25.mlp.down_proj.weight
layers.25.input_layernorm.weight
layers.25.post_attention_layernorm.weight
layers.26.self_attn.q_proj.weight
layers.26.self_attn.k_proj.weight
layers.26.self_attn.v_proj.weight
layers.26.self_attn.o_proj.weight
layers.26.mlp.gate_proj.weight
layers.26.mlp.up_proj.weight
layers.26.mlp.down_proj.weight
layers.26.input_layernorm.weight
layers.26.post_attention_layernorm.weight
layers.27.self_attn.q_proj.weight
layers.27.self_attn.k_proj.weight
layers.27.self_attn.v_proj.weight
layers.27.self_attn.o_proj.weight
layers.27.mlp.gate_proj.weight
layers.27.mlp.up_proj.weight
layers.27.mlp.down_proj.weight
layers.27.input_layernorm.weight
layers.27.post_attention_layernorm.weight
layers.28.self_attn.q_proj.weight
layers.28.self_attn.k_proj.weight
layers.28.self_attn.v_proj.weight
layers.28.self_attn.o_proj.weight
layers.28.mlp.gate_proj.weight
layers.28.mlp.up_proj.weight
layers.28.mlp.down_proj.weight
layers.28.input_layernorm.weight
layers.28.post_attention_layernorm.weight
layers.29.self_attn.q_proj.weight
layers.29.self_attn.k_proj.weight
layers.29.self_attn.v_proj.weight
layers.29.self_attn.o_proj.weight
layers.29.mlp.gate_proj.weight
layers.29.mlp.up_proj.weight
layers.29.mlp.down_proj.weight
layers.29.input_layernorm.weight
layers.29.post_attention_layernorm.weight
layers.30.self_attn.q_proj.weight
layers.30.self_attn.k_proj.weight
layers.30.self_attn.v_proj.weight
layers.30.self_attn.o_proj.weight
layers.30.mlp.gate_proj.weight
layers.30.mlp.up_proj.weight
layers.30.mlp.down_proj.weight
layers.30.input_layernorm.weight
layers.30.post_attention_layernorm.weight
layers.31.self_attn.q_proj.weight
layers.31.self_attn.k_proj.weight
layers.31.self_attn.v_proj.weight
layers.31.self_attn.o_proj.weight
layers.31.mlp.gate_proj.weight
layers.31.mlp.up_proj.weight
layers.31.mlp.down_proj.weight
layers.31.input_layernorm.weight
layers.31.post_attention_layernorm.weight
layers.32.self_attn.q_proj.weight
layers.32.self_attn.k_proj.weight
layers.32.self_attn.v_proj.weight
layers.32.self_attn.o_proj.weight
layers.32.mlp.gate_proj.weight
layers.32.mlp.up_proj.weight
layers.32.mlp.down_proj.weight
layers.32.input_layernorm.weight
layers.32.post_attention_layernorm.weight
layers.33.self_attn.q_proj.weight
layers.33.self_attn.k_proj.weight
layers.33.self_attn.v_proj.weight
layers.33.self_attn.o_proj.weight
layers.33.mlp.gate_proj.weight
layers.33.mlp.up_proj.weight
layers.33.mlp.down_proj.weight
layers.33.input_layernorm.weight
layers.33.post_attention_layernorm.weight
layers.34.self_attn.q_proj.weight
layers.34.self_attn.k_proj.weight
layers.34.self_attn.v_proj.weight
layers.34.self_attn.o_proj.weight
layers.34.mlp.gate_proj.weight
layers.34.mlp.up_proj.weight
layers.34.mlp.down_proj.weight
layers.34.input_layernorm.weight
layers.34.post_attention_layernorm.weight
layers.35.self_attn.q_proj.weight
layers.35.self_attn.k_proj.weight
layers.35.self_attn.v_proj.weight
layers.35.self_attn.o_proj.weight
layers.35.mlp.gate_proj.weight
layers.35.mlp.up_proj.weight
layers.35.mlp.down_proj.weight
layers.35.input_layernorm.weight
layers.35.post_attention_layernorm.weight
layers.36.self_attn.q_proj.weight
layers.36.self_attn.k_proj.weight
layers.36.self_attn.v_proj.weight
layers.36.self_attn.o_proj.weight
layers.36.mlp.gate_proj.weight
layers.36.mlp.up_proj.weight
layers.36.mlp.down_proj.weight
layers.36.input_layernorm.weight
layers.36.post_attention_layernorm.weight
layers.37.self_attn.q_proj.weight
layers.37.self_attn.k_proj.weight
layers.37.self_attn.v_proj.weight
layers.37.self_attn.o_proj.weight
layers.37.mlp.gate_proj.weight
layers.37.mlp.up_proj.weight
layers.37.mlp.down_proj.weight
layers.37.input_layernorm.weight
layers.37.post_attention_layernorm.weight
layers.38.self_attn.q_proj.weight
layers.38.self_attn.k_proj.weight
layers.38.self_attn.v_proj.weight
layers.38.self_attn.o_proj.weight
layers.38.mlp.gate_proj.weight
layers.38.mlp.up_proj.weight
layers.38.mlp.down_proj.weight
layers.38.input_layernorm.weight
layers.38.post_attention_layernorm.weight
layers.39.self_attn.q_proj.weight
layers.39.self_attn.k_proj.weight
layers.39.self_attn.v_proj.weight
layers.39.self_attn.o_proj.weight
layers.39.mlp.gate_proj.weight
layers.39.mlp.up_proj.weight
layers.39.mlp.down_proj.weight
layers.39.input_layernorm.weight
layers.39.post_attention_layernorm.weight
norm.weight
vision_tower.vision_tower.vision_model.embeddings.class_embedding
vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight
vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight
vision_tower.vision_tower.vision_model.pre_layrnorm.weight
vision_tower.vision_tower.vision_model.pre_layrnorm.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias
vision_tower.vision_tower.vision_model.post_layernorm.weight
vision_tower.vision_tower.vision_model.post_layernorm.bias
mm_projector.query
mm_projector.proj
mm_projector.kv_proj.weight
mm_projector.attn.in_proj_weight
mm_projector.attn.in_proj_bias
mm_projector.attn.out_proj.weight
mm_projector.attn.out_proj.bias
mm_projector.ln_q.weight
mm_projector.ln_q.bias
mm_projector.ln_kv.weight
mm_projector.ln_kv.bias
mm_projector.ln_post.weight
mm_projector.ln_post.bias
lm_head.weight
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
layers.12.self_attn.q_proj.weight
layers.12.self_attn.k_proj.weight
layers.12.self_attn.v_proj.weight
layers.12.self_attn.o_proj.weight
layers.12.mlp.gate_proj.weight
layers.12.mlp.up_proj.weight
layers.12.mlp.down_proj.weight
layers.12.input_layernorm.weight
layers.12.post_attention_layernorm.weight
layers.13.self_attn.q_proj.weight
layers.13.self_attn.k_proj.weight
layers.13.self_attn.v_proj.weight
layers.13.self_attn.o_proj.weight
layers.13.mlp.gate_proj.weight
layers.13.mlp.up_proj.weight
layers.13.mlp.down_proj.weight
layers.13.input_layernorm.weight
layers.13.post_attention_layernorm.weight
layers.14.self_attn.q_proj.weight
layers.14.self_attn.k_proj.weight
layers.14.self_attn.v_proj.weight
layers.14.self_attn.o_proj.weight
layers.14.mlp.gate_proj.weight
layers.14.mlp.up_proj.weight
layers.14.mlp.down_proj.weight
layers.14.input_layernorm.weight
layers.14.post_attention_layernorm.weight
layers.15.self_attn.q_proj.weight
layers.15.self_attn.k_proj.weight
layers.15.self_attn.v_proj.weight
layers.15.self_attn.o_proj.weight
layers.15.mlp.gate_proj.weight
layers.15.mlp.up_proj.weight
layers.15.mlp.down_proj.weight
layers.15.input_layernorm.weight
layers.15.post_attention_layernorm.weight
layers.16.self_attn.q_proj.weight
layers.16.self_attn.k_proj.weight
layers.16.self_attn.v_proj.weight
layers.16.self_attn.o_proj.weight
layers.16.mlp.gate_proj.weight
layers.16.mlp.up_proj.weight
layers.16.mlp.down_proj.weight
layers.16.input_layernorm.weight
layers.16.post_attention_layernorm.weight
layers.17.self_attn.q_proj.weight
layers.17.self_attn.k_proj.weight
layers.17.self_attn.v_proj.weight
layers.17.self_attn.o_proj.weight
layers.17.mlp.gate_proj.weight
layers.17.mlp.up_proj.weight
layers.17.mlp.down_proj.weight
layers.17.input_layernorm.weight
layers.17.post_attention_layernorm.weight
layers.18.self_attn.q_proj.weight
layers.18.self_attn.k_proj.weight
layers.18.self_attn.v_proj.weight
layers.18.self_attn.o_proj.weight
layers.18.mlp.gate_proj.weight
layers.18.mlp.up_proj.weight
layers.18.mlp.down_proj.weight
layers.18.input_layernorm.weight
layers.18.post_attention_layernorm.weight
layers.19.self_attn.q_proj.weight
layers.19.self_attn.k_proj.weight
layers.19.self_attn.v_proj.weight
layers.19.self_attn.o_proj.weight
layers.19.mlp.gate_proj.weight
layers.19.mlp.up_proj.weight
layers.19.mlp.down_proj.weight
layers.19.input_layernorm.weight
layers.19.post_attention_layernorm.weight
layers.20.self_attn.q_proj.weight
layers.20.self_attn.k_proj.weight
layers.20.self_attn.v_proj.weight
layers.20.self_attn.o_proj.weight
layers.20.mlp.gate_proj.weight
layers.20.mlp.up_proj.weight
layers.20.mlp.down_proj.weight
layers.20.input_layernorm.weight
layers.20.post_attention_layernorm.weight
layers.21.self_attn.q_proj.weight
layers.21.self_attn.k_proj.weight
layers.21.self_attn.v_proj.weight
layers.21.self_attn.o_proj.weight
layers.21.mlp.gate_proj.weight
layers.21.mlp.up_proj.weight
layers.21.mlp.down_proj.weight
layers.21.input_layernorm.weight
layers.21.post_attention_layernorm.weight
layers.22.self_attn.q_proj.weight
layers.22.self_attn.k_proj.weight
layers.22.self_attn.v_proj.weight
layers.22.self_attn.o_proj.weight
layers.22.mlp.gate_proj.weight
layers.22.mlp.up_proj.weight
layers.22.mlp.down_proj.weight
layers.22.input_layernorm.weight
layers.22.post_attention_layernorm.weight
layers.23.self_attn.q_proj.weight
layers.23.self_attn.k_proj.weight
layers.23.self_attn.v_proj.weight
layers.23.self_attn.o_proj.weight
layers.23.mlp.gate_proj.weight
layers.23.mlp.up_proj.weight
layers.23.mlp.down_proj.weight
layers.23.input_layernorm.weight
layers.23.post_attention_layernorm.weight
layers.24.self_attn.q_proj.weight
layers.24.self_attn.k_proj.weight
layers.24.self_attn.v_proj.weight
layers.24.self_attn.o_proj.weight
layers.24.mlp.gate_proj.weight
layers.24.mlp.up_proj.weight
layers.24.mlp.down_proj.weight
layers.24.input_layernorm.weight
layers.24.post_attention_layernorm.weight
layers.25.self_attn.q_proj.weight
layers.25.self_attn.k_proj.weight
layers.25.self_attn.v_proj.weight
layers.25.self_attn.o_proj.weight
layers.25.mlp.gate_proj.weight
layers.25.mlp.up_proj.weight
layers.25.mlp.down_proj.weight
layers.25.input_layernorm.weight
layers.25.post_attention_layernorm.weight
layers.26.self_attn.q_proj.weight
layers.26.self_attn.k_proj.weight
layers.26.self_attn.v_proj.weight
layers.26.self_attn.o_proj.weight
layers.26.mlp.gate_proj.weight
layers.26.mlp.up_proj.weight
layers.26.mlp.down_proj.weight
layers.26.input_layernorm.weight
layers.26.post_attention_layernorm.weight
layers.27.self_attn.q_proj.weight
layers.27.self_attn.k_proj.weight
layers.27.self_attn.v_proj.weight
layers.27.self_attn.o_proj.weight
layers.27.mlp.gate_proj.weight
layers.27.mlp.up_proj.weight
layers.27.mlp.down_proj.weight
layers.27.input_layernorm.weight
layers.27.post_attention_layernorm.weight
layers.28.self_attn.q_proj.weight
layers.28.self_attn.k_proj.weight
layers.28.self_attn.v_proj.weight
layers.28.self_attn.o_proj.weight
layers.28.mlp.gate_proj.weight
layers.28.mlp.up_proj.weight
layers.28.mlp.down_proj.weight
layers.28.input_layernorm.weight
layers.28.post_attention_layernorm.weight
layers.29.self_attn.q_proj.weight
layers.29.self_attn.k_proj.weight
layers.29.self_attn.v_proj.weight
layers.29.self_attn.o_proj.weight
layers.29.mlp.gate_proj.weight
layers.29.mlp.up_proj.weight
layers.29.mlp.down_proj.weight
layers.29.input_layernorm.weight
layers.29.post_attention_layernorm.weight
layers.30.self_attn.q_proj.weight
layers.30.self_attn.k_proj.weight
layers.30.self_attn.v_proj.weight
layers.30.self_attn.o_proj.weight
layers.30.mlp.gate_proj.weight
layers.30.mlp.up_proj.weight
layers.30.mlp.down_proj.weight
layers.30.input_layernorm.weight
layers.30.post_attention_layernorm.weight
layers.31.self_attn.q_proj.weight
layers.31.self_attn.k_proj.weight
layers.31.self_attn.v_proj.weight
layers.31.self_attn.o_proj.weight
layers.31.mlp.gate_proj.weight
layers.31.mlp.up_proj.weight
layers.31.mlp.down_proj.weight
layers.31.input_layernorm.weight
layers.31.post_attention_layernorm.weight
layers.32.self_attn.q_proj.weight
layers.32.self_attn.k_proj.weight
layers.32.self_attn.v_proj.weight
layers.32.self_attn.o_proj.weight
layers.32.mlp.gate_proj.weight
layers.32.mlp.up_proj.weight
layers.32.mlp.down_proj.weight
layers.32.input_layernorm.weight
layers.32.post_attention_layernorm.weight
layers.33.self_attn.q_proj.weight
layers.33.self_attn.k_proj.weight
layers.33.self_attn.v_proj.weight
layers.33.self_attn.o_proj.weight
layers.33.mlp.gate_proj.weight
layers.33.mlp.up_proj.weight
layers.33.mlp.down_proj.weight
layers.33.input_layernorm.weight
layers.33.post_attention_layernorm.weight
layers.34.self_attn.q_proj.weight
layers.34.self_attn.k_proj.weight
layers.34.self_attn.v_proj.weight
layers.34.self_attn.o_proj.weight
layers.34.mlp.gate_proj.weight
layers.34.mlp.up_proj.weight
layers.34.mlp.down_proj.weight
layers.34.input_layernorm.weight
layers.34.post_attention_layernorm.weight
layers.35.self_attn.q_proj.weight
layers.35.self_attn.k_proj.weight
layers.35.self_attn.v_proj.weight
layers.35.self_attn.o_proj.weight
layers.35.mlp.gate_proj.weight
layers.35.mlp.up_proj.weight
layers.35.mlp.down_proj.weight
layers.35.input_layernorm.weight
layers.35.post_attention_layernorm.weight
layers.36.self_attn.q_proj.weight
layers.36.self_attn.k_proj.weight
layers.36.self_attn.v_proj.weight
layers.36.self_attn.o_proj.weight
layers.36.mlp.gate_proj.weight
layers.36.mlp.up_proj.weight
layers.36.mlp.down_proj.weight
layers.36.input_layernorm.weight
layers.36.post_attention_layernorm.weight
layers.37.self_attn.q_proj.weight
layers.37.self_attn.k_proj.weight
layers.37.self_attn.v_proj.weight
layers.37.self_attn.o_proj.weight
layers.37.mlp.gate_proj.weight
layers.37.mlp.up_proj.weight
layers.37.mlp.down_proj.weight
layers.37.input_layernorm.weight
layers.37.post_attention_layernorm.weight
layers.38.self_attn.q_proj.weight
layers.38.self_attn.k_proj.weight
layers.38.self_attn.v_proj.weight
layers.38.self_attn.o_proj.weight
layers.38.mlp.gate_proj.weight
layers.38.mlp.up_proj.weight
layers.38.mlp.down_proj.weight
layers.38.input_layernorm.weight
layers.38.post_attention_layernorm.weight
layers.39.self_attn.q_proj.weight
layers.39.self_attn.k_proj.weight
layers.39.self_attn.v_proj.weight
layers.39.self_attn.o_proj.weight
layers.39.mlp.gate_proj.weight
layers.39.mlp.up_proj.weight
layers.39.mlp.down_proj.weight
layers.39.input_layernorm.weight
layers.39.post_attention_layernorm.weight
norm.weight
vision_tower.vision_tower.vision_model.embeddings.class_embedding
vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight
vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight
vision_tower.vision_tower.vision_model.pre_layrnorm.weight
vision_tower.vision_tower.vision_model.pre_layrnorm.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias
vision_tower.vision_tower.vision_model.post_layernorm.weight
vision_tower.vision_tower.vision_model.post_layernorm.bias
mm_projector.query
mm_projector.proj
mm_projector.kv_proj.weight
mm_projector.attn.in_proj_weight
mm_projector.attn.in_proj_bias
mm_projector.attn.out_proj.weight
mm_projector.attn.out_proj.bias
mm_projector.ln_q.weight
mm_projector.ln_q.bias
mm_projector.ln_kv.weight
mm_projector.ln_kv.bias
mm_projector.ln_post.weight
mm_projector.ln_post.bias
lm_head.weight
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
layers.12.self_attn.q_proj.weight
layers.12.self_attn.k_proj.weight
layers.12.self_attn.v_proj.weight
layers.12.self_attn.o_proj.weight
layers.12.mlp.gate_proj.weight
layers.12.mlp.up_proj.weight
layers.12.mlp.down_proj.weight
layers.12.input_layernorm.weight
layers.12.post_attention_layernorm.weight
layers.13.self_attn.q_proj.weight
layers.13.self_attn.k_proj.weight
layers.13.self_attn.v_proj.weight
layers.13.self_attn.o_proj.weight
layers.13.mlp.gate_proj.weight
layers.13.mlp.up_proj.weight
layers.13.mlp.down_proj.weight
layers.13.input_layernorm.weight
layers.13.post_attention_layernorm.weight
layers.14.self_attn.q_proj.weight
layers.14.self_attn.k_proj.weight
layers.14.self_attn.v_proj.weight
layers.14.self_attn.o_proj.weight
layers.14.mlp.gate_proj.weight
layers.14.mlp.up_proj.weight
layers.14.mlp.down_proj.weight
layers.14.input_layernorm.weight
layers.14.post_attention_layernorm.weight
layers.15.self_attn.q_proj.weight
layers.15.self_attn.k_proj.weight
layers.15.self_attn.v_proj.weight
layers.15.self_attn.o_proj.weight
layers.15.mlp.gate_proj.weight
layers.15.mlp.up_proj.weight
layers.15.mlp.down_proj.weight
layers.15.input_layernorm.weight
layers.15.post_attention_layernorm.weight
layers.16.self_attn.q_proj.weight
layers.16.self_attn.k_proj.weight
layers.16.self_attn.v_proj.weight
layers.16.self_attn.o_proj.weight
layers.16.mlp.gate_proj.weight
layers.16.mlp.up_proj.weight
layers.16.mlp.down_proj.weight
layers.16.input_layernorm.weight
layers.16.post_attention_layernorm.weight
layers.17.self_attn.q_proj.weight
layers.17.self_attn.k_proj.weight
layers.17.self_attn.v_proj.weight
layers.17.self_attn.o_proj.weight
layers.17.mlp.gate_proj.weight
layers.17.mlp.up_proj.weight
layers.17.mlp.down_proj.weight
layers.17.input_layernorm.weight
layers.17.post_attention_layernorm.weight
layers.18.self_attn.q_proj.weight
layers.18.self_attn.k_proj.weight
layers.18.self_attn.v_proj.weight
layers.18.self_attn.o_proj.weight
layers.18.mlp.gate_proj.weight
layers.18.mlp.up_proj.weight
layers.18.mlp.down_proj.weight
layers.18.input_layernorm.weight
layers.18.post_attention_layernorm.weight
layers.19.self_attn.q_proj.weight
layers.19.self_attn.k_proj.weight
layers.19.self_attn.v_proj.weight
layers.19.self_attn.o_proj.weight
layers.19.mlp.gate_proj.weight
layers.19.mlp.up_proj.weight
layers.19.mlp.down_proj.weight
layers.19.input_layernorm.weight
layers.19.post_attention_layernorm.weight
layers.20.self_attn.q_proj.weight
layers.20.self_attn.k_proj.weight
layers.20.self_attn.v_proj.weight
layers.20.self_attn.o_proj.weight
layers.20.mlp.gate_proj.weight
layers.20.mlp.up_proj.weight
layers.20.mlp.down_proj.weight
layers.20.input_layernorm.weight
layers.20.post_attention_layernorm.weight
layers.21.self_attn.q_proj.weight
layers.21.self_attn.k_proj.weight
layers.21.self_attn.v_proj.weight
layers.21.self_attn.o_proj.weight
layers.21.mlp.gate_proj.weight
layers.21.mlp.up_proj.weight
layers.21.mlp.down_proj.weight
layers.21.input_layernorm.weight
layers.21.post_attention_layernorm.weight
layers.22.self_attn.q_proj.weight
layers.22.self_attn.k_proj.weight
layers.22.self_attn.v_proj.weight
layers.22.self_attn.o_proj.weight
layers.22.mlp.gate_proj.weight
layers.22.mlp.up_proj.weight
layers.22.mlp.down_proj.weight
layers.22.input_layernorm.weight
layers.22.post_attention_layernorm.weight
layers.23.self_attn.q_proj.weight
layers.23.self_attn.k_proj.weight
layers.23.self_attn.v_proj.weight
layers.23.self_attn.o_proj.weight
layers.23.mlp.gate_proj.weight
layers.23.mlp.up_proj.weight
layers.23.mlp.down_proj.weight
layers.23.input_layernorm.weight
layers.23.post_attention_layernorm.weight
layers.24.self_attn.q_proj.weight
layers.24.self_attn.k_proj.weight
layers.24.self_attn.v_proj.weight
layers.24.self_attn.o_proj.weight
layers.24.mlp.gate_proj.weight
layers.24.mlp.up_proj.weight
layers.24.mlp.down_proj.weight
layers.24.input_layernorm.weight
layers.24.post_attention_layernorm.weight
layers.25.self_attn.q_proj.weight
layers.25.self_attn.k_proj.weight
layers.25.self_attn.v_proj.weight
layers.25.self_attn.o_proj.weight
layers.25.mlp.gate_proj.weight
layers.25.mlp.up_proj.weight
layers.25.mlp.down_proj.weight
layers.25.input_layernorm.weight
layers.25.post_attention_layernorm.weight
layers.26.self_attn.q_proj.weight
layers.26.self_attn.k_proj.weight
layers.26.self_attn.v_proj.weight
layers.26.self_attn.o_proj.weight
layers.26.mlp.gate_proj.weight
layers.26.mlp.up_proj.weight
layers.26.mlp.down_proj.weight
layers.26.input_layernorm.weight
layers.26.post_attention_layernorm.weight
layers.27.self_attn.q_proj.weight
layers.27.self_attn.k_proj.weight
layers.27.self_attn.v_proj.weight
layers.27.self_attn.o_proj.weight
layers.27.mlp.gate_proj.weight
layers.27.mlp.up_proj.weight
layers.27.mlp.down_proj.weight
layers.27.input_layernorm.weight
layers.27.post_attention_layernorm.weight
layers.28.self_attn.q_proj.weight
layers.28.self_attn.k_proj.weight
layers.28.self_attn.v_proj.weight
layers.28.self_attn.o_proj.weight
layers.28.mlp.gate_proj.weight
layers.28.mlp.up_proj.weight
layers.28.mlp.down_proj.weight
layers.28.input_layernorm.weight
layers.28.post_attention_layernorm.weight
layers.29.self_attn.q_proj.weight
layers.29.self_attn.k_proj.weight
layers.29.self_attn.v_proj.weight
layers.29.self_attn.o_proj.weight
layers.29.mlp.gate_proj.weight
layers.29.mlp.up_proj.weight
layers.29.mlp.down_proj.weight
layers.29.input_layernorm.weight
layers.29.post_attention_layernorm.weight
layers.30.self_attn.q_proj.weight
layers.30.self_attn.k_proj.weight
layers.30.self_attn.v_proj.weight
layers.30.self_attn.o_proj.weight
layers.30.mlp.gate_proj.weight
layers.30.mlp.up_proj.weight
layers.30.mlp.down_proj.weight
layers.30.input_layernorm.weight
layers.30.post_attention_layernorm.weight
layers.31.self_attn.q_proj.weight
layers.31.self_attn.k_proj.weight
layers.31.self_attn.v_proj.weight
layers.31.self_attn.o_proj.weight
layers.31.mlp.gate_proj.weight
layers.31.mlp.up_proj.weight
layers.31.mlp.down_proj.weight
layers.31.input_layernorm.weight
layers.31.post_attention_layernorm.weight
layers.32.self_attn.q_proj.weight
layers.32.self_attn.k_proj.weight
layers.32.self_attn.v_proj.weight
layers.32.self_attn.o_proj.weight
layers.32.mlp.gate_proj.weight
layers.32.mlp.up_proj.weight
layers.32.mlp.down_proj.weight
layers.32.input_layernorm.weight
layers.32.post_attention_layernorm.weight
layers.33.self_attn.q_proj.weight
layers.33.self_attn.k_proj.weight
layers.33.self_attn.v_proj.weight
layers.33.self_attn.o_proj.weight
layers.33.mlp.gate_proj.weight
layers.33.mlp.up_proj.weight
layers.33.mlp.down_proj.weight
layers.33.input_layernorm.weight
layers.33.post_attention_layernorm.weight
layers.34.self_attn.q_proj.weight
layers.34.self_attn.k_proj.weight
layers.34.self_attn.v_proj.weight
layers.34.self_attn.o_proj.weight
layers.34.mlp.gate_proj.weight
layers.34.mlp.up_proj.weight
layers.34.mlp.down_proj.weight
layers.34.input_layernorm.weight
layers.34.post_attention_layernorm.weight
layers.35.self_attn.q_proj.weight
layers.35.self_attn.k_proj.weight
layers.35.self_attn.v_proj.weight
layers.35.self_attn.o_proj.weight
layers.35.mlp.gate_proj.weight
layers.35.mlp.up_proj.weight
layers.35.mlp.down_proj.weight
layers.35.input_layernorm.weight
layers.35.post_attention_layernorm.weight
layers.36.self_attn.q_proj.weight
layers.36.self_attn.k_proj.weight
layers.36.self_attn.v_proj.weight
layers.36.self_attn.o_proj.weight
layers.36.mlp.gate_proj.weight
layers.36.mlp.up_proj.weight
layers.36.mlp.down_proj.weight
layers.36.input_layernorm.weight
layers.36.post_attention_layernorm.weight
layers.37.self_attn.q_proj.weight
layers.37.self_attn.k_proj.weight
layers.37.self_attn.v_proj.weight
layers.37.self_attn.o_proj.weight
layers.37.mlp.gate_proj.weight
layers.37.mlp.up_proj.weight
layers.37.mlp.down_proj.weight
layers.37.input_layernorm.weight
layers.37.post_attention_layernorm.weight
layers.38.self_attn.q_proj.weight
layers.38.self_attn.k_proj.weight
layers.38.self_attn.v_proj.weight
layers.38.self_attn.o_proj.weight
layers.38.mlp.gate_proj.weight
layers.38.mlp.up_proj.weight
layers.38.mlp.down_proj.weight
layers.38.input_layernorm.weight
layers.38.post_attention_layernorm.weight
layers.39.self_attn.q_proj.weight
layers.39.self_attn.k_proj.weight
layers.39.self_attn.v_proj.weight
layers.39.self_attn.o_proj.weight
layers.39.mlp.gate_proj.weight
layers.39.mlp.up_proj.weight
layers.39.mlp.down_proj.weight
layers.39.input_layernorm.weight
layers.39.post_attention_layernorm.weight
norm.weight
vision_tower.vision_tower.vision_model.embeddings.class_embedding
vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight
vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight
vision_tower.vision_tower.vision_model.pre_layrnorm.weight
vision_tower.vision_tower.vision_model.pre_layrnorm.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias
vision_tower.vision_tower.vision_model.post_layernorm.weight
vision_tower.vision_tower.vision_model.post_layernorm.bias
mm_projector.query
mm_projector.proj
mm_projector.kv_proj.weight
mm_projector.attn.in_proj_weight
mm_projector.attn.in_proj_bias
mm_projector.attn.out_proj.weight
mm_projector.attn.out_proj.bias
mm_projector.ln_q.weight
mm_projector.ln_q.bias
mm_projector.ln_kv.weight
mm_projector.ln_kv.bias
mm_projector.ln_post.weight
mm_projector.ln_post.bias
lm_head.weight
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
layers.12.self_attn.q_proj.weight
layers.12.self_attn.k_proj.weight
layers.12.self_attn.v_proj.weight
layers.12.self_attn.o_proj.weight
layers.12.mlp.gate_proj.weight
layers.12.mlp.up_proj.weight
layers.12.mlp.down_proj.weight
layers.12.input_layernorm.weight
layers.12.post_attention_layernorm.weight
layers.13.self_attn.q_proj.weight
layers.13.self_attn.k_proj.weight
layers.13.self_attn.v_proj.weight
layers.13.self_attn.o_proj.weight
layers.13.mlp.gate_proj.weight
layers.13.mlp.up_proj.weight
layers.13.mlp.down_proj.weight
layers.13.input_layernorm.weight
layers.13.post_attention_layernorm.weight
layers.14.self_attn.q_proj.weight
layers.14.self_attn.k_proj.weight
layers.14.self_attn.v_proj.weight
layers.14.self_attn.o_proj.weight
layers.14.mlp.gate_proj.weight
layers.14.mlp.up_proj.weight
layers.14.mlp.down_proj.weight
layers.14.input_layernorm.weight
layers.14.post_attention_layernorm.weight
layers.15.self_attn.q_proj.weight
layers.15.self_attn.k_proj.weight
layers.15.self_attn.v_proj.weight
layers.15.self_attn.o_proj.weight
layers.15.mlp.gate_proj.weight
layers.15.mlp.up_proj.weight
layers.15.mlp.down_proj.weight
layers.15.input_layernorm.weight
layers.15.post_attention_layernorm.weight
layers.16.self_attn.q_proj.weight
layers.16.self_attn.k_proj.weight
layers.16.self_attn.v_proj.weight
layers.16.self_attn.o_proj.weight
layers.16.mlp.gate_proj.weight
layers.16.mlp.up_proj.weight
layers.16.mlp.down_proj.weight
layers.16.input_layernorm.weight
layers.16.post_attention_layernorm.weight
layers.17.self_attn.q_proj.weight
layers.17.self_attn.k_proj.weight
layers.17.self_attn.v_proj.weight
layers.17.self_attn.o_proj.weight
layers.17.mlp.gate_proj.weight
layers.17.mlp.up_proj.weight
layers.17.mlp.down_proj.weight
layers.17.input_layernorm.weight
layers.17.post_attention_layernorm.weight
layers.18.self_attn.q_proj.weight
layers.18.self_attn.k_proj.weight
layers.18.self_attn.v_proj.weight
layers.18.self_attn.o_proj.weight
layers.18.mlp.gate_proj.weight
layers.18.mlp.up_proj.weight
layers.18.mlp.down_proj.weight
layers.18.input_layernorm.weight
layers.18.post_attention_layernorm.weight
layers.19.self_attn.q_proj.weight
layers.19.self_attn.k_proj.weight
layers.19.self_attn.v_proj.weight
layers.19.self_attn.o_proj.weight
layers.19.mlp.gate_proj.weight
layers.19.mlp.up_proj.weight
layers.19.mlp.down_proj.weight
layers.19.input_layernorm.weight
layers.19.post_attention_layernorm.weight
layers.20.self_attn.q_proj.weight
layers.20.self_attn.k_proj.weight
layers.20.self_attn.v_proj.weight
layers.20.self_attn.o_proj.weight
layers.20.mlp.gate_proj.weight
layers.20.mlp.up_proj.weight
layers.20.mlp.down_proj.weight
layers.20.input_layernorm.weight
layers.20.post_attention_layernorm.weight
layers.21.self_attn.q_proj.weight
layers.21.self_attn.k_proj.weight
layers.21.self_attn.v_proj.weight
layers.21.self_attn.o_proj.weight
layers.21.mlp.gate_proj.weight
layers.21.mlp.up_proj.weight
layers.21.mlp.down_proj.weight
layers.21.input_layernorm.weight
layers.21.post_attention_layernorm.weight
layers.22.self_attn.q_proj.weight
layers.22.self_attn.k_proj.weight
layers.22.self_attn.v_proj.weight
layers.22.self_attn.o_proj.weight
layers.22.mlp.gate_proj.weight
layers.22.mlp.up_proj.weight
layers.22.mlp.down_proj.weight
layers.22.input_layernorm.weight
layers.22.post_attention_layernorm.weight
layers.23.self_attn.q_proj.weight
layers.23.self_attn.k_proj.weight
layers.23.self_attn.v_proj.weight
layers.23.self_attn.o_proj.weight
layers.23.mlp.gate_proj.weight
layers.23.mlp.up_proj.weight
layers.23.mlp.down_proj.weight
layers.23.input_layernorm.weight
layers.23.post_attention_layernorm.weight
layers.24.self_attn.q_proj.weight
layers.24.self_attn.k_proj.weight
layers.24.self_attn.v_proj.weight
layers.24.self_attn.o_proj.weight
layers.24.mlp.gate_proj.weight
layers.24.mlp.up_proj.weight
layers.24.mlp.down_proj.weight
layers.24.input_layernorm.weight
layers.24.post_attention_layernorm.weight
layers.25.self_attn.q_proj.weight
layers.25.self_attn.k_proj.weight
layers.25.self_attn.v_proj.weight
layers.25.self_attn.o_proj.weight
layers.25.mlp.gate_proj.weight
layers.25.mlp.up_proj.weight
layers.25.mlp.down_proj.weight
layers.25.input_layernorm.weight
layers.25.post_attention_layernorm.weight
layers.26.self_attn.q_proj.weight
layers.26.self_attn.k_proj.weight
layers.26.self_attn.v_proj.weight
layers.26.self_attn.o_proj.weight
layers.26.mlp.gate_proj.weight
layers.26.mlp.up_proj.weight
layers.26.mlp.down_proj.weight
layers.26.input_layernorm.weight
layers.26.post_attention_layernorm.weight
layers.27.self_attn.q_proj.weight
layers.27.self_attn.k_proj.weight
layers.27.self_attn.v_proj.weight
layers.27.self_attn.o_proj.weight
layers.27.mlp.gate_proj.weight
layers.27.mlp.up_proj.weight
layers.27.mlp.down_proj.weight
layers.27.input_layernorm.weight
layers.27.post_attention_layernorm.weight
layers.28.self_attn.q_proj.weight
layers.28.self_attn.k_proj.weight
layers.28.self_attn.v_proj.weight
layers.28.self_attn.o_proj.weight
layers.28.mlp.gate_proj.weight
layers.28.mlp.up_proj.weight
layers.28.mlp.down_proj.weight
layers.28.input_layernorm.weight
layers.28.post_attention_layernorm.weight
layers.29.self_attn.q_proj.weight
layers.29.self_attn.k_proj.weight
layers.29.self_attn.v_proj.weight
layers.29.self_attn.o_proj.weight
layers.29.mlp.gate_proj.weight
layers.29.mlp.up_proj.weight
layers.29.mlp.down_proj.weight
layers.29.input_layernorm.weight
layers.29.post_attention_layernorm.weight
layers.30.self_attn.q_proj.weight
layers.30.self_attn.k_proj.weight
layers.30.self_attn.v_proj.weight
layers.30.self_attn.o_proj.weight
layers.30.mlp.gate_proj.weight
layers.30.mlp.up_proj.weight
layers.30.mlp.down_proj.weight
layers.30.input_layernorm.weight
layers.30.post_attention_layernorm.weight
layers.31.self_attn.q_proj.weight
layers.31.self_attn.k_proj.weight
layers.31.self_attn.v_proj.weight
layers.31.self_attn.o_proj.weight
layers.31.mlp.gate_proj.weight
layers.31.mlp.up_proj.weight
layers.31.mlp.down_proj.weight
layers.31.input_layernorm.weight
layers.31.post_attention_layernorm.weight
layers.32.self_attn.q_proj.weight
layers.32.self_attn.k_proj.weight
layers.32.self_attn.v_proj.weight
layers.32.self_attn.o_proj.weight
layers.32.mlp.gate_proj.weight
layers.32.mlp.up_proj.weight
layers.32.mlp.down_proj.weight
layers.32.input_layernorm.weight
layers.32.post_attention_layernorm.weight
layers.33.self_attn.q_proj.weight
layers.33.self_attn.k_proj.weight
layers.33.self_attn.v_proj.weight
layers.33.self_attn.o_proj.weight
layers.33.mlp.gate_proj.weight
layers.33.mlp.up_proj.weight
layers.33.mlp.down_proj.weight
layers.33.input_layernorm.weight
layers.33.post_attention_layernorm.weight
layers.34.self_attn.q_proj.weight
layers.34.self_attn.k_proj.weight
layers.34.self_attn.v_proj.weight
layers.34.self_attn.o_proj.weight
layers.34.mlp.gate_proj.weight
layers.34.mlp.up_proj.weight
layers.34.mlp.down_proj.weight
layers.34.input_layernorm.weight
layers.34.post_attention_layernorm.weight
layers.35.self_attn.q_proj.weight
layers.35.self_attn.k_proj.weight
layers.35.self_attn.v_proj.weight
layers.35.self_attn.o_proj.weight
layers.35.mlp.gate_proj.weight
layers.35.mlp.up_proj.weight
layers.35.mlp.down_proj.weight
layers.35.input_layernorm.weight
layers.35.post_attention_layernorm.weight
layers.36.self_attn.q_proj.weight
layers.36.self_attn.k_proj.weight
layers.36.self_attn.v_proj.weight
layers.36.self_attn.o_proj.weight
layers.36.mlp.gate_proj.weight
layers.36.mlp.up_proj.weight
layers.36.mlp.down_proj.weight
layers.36.input_layernorm.weight
layers.36.post_attention_layernorm.weight
layers.37.self_attn.q_proj.weight
layers.37.self_attn.k_proj.weight
layers.37.self_attn.v_proj.weight
layers.37.self_attn.o_proj.weight
layers.37.mlp.gate_proj.weight
layers.37.mlp.up_proj.weight
layers.37.mlp.down_proj.weight
layers.37.input_layernorm.weight
layers.37.post_attention_layernorm.weight
layers.38.self_attn.q_proj.weight
layers.38.self_attn.k_proj.weight
layers.38.self_attn.v_proj.weight
layers.38.self_attn.o_proj.weight
layers.38.mlp.gate_proj.weight
layers.38.mlp.up_proj.weight
layers.38.mlp.down_proj.weight
layers.38.input_layernorm.weight
layers.38.post_attention_layernorm.weight
layers.39.self_attn.q_proj.weight
layers.39.self_attn.k_proj.weight
layers.39.self_attn.v_proj.weight
layers.39.self_attn.o_proj.weight
layers.39.mlp.gate_proj.weight
layers.39.mlp.up_proj.weight
layers.39.mlp.down_proj.weight
layers.39.input_layernorm.weight
layers.39.post_attention_layernorm.weight
norm.weight
vision_tower.vision_tower.vision_model.embeddings.class_embedding
vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight
vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight
vision_tower.vision_tower.vision_model.pre_layrnorm.weight
vision_tower.vision_tower.vision_model.pre_layrnorm.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight
vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias
vision_tower.vision_tower.vision_model.post_layernorm.weight
vision_tower.vision_tower.vision_model.post_layernorm.bias
mm_projector.query
mm_projector.proj
mm_projector.kv_proj.weight
mm_projector.attn.in_proj_weight
mm_projector.attn.in_proj_bias
mm_projector.attn.out_proj.weight
mm_projector.attn.out_proj.bias
mm_projector.ln_q.weight
mm_projector.ln_q.bias
mm_projector.ln_kv.weight
mm_projector.ln_kv.bias
mm_projector.ln_post.weight
mm_projector.ln_post.bias
lm_head.weight
wandb: Tracking run with wandb version 0.17.6
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
  0%|          | 0/1426 [00:00<?, ?it/s]/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  0%|          | 1/1426 [00:50<20:05:43, 50.77s/it]                                                   {'loss': 1.2051, 'learning_rate': 4.651162790697675e-07, 'epoch': 0.0}
  0%|          | 1/1426 [00:50<20:05:43, 50.77s/it]  0%|          | 2/1426 [01:08<12:25:43, 31.42s/it]                                                   {'loss': 1.2376, 'learning_rate': 9.30232558139535e-07, 'epoch': 0.0}
  0%|          | 2/1426 [01:08<12:25:43, 31.42s/it]  0%|          | 3/1426 [01:25<9:44:43, 24.65s/it]                                                   {'loss': 1.2097, 'learning_rate': 1.3953488372093025e-06, 'epoch': 0.0}
  0%|          | 3/1426 [01:25<9:44:43, 24.65s/it]  0%|          | 4/1426 [01:43<8:41:30, 22.00s/it]                                                  {'loss': 1.2477, 'learning_rate': 1.86046511627907e-06, 'epoch': 0.0}
  0%|          | 4/1426 [01:43<8:41:30, 22.00s/it]  0%|          | 5/1426 [02:01<8:06:59, 20.56s/it]                                                  {'loss': 1.1718, 'learning_rate': 2.3255813953488376e-06, 'epoch': 0.0}
  0%|          | 5/1426 [02:01<8:06:59, 20.56s/it]  0%|          | 6/1426 [02:18<7:39:57, 19.43s/it]                                                  {'loss': 1.1395, 'learning_rate': 2.790697674418605e-06, 'epoch': 0.0}
  0%|          | 6/1426 [02:18<7:39:57, 19.43s/it]  0%|          | 7/1426 [02:36<7:29:38, 19.01s/it]                                                  {'loss': 1.1351, 'learning_rate': 3.2558139534883724e-06, 'epoch': 0.0}
  0%|          | 7/1426 [02:36<7:29:38, 19.01s/it]  1%|          | 8/1426 [02:53<7:17:04, 18.49s/it]                                                  {'loss': 1.0652, 'learning_rate': 3.72093023255814e-06, 'epoch': 0.01}
  1%|          | 8/1426 [02:53<7:17:04, 18.49s/it]  1%|          | 9/1426 [03:10<7:05:38, 18.02s/it]                                                  {'loss': 1.0281, 'learning_rate': 4.186046511627907e-06, 'epoch': 0.01}
  1%|          | 9/1426 [03:10<7:05:38, 18.02s/it]  1%|          | 10/1426 [03:29<7:05:55, 18.05s/it]                                                   {'loss': 1.0222, 'learning_rate': 4.651162790697675e-06, 'epoch': 0.01}
  1%|          | 10/1426 [03:29<7:05:55, 18.05s/it]  1%|          | 11/1426 [03:45<6:53:22, 17.53s/it]                                                   {'loss': 1.0953, 'learning_rate': 5.116279069767442e-06, 'epoch': 0.01}
  1%|          | 11/1426 [03:45<6:53:22, 17.53s/it]  1%|          | 12/1426 [04:03<6:55:09, 17.62s/it]                                                   {'loss': 1.0409, 'learning_rate': 5.58139534883721e-06, 'epoch': 0.01}
  1%|          | 12/1426 [04:03<6:55:09, 17.62s/it]  1%|          | 13/1426 [04:19<6:48:14, 17.34s/it]                                                   {'loss': 1.0021, 'learning_rate': 6.046511627906977e-06, 'epoch': 0.01}
  1%|          | 13/1426 [04:19<6:48:14, 17.34s/it]  1%|          | 14/1426 [04:39<7:05:58, 18.10s/it]                                                   {'loss': 1.0098, 'learning_rate': 6.511627906976745e-06, 'epoch': 0.01}
  1%|          | 14/1426 [04:39<7:05:58, 18.10s/it]  1%|          | 15/1426 [05:01<7:30:58, 19.18s/it]                                                   {'loss': 0.9688, 'learning_rate': 6.976744186046513e-06, 'epoch': 0.01}
  1%|          | 15/1426 [05:01<7:30:58, 19.18s/it]  1%|          | 16/1426 [05:24<7:56:26, 20.27s/it]                                                   {'loss': 0.9561, 'learning_rate': 7.44186046511628e-06, 'epoch': 0.01}
  1%|          | 16/1426 [05:24<7:56:26, 20.27s/it]  1%|          | 17/1426 [05:48<8:20:31, 21.31s/it]                                                   {'loss': 0.9483, 'learning_rate': 7.906976744186048e-06, 'epoch': 0.01}
  1%|          | 17/1426 [05:48<8:20:31, 21.31s/it]this iter is wrong in something... skip...
  1%|▏         | 18/1426 [06:09<8:23:07, 21.44s/it]                                                   {'loss': 0.9473, 'learning_rate': 8.372093023255815e-06, 'epoch': 0.01}
  1%|▏         | 18/1426 [06:09<8:23:07, 21.44s/it]  1%|▏         | 19/1426 [06:30<8:18:26, 21.26s/it]                                                   {'loss': 0.9375, 'learning_rate': 8.837209302325582e-06, 'epoch': 0.01}
  1%|▏         | 19/1426 [06:30<8:18:26, 21.26s/it]  1%|▏         | 20/1426 [06:51<8:16:11, 21.17s/it]                                                   {'loss': 0.9365, 'learning_rate': 9.30232558139535e-06, 'epoch': 0.01}
  1%|▏         | 20/1426 [06:51<8:16:11, 21.17s/it]  1%|▏         | 21/1426 [07:12<8:17:06, 21.23s/it]                                                   {'loss': 0.8952, 'learning_rate': 9.767441860465117e-06, 'epoch': 0.01}
  1%|▏         | 21/1426 [07:12<8:17:06, 21.23s/it]  2%|▏         | 22/1426 [07:32<8:03:47, 20.67s/it]                                                   {'loss': 0.9254, 'learning_rate': 1.0232558139534884e-05, 'epoch': 0.02}
  2%|▏         | 22/1426 [07:32<8:03:47, 20.67s/it]  2%|▏         | 23/1426 [07:49<7:39:13, 19.64s/it]                                                   {'loss': 0.8824, 'learning_rate': 1.0697674418604651e-05, 'epoch': 0.02}
  2%|▏         | 23/1426 [07:49<7:39:13, 19.64s/it]  2%|▏         | 24/1426 [08:07<7:27:30, 19.15s/it]                                                   {'loss': 0.9008, 'learning_rate': 1.116279069767442e-05, 'epoch': 0.02}
  2%|▏         | 24/1426 [08:07<7:27:30, 19.15s/it]  2%|▏         | 25/1426 [08:25<7:15:46, 18.66s/it]                                                   {'loss': 0.9023, 'learning_rate': 1.1627906976744187e-05, 'epoch': 0.02}
  2%|▏         | 25/1426 [08:25<7:15:46, 18.66s/it]  2%|▏         | 26/1426 [08:44<7:19:35, 18.84s/it]                                                   {'loss': 0.8647, 'learning_rate': 1.2093023255813954e-05, 'epoch': 0.02}
  2%|▏         | 26/1426 [08:44<7:19:35, 18.84s/it]  2%|▏         | 27/1426 [09:04<7:25:57, 19.13s/it]                                                   {'loss': 0.8477, 'learning_rate': 1.2558139534883723e-05, 'epoch': 0.02}
  2%|▏         | 27/1426 [09:04<7:25:57, 19.13s/it]  2%|▏         | 28/1426 [09:21<7:13:16, 18.60s/it]                                                   {'loss': 0.8666, 'learning_rate': 1.302325581395349e-05, 'epoch': 0.02}
  2%|▏         | 28/1426 [09:21<7:13:16, 18.60s/it]  2%|▏         | 29/1426 [09:40<7:13:16, 18.61s/it]                                                   {'loss': 0.8841, 'learning_rate': 1.3488372093023257e-05, 'epoch': 0.02}
  2%|▏         | 29/1426 [09:40<7:13:16, 18.61s/it]  2%|▏         | 30/1426 [09:57<7:02:42, 18.17s/it]                                                   {'loss': 0.829, 'learning_rate': 1.3953488372093025e-05, 'epoch': 0.02}
  2%|▏         | 30/1426 [09:57<7:02:42, 18.17s/it]  2%|▏         | 31/1426 [10:17<7:13:56, 18.66s/it]                                                   {'loss': 0.8505, 'learning_rate': 1.441860465116279e-05, 'epoch': 0.02}
  2%|▏         | 31/1426 [10:17<7:13:56, 18.66s/it]  2%|▏         | 32/1426 [10:34<7:08:19, 18.44s/it]                                                   {'loss': 0.8444, 'learning_rate': 1.488372093023256e-05, 'epoch': 0.02}
  2%|▏         | 32/1426 [10:34<7:08:19, 18.44s/it]  2%|▏         | 33/1426 [10:52<6:59:48, 18.08s/it]                                                   {'loss': 0.8506, 'learning_rate': 1.5348837209302328e-05, 'epoch': 0.02}
  2%|▏         | 33/1426 [10:52<6:59:48, 18.08s/it]  2%|▏         | 34/1426 [11:11<7:05:46, 18.35s/it]                                                   {'loss': 0.828, 'learning_rate': 1.5813953488372095e-05, 'epoch': 0.02}
  2%|▏         | 34/1426 [11:11<7:05:46, 18.35s/it]  2%|▏         | 35/1426 [11:28<6:58:36, 18.06s/it]                                                   {'loss': 0.8179, 'learning_rate': 1.6279069767441862e-05, 'epoch': 0.02}
  2%|▏         | 35/1426 [11:28<6:58:36, 18.06s/it]  3%|▎         | 36/1426 [11:47<7:01:36, 18.20s/it]                                                   {'loss': 0.8408, 'learning_rate': 1.674418604651163e-05, 'epoch': 0.03}
  3%|▎         | 36/1426 [11:47<7:01:36, 18.20s/it]  3%|▎         | 37/1426 [12:04<6:56:22, 17.99s/it]                                                   {'loss': 0.8051, 'learning_rate': 1.7209302325581396e-05, 'epoch': 0.03}
  3%|▎         | 37/1426 [12:04<6:56:22, 17.99s/it]  3%|▎         | 38/1426 [12:22<6:54:01, 17.90s/it]                                                   {'loss': 0.8128, 'learning_rate': 1.7674418604651163e-05, 'epoch': 0.03}
  3%|▎         | 38/1426 [12:22<6:54:01, 17.90s/it]  3%|▎         | 39/1426 [12:39<6:49:40, 17.72s/it]                                                   {'loss': 0.8129, 'learning_rate': 1.813953488372093e-05, 'epoch': 0.03}
  3%|▎         | 39/1426 [12:39<6:49:40, 17.72s/it]  3%|▎         | 40/1426 [12:56<6:41:01, 17.36s/it]                                                   {'loss': 0.8301, 'learning_rate': 1.86046511627907e-05, 'epoch': 0.03}
  3%|▎         | 40/1426 [12:56<6:41:01, 17.36s/it]  3%|▎         | 41/1426 [13:14<6:44:25, 17.52s/it]                                                   {'loss': 0.8146, 'learning_rate': 1.9069767441860468e-05, 'epoch': 0.03}
  3%|▎         | 41/1426 [13:14<6:44:25, 17.52s/it]  3%|▎         | 42/1426 [13:32<6:47:35, 17.67s/it]                                                   {'loss': 0.8386, 'learning_rate': 1.9534883720930235e-05, 'epoch': 0.03}
  3%|▎         | 42/1426 [13:32<6:47:35, 17.67s/it]  3%|▎         | 43/1426 [13:49<6:43:14, 17.49s/it]                                                   {'loss': 0.8532, 'learning_rate': 2e-05, 'epoch': 0.03}
  3%|▎         | 43/1426 [13:49<6:43:14, 17.49s/it]  3%|▎         | 44/1426 [14:08<6:56:18, 18.07s/it]                                                   {'loss': 0.8195, 'learning_rate': 1.9999974199673452e-05, 'epoch': 0.03}
  3%|▎         | 44/1426 [14:08<6:56:18, 18.07s/it]  3%|▎         | 45/1426 [14:26<6:56:28, 18.09s/it]                                                   {'loss': 0.8012, 'learning_rate': 1.999989679882694e-05, 'epoch': 0.03}
  3%|▎         | 45/1426 [14:26<6:56:28, 18.09s/it]  3%|▎         | 46/1426 [14:44<6:57:12, 18.14s/it]                                                   {'loss': 0.8097, 'learning_rate': 1.9999767797859853e-05, 'epoch': 0.03}
  3%|▎         | 46/1426 [14:44<6:57:12, 18.14s/it]  3%|▎         | 47/1426 [15:03<6:56:53, 18.14s/it]                                                   {'loss': 0.8034, 'learning_rate': 1.9999587197437852e-05, 'epoch': 0.03}
  3%|▎         | 47/1426 [15:03<6:56:53, 18.14s/it]  3%|▎         | 48/1426 [15:20<6:53:08, 17.99s/it]                                                   {'loss': 0.8178, 'learning_rate': 1.9999354998492838e-05, 'epoch': 0.03}
  3%|▎         | 48/1426 [15:20<6:53:08, 17.99s/it]  3%|▎         | 49/1426 [15:38<6:51:19, 17.92s/it]                                                   {'loss': 0.8046, 'learning_rate': 1.9999071202222974e-05, 'epoch': 0.03}
  3%|▎         | 49/1426 [15:38<6:51:19, 17.92s/it]  4%|▎         | 50/1426 [15:55<6:47:10, 17.75s/it]                                                   {'loss': 0.826, 'learning_rate': 1.9998735810092676e-05, 'epoch': 0.04}
  4%|▎         | 50/1426 [15:55<6:47:10, 17.75s/it]  4%|▎         | 51/1426 [16:13<6:44:21, 17.64s/it]                                                   {'loss': 0.7957, 'learning_rate': 1.9998348823832582e-05, 'epoch': 0.04}
  4%|▎         | 51/1426 [16:13<6:44:21, 17.64s/it]  4%|▎         | 52/1426 [16:30<6:43:32, 17.62s/it]                                                   {'loss': 0.8294, 'learning_rate': 1.9997910245439568e-05, 'epoch': 0.04}
  4%|▎         | 52/1426 [16:30<6:43:32, 17.62s/it]  4%|▎         | 53/1426 [16:48<6:45:13, 17.71s/it]                                                   {'loss': 0.7927, 'learning_rate': 1.9997420077176727e-05, 'epoch': 0.04}
  4%|▎         | 53/1426 [16:48<6:45:13, 17.71s/it]  4%|▍         | 54/1426 [17:06<6:46:53, 17.79s/it]                                                   {'loss': 0.8122, 'learning_rate': 1.9996878321573363e-05, 'epoch': 0.04}
  4%|▍         | 54/1426 [17:06<6:46:53, 17.79s/it]  4%|▍         | 55/1426 [17:24<6:43:57, 17.68s/it]                                                   {'loss': 0.7968, 'learning_rate': 1.9996284981424963e-05, 'epoch': 0.04}
  4%|▍         | 55/1426 [17:24<6:43:57, 17.68s/it]  4%|▍         | 56/1426 [17:38<6:21:37, 16.71s/it]                                                   {'loss': 0.3234, 'learning_rate': 1.9995640059793206e-05, 'epoch': 0.04}
  4%|▍         | 56/1426 [17:38<6:21:37, 16.71s/it]  4%|▍         | 57/1426 [17:56<6:30:06, 17.10s/it]                                                   {'loss': 0.7802, 'learning_rate': 1.9994943560005924e-05, 'epoch': 0.04}
  4%|▍         | 57/1426 [17:56<6:30:06, 17.10s/it]  4%|▍         | 58/1426 [18:13<6:31:43, 17.18s/it]                                                   {'loss': 0.7909, 'learning_rate': 1.9994195485657113e-05, 'epoch': 0.04}
  4%|▍         | 58/1426 [18:13<6:31:43, 17.18s/it]  4%|▍         | 59/1426 [18:31<6:36:01, 17.38s/it]                                                   {'loss': 0.8276, 'learning_rate': 1.9993395840606875e-05, 'epoch': 0.04}
  4%|▍         | 59/1426 [18:31<6:36:01, 17.38s/it]  4%|▍         | 60/1426 [18:49<6:38:10, 17.49s/it]                                                   {'loss': 0.7978, 'learning_rate': 1.999254462898143e-05, 'epoch': 0.04}
  4%|▍         | 60/1426 [18:49<6:38:10, 17.49s/it]this iter is wrong in something... skip...
  4%|▍         | 61/1426 [19:07<6:41:03, 17.63s/it]                                                   {'loss': 0.7999, 'learning_rate': 1.9991641855173095e-05, 'epoch': 0.04}
  4%|▍         | 61/1426 [19:07<6:41:03, 17.63s/it]  4%|▍         | 62/1426 [19:25<6:42:08, 17.69s/it]                                                   {'loss': 0.8206, 'learning_rate': 1.9990687523840234e-05, 'epoch': 0.04}
  4%|▍         | 62/1426 [19:25<6:42:08, 17.69s/it]  4%|▍         | 63/1426 [19:43<6:42:45, 17.73s/it]                                                   {'loss': 0.8189, 'learning_rate': 1.9989681639907267e-05, 'epoch': 0.04}
  4%|▍         | 63/1426 [19:43<6:42:45, 17.73s/it]  4%|▍         | 64/1426 [20:01<6:44:37, 17.83s/it]                                                   {'loss': 0.7891, 'learning_rate': 1.998862420856461e-05, 'epoch': 0.04}
  4%|▍         | 64/1426 [20:01<6:44:37, 17.83s/it]  5%|▍         | 65/1426 [20:23<7:18:07, 19.31s/it]                                                   {'loss': 0.7917, 'learning_rate': 1.9987515235268682e-05, 'epoch': 0.05}
  5%|▍         | 65/1426 [20:23<7:18:07, 19.31s/it]  5%|▍         | 66/1426 [20:40<6:59:13, 18.50s/it]                                                   {'loss': 0.7967, 'learning_rate': 1.9986354725741857e-05, 'epoch': 0.05}
  5%|▍         | 66/1426 [20:40<6:59:13, 18.50s/it]  5%|▍         | 67/1426 [21:01<7:13:52, 19.16s/it]                                                   {'loss': 0.8071, 'learning_rate': 1.998514268597245e-05, 'epoch': 0.05}
  5%|▍         | 67/1426 [21:01<7:13:52, 19.16s/it]  5%|▍         | 68/1426 [21:17<6:52:35, 18.23s/it]                                                   {'loss': 0.268, 'learning_rate': 1.998387912221465e-05, 'epoch': 0.05}
  5%|▍         | 68/1426 [21:17<6:52:35, 18.23s/it]  5%|▍         | 69/1426 [21:37<7:08:31, 18.95s/it]                                                   {'loss': 0.8153, 'learning_rate': 1.9982564040988536e-05, 'epoch': 0.05}
  5%|▍         | 69/1426 [21:37<7:08:31, 18.95s/it]  5%|▍         | 70/1426 [21:59<7:27:12, 19.79s/it]                                                   {'loss': 0.7797, 'learning_rate': 1.9981197449080012e-05, 'epoch': 0.05}
  5%|▍         | 70/1426 [21:59<7:27:12, 19.79s/it]  5%|▍         | 71/1426 [22:19<7:24:58, 19.70s/it]                                                   {'loss': 0.8161, 'learning_rate': 1.9979779353540785e-05, 'epoch': 0.05}
  5%|▍         | 71/1426 [22:19<7:24:58, 19.70s/it]  5%|▌         | 72/1426 [22:41<7:39:13, 20.35s/it]                                                   {'loss': 0.7736, 'learning_rate': 1.9978309761688316e-05, 'epoch': 0.05}
  5%|▌         | 72/1426 [22:41<7:39:13, 20.35s/it]  5%|▌         | 73/1426 [22:55<6:58:28, 18.56s/it]                                                   {'loss': 0.2447, 'learning_rate': 1.9976788681105795e-05, 'epoch': 0.05}
  5%|▌         | 73/1426 [22:55<6:58:28, 18.56s/it]  5%|▌         | 74/1426 [23:17<7:19:36, 19.51s/it]                                                   {'loss': 0.8119, 'learning_rate': 1.99752161196421e-05, 'epoch': 0.05}
  5%|▌         | 74/1426 [23:17<7:19:36, 19.51s/it]  5%|▌         | 75/1426 [23:39<7:39:51, 20.42s/it]                                                   {'loss': 0.8135, 'learning_rate': 1.997359208541175e-05, 'epoch': 0.05}
  5%|▌         | 75/1426 [23:39<7:39:51, 20.42s/it]  5%|▌         | 76/1426 [24:01<7:51:06, 20.94s/it]                                                   {'loss': 0.8107, 'learning_rate': 1.9971916586794866e-05, 'epoch': 0.05}
  5%|▌         | 76/1426 [24:01<7:51:06, 20.94s/it]  5%|▌         | 77/1426 [24:21<7:43:02, 20.59s/it]                                                   {'loss': 0.8077, 'learning_rate': 1.9970189632437134e-05, 'epoch': 0.05}
  5%|▌         | 77/1426 [24:21<7:43:02, 20.59s/it]  5%|▌         | 78/1426 [24:42<7:42:15, 20.58s/it]                                                   {'loss': 0.7747, 'learning_rate': 1.9968411231249746e-05, 'epoch': 0.05}
  5%|▌         | 78/1426 [24:42<7:42:15, 20.58s/it]  6%|▌         | 79/1426 [25:02<7:38:27, 20.42s/it]                                                   {'loss': 0.7761, 'learning_rate': 1.9966581392409373e-05, 'epoch': 0.06}
  6%|▌         | 79/1426 [25:02<7:38:27, 20.42s/it]  6%|▌         | 80/1426 [25:22<7:36:04, 20.33s/it]                                                   {'loss': 0.7995, 'learning_rate': 1.99647001253581e-05, 'epoch': 0.06}
  6%|▌         | 80/1426 [25:22<7:36:04, 20.33s/it]  6%|▌         | 81/1426 [25:40<7:19:52, 19.62s/it]                                                   {'loss': 0.8103, 'learning_rate': 1.9962767439803387e-05, 'epoch': 0.06}
  6%|▌         | 81/1426 [25:40<7:19:52, 19.62s/it]  6%|▌         | 82/1426 [25:59<7:14:02, 19.38s/it]                                                   {'loss': 0.7913, 'learning_rate': 1.9960783345718023e-05, 'epoch': 0.06}
  6%|▌         | 82/1426 [25:59<7:14:02, 19.38s/it]  6%|▌         | 83/1426 [26:19<7:18:52, 19.61s/it]                                                   {'loss': 0.7917, 'learning_rate': 1.9958747853340057e-05, 'epoch': 0.06}
  6%|▌         | 83/1426 [26:19<7:18:52, 19.61s/it]  6%|▌         | 84/1426 [26:37<7:10:15, 19.24s/it]                                                   {'loss': 0.7804, 'learning_rate': 1.9956660973172767e-05, 'epoch': 0.06}
  6%|▌         | 84/1426 [26:37<7:10:15, 19.24s/it]  6%|▌         | 85/1426 [26:56<7:06:40, 19.09s/it]                                                   {'loss': 0.78, 'learning_rate': 1.9954522715984584e-05, 'epoch': 0.06}
  6%|▌         | 85/1426 [26:56<7:06:40, 19.09s/it]  6%|▌         | 86/1426 [27:13<6:56:19, 18.64s/it]                                                   {'loss': 0.7521, 'learning_rate': 1.9952333092809067e-05, 'epoch': 0.06}
  6%|▌         | 86/1426 [27:14<6:56:19, 18.64s/it]  6%|▌         | 87/1426 [27:31<6:50:55, 18.41s/it]                                                   {'loss': 0.7332, 'learning_rate': 1.9950092114944802e-05, 'epoch': 0.06}
  6%|▌         | 87/1426 [27:31<6:50:55, 18.41s/it]  6%|▌         | 88/1426 [27:50<6:51:25, 18.45s/it]                                                   {'loss': 0.7683, 'learning_rate': 1.994779979395539e-05, 'epoch': 0.06}
  6%|▌         | 88/1426 [27:50<6:51:25, 18.45s/it]  6%|▌         | 89/1426 [28:07<6:43:43, 18.12s/it]                                                   {'loss': 0.7958, 'learning_rate': 1.9945456141669354e-05, 'epoch': 0.06}
  6%|▌         | 89/1426 [28:07<6:43:43, 18.12s/it]  6%|▋         | 90/1426 [28:26<6:44:21, 18.16s/it]                                                   {'loss': 0.7953, 'learning_rate': 1.994306117018009e-05, 'epoch': 0.06}
  6%|▋         | 90/1426 [28:26<6:44:21, 18.16s/it]  6%|▋         | 91/1426 [28:44<6:47:06, 18.30s/it]                                                   {'loss': 0.7749, 'learning_rate': 1.994061489184581e-05, 'epoch': 0.06}
  6%|▋         | 91/1426 [28:44<6:47:06, 18.30s/it]  6%|▋         | 92/1426 [29:02<6:46:33, 18.29s/it]                                                   {'loss': 0.7887, 'learning_rate': 1.993811731928947e-05, 'epoch': 0.06}
  6%|▋         | 92/1426 [29:02<6:46:33, 18.29s/it]  7%|▋         | 93/1426 [29:20<6:40:12, 18.01s/it]                                                   {'loss': 0.7837, 'learning_rate': 1.9935568465398707e-05, 'epoch': 0.07}
  7%|▋         | 93/1426 [29:20<6:40:12, 18.01s/it]  7%|▋         | 94/1426 [29:38<6:38:46, 17.96s/it]                                                   {'loss': 0.7588, 'learning_rate': 1.9932968343325774e-05, 'epoch': 0.07}
  7%|▋         | 94/1426 [29:38<6:38:46, 17.96s/it]  7%|▋         | 95/1426 [29:55<6:37:32, 17.92s/it]                                                   {'loss': 0.7893, 'learning_rate': 1.993031696648747e-05, 'epoch': 0.07}
  7%|▋         | 95/1426 [29:55<6:37:32, 17.92s/it]  7%|▋         | 96/1426 [30:14<6:38:29, 17.98s/it]                                                   {'loss': 0.7676, 'learning_rate': 1.9927614348565073e-05, 'epoch': 0.07}
  7%|▋         | 96/1426 [30:14<6:38:29, 17.98s/it]  7%|▋         | 97/1426 [30:32<6:40:02, 18.06s/it]                                                   {'loss': 0.7733, 'learning_rate': 1.992486050350427e-05, 'epoch': 0.07}
  7%|▋         | 97/1426 [30:32<6:40:02, 18.06s/it]  7%|▋         | 98/1426 [30:49<6:37:11, 17.95s/it]                                                   {'loss': 0.7745, 'learning_rate': 1.992205544551508e-05, 'epoch': 0.07}
  7%|▋         | 98/1426 [30:49<6:37:11, 17.95s/it]  7%|▋         | 99/1426 [31:08<6:42:27, 18.20s/it]                                                   {'loss': 0.7591, 'learning_rate': 1.9919199189071782e-05, 'epoch': 0.07}
  7%|▋         | 99/1426 [31:08<6:42:27, 18.20s/it]  7%|▋         | 100/1426 [31:27<6:43:32, 18.26s/it]                                                    {'loss': 0.7777, 'learning_rate': 1.9916291748912847e-05, 'epoch': 0.07}
  7%|▋         | 100/1426 [31:27<6:43:32, 18.26s/it]  7%|▋         | 101/1426 [31:45<6:44:10, 18.30s/it]                                                    {'loss': 0.7761, 'learning_rate': 1.991333314004086e-05, 'epoch': 0.07}
  7%|▋         | 101/1426 [31:45<6:44:10, 18.30s/it]  7%|▋         | 102/1426 [32:02<6:33:37, 17.84s/it]                                                    {'loss': 0.7518, 'learning_rate': 1.9910323377722435e-05, 'epoch': 0.07}
  7%|▋         | 102/1426 [32:02<6:33:37, 17.84s/it]  7%|▋         | 103/1426 [32:20<6:36:31, 17.98s/it]                                                    {'loss': 0.8054, 'learning_rate': 1.990726247748814e-05, 'epoch': 0.07}
  7%|▋         | 103/1426 [32:20<6:36:31, 17.98s/it]  7%|▋         | 104/1426 [32:38<6:34:17, 17.90s/it]                                                    {'loss': 0.7771, 'learning_rate': 1.9904150455132424e-05, 'epoch': 0.07}
  7%|▋         | 104/1426 [32:38<6:34:17, 17.90s/it]  7%|▋         | 105/1426 [32:56<6:33:41, 17.88s/it]                                                    {'loss': 0.7397, 'learning_rate': 1.990098732671352e-05, 'epoch': 0.07}
  7%|▋         | 105/1426 [32:56<6:33:41, 17.88s/it]  7%|▋         | 106/1426 [33:13<6:27:01, 17.59s/it]                                                    {'loss': 0.7665, 'learning_rate': 1.9897773108553378e-05, 'epoch': 0.07}
  7%|▋         | 106/1426 [33:13<6:27:01, 17.59s/it]  8%|▊         | 107/1426 [33:31<6:29:40, 17.73s/it]                                                    {'loss': 0.77, 'learning_rate': 1.9894507817237577e-05, 'epoch': 0.08}
  8%|▊         | 107/1426 [33:31<6:29:40, 17.73s/it]  8%|▊         | 108/1426 [33:49<6:35:50, 18.02s/it]                                                    {'loss': 0.7541, 'learning_rate': 1.989119146961523e-05, 'epoch': 0.08}
  8%|▊         | 108/1426 [33:49<6:35:50, 18.02s/it]  8%|▊         | 109/1426 [34:07<6:31:54, 17.85s/it]                                                    {'loss': 0.7407, 'learning_rate': 1.9887824082798916e-05, 'epoch': 0.08}
  8%|▊         | 109/1426 [34:07<6:31:54, 17.85s/it]  8%|▊         | 110/1426 [34:25<6:35:32, 18.03s/it]                                                    {'loss': 0.7603, 'learning_rate': 1.9884405674164557e-05, 'epoch': 0.08}
  8%|▊         | 110/1426 [34:25<6:35:32, 18.03s/it]  8%|▊         | 111/1426 [34:43<6:35:26, 18.04s/it]                                                    {'loss': 0.7701, 'learning_rate': 1.9880936261351375e-05, 'epoch': 0.08}
  8%|▊         | 111/1426 [34:43<6:35:26, 18.04s/it]  8%|▊         | 112/1426 [35:00<6:28:09, 17.72s/it]                                                    {'loss': 0.7883, 'learning_rate': 1.9877415862261764e-05, 'epoch': 0.08}
  8%|▊         | 112/1426 [35:00<6:28:09, 17.72s/it]  8%|▊         | 113/1426 [35:18<6:27:00, 17.68s/it]                                                    {'loss': 0.719, 'learning_rate': 1.9873844495061213e-05, 'epoch': 0.08}
  8%|▊         | 113/1426 [35:18<6:27:00, 17.68s/it]this iter is wrong in something... skip...
  8%|▊         | 114/1426 [35:35<6:20:17, 17.39s/it]                                                    {'loss': 0.7584, 'learning_rate': 1.987022217817821e-05, 'epoch': 0.08}
  8%|▊         | 114/1426 [35:35<6:20:17, 17.39s/it]  8%|▊         | 115/1426 [35:52<6:21:57, 17.48s/it]                                                    {'loss': 0.784, 'learning_rate': 1.9866548930304145e-05, 'epoch': 0.08}
  8%|▊         | 115/1426 [35:52<6:21:57, 17.48s/it]  8%|▊         | 116/1426 [36:13<6:39:32, 18.30s/it]                                                    {'loss': 0.7714, 'learning_rate': 1.9862824770393218e-05, 'epoch': 0.08}
  8%|▊         | 116/1426 [36:13<6:39:32, 18.30s/it]  8%|▊         | 117/1426 [36:33<6:51:28, 18.86s/it]                                                    {'loss': 0.7604, 'learning_rate': 1.9859049717662337e-05, 'epoch': 0.08}
  8%|▊         | 117/1426 [36:33<6:51:28, 18.86s/it]  8%|▊         | 118/1426 [36:50<6:43:10, 18.49s/it]                                                    {'loss': 0.763, 'learning_rate': 1.9855223791591025e-05, 'epoch': 0.08}
  8%|▊         | 118/1426 [36:50<6:43:10, 18.49s/it]  8%|▊         | 119/1426 [37:11<6:57:59, 19.19s/it]                                                    {'loss': 0.7663, 'learning_rate': 1.9851347011921304e-05, 'epoch': 0.08}
  8%|▊         | 119/1426 [37:11<6:57:59, 19.19s/it]  8%|▊         | 120/1426 [37:32<7:05:47, 19.56s/it]                                                    {'loss': 0.7591, 'learning_rate': 1.9847419398657616e-05, 'epoch': 0.08}
  8%|▊         | 120/1426 [37:32<7:05:47, 19.56s/it]  8%|▊         | 121/1426 [37:54<7:23:58, 20.41s/it]                                                    {'loss': 0.7331, 'learning_rate': 1.9843440972066696e-05, 'epoch': 0.08}
  8%|▊         | 121/1426 [37:54<7:23:58, 20.41s/it]  9%|▊         | 122/1426 [38:16<7:32:01, 20.80s/it]                                                    {'loss': 0.7763, 'learning_rate': 1.983941175267749e-05, 'epoch': 0.09}
  9%|▊         | 122/1426 [38:16<7:32:01, 20.80s/it]  9%|▊         | 123/1426 [38:35<7:22:38, 20.38s/it]                                                    {'loss': 0.7579, 'learning_rate': 1.983533176128103e-05, 'epoch': 0.09}
  9%|▊         | 123/1426 [38:35<7:22:38, 20.38s/it]  9%|▊         | 124/1426 [38:57<7:31:18, 20.80s/it]                                                    {'loss': 0.7658, 'learning_rate': 1.9831201018930343e-05, 'epoch': 0.09}
  9%|▊         | 124/1426 [38:57<7:31:18, 20.80s/it]  9%|▉         | 125/1426 [39:20<7:45:34, 21.47s/it]                                                    {'loss': 0.7431, 'learning_rate': 1.982701954694032e-05, 'epoch': 0.09}
  9%|▉         | 125/1426 [39:20<7:45:34, 21.47s/it]  9%|▉         | 126/1426 [39:44<8:01:26, 22.22s/it]                                                    {'loss': 0.7228, 'learning_rate': 1.982278736688764e-05, 'epoch': 0.09}
  9%|▉         | 126/1426 [39:44<8:01:26, 22.22s/it]  9%|▉         | 127/1426 [40:04<7:47:02, 21.57s/it]                                                    {'loss': 0.7559, 'learning_rate': 1.981850450061062e-05, 'epoch': 0.09}
  9%|▉         | 127/1426 [40:04<7:47:02, 21.57s/it]  9%|▉         | 128/1426 [40:25<7:44:04, 21.45s/it]                                                    {'loss': 0.7808, 'learning_rate': 1.9814170970209138e-05, 'epoch': 0.09}
  9%|▉         | 128/1426 [40:25<7:44:04, 21.45s/it]  9%|▉         | 129/1426 [40:47<7:47:34, 21.63s/it]                                                    {'loss': 0.7634, 'learning_rate': 1.9809786798044488e-05, 'epoch': 0.09}
  9%|▉         | 129/1426 [40:47<7:47:34, 21.63s/it]  9%|▉         | 130/1426 [41:07<7:36:23, 21.13s/it]                                                    {'loss': 0.7704, 'learning_rate': 1.9805352006739286e-05, 'epoch': 0.09}
  9%|▉         | 130/1426 [41:07<7:36:23, 21.13s/it]  9%|▉         | 131/1426 [41:28<7:37:21, 21.19s/it]                                                    {'loss': 0.7431, 'learning_rate': 1.980086661917734e-05, 'epoch': 0.09}
  9%|▉         | 131/1426 [41:28<7:37:21, 21.19s/it]  9%|▉         | 132/1426 [41:43<6:52:14, 19.11s/it]                                                    {'loss': 0.2564, 'learning_rate': 1.9796330658503553e-05, 'epoch': 0.09}
  9%|▉         | 132/1426 [41:43<6:52:14, 19.11s/it]  9%|▉         | 133/1426 [42:05<7:15:23, 20.20s/it]                                                    {'loss': 0.7363, 'learning_rate': 1.979174414812377e-05, 'epoch': 0.09}
  9%|▉         | 133/1426 [42:05<7:15:23, 20.20s/it]  9%|▉         | 134/1426 [42:24<7:06:13, 19.79s/it]                                                    {'loss': 0.7667, 'learning_rate': 1.9787107111704687e-05, 'epoch': 0.09}
  9%|▉         | 134/1426 [42:24<7:06:13, 19.79s/it]  9%|▉         | 135/1426 [42:44<7:05:19, 19.77s/it]                                                    {'loss': 0.7671, 'learning_rate': 1.9782419573173718e-05, 'epoch': 0.09}
  9%|▉         | 135/1426 [42:44<7:05:19, 19.77s/it] 10%|▉         | 136/1426 [43:01<6:47:22, 18.95s/it]                                                    {'loss': 0.7448, 'learning_rate': 1.9777681556718862e-05, 'epoch': 0.1}
 10%|▉         | 136/1426 [43:01<6:47:22, 18.95s/it] 10%|▉         | 137/1426 [43:19<6:39:31, 18.60s/it]                                                    {'loss': 0.7345, 'learning_rate': 1.9772893086788598e-05, 'epoch': 0.1}
 10%|▉         | 137/1426 [43:19<6:39:31, 18.60s/it] 10%|▉         | 138/1426 [43:36<6:33:30, 18.33s/it]                                                    {'loss': 0.7839, 'learning_rate': 1.9768054188091742e-05, 'epoch': 0.1}
 10%|▉         | 138/1426 [43:37<6:33:30, 18.33s/it] 10%|▉         | 139/1426 [43:57<6:45:56, 18.93s/it]                                                    {'loss': 0.7541, 'learning_rate': 1.9763164885597326e-05, 'epoch': 0.1}
 10%|▉         | 139/1426 [43:57<6:45:56, 18.93s/it] 10%|▉         | 140/1426 [44:15<6:39:27, 18.64s/it]                                                    {'loss': 0.7452, 'learning_rate': 1.975822520453447e-05, 'epoch': 0.1}
 10%|▉         | 140/1426 [44:15<6:39:27, 18.64s/it] 10%|▉         | 141/1426 [44:32<6:28:16, 18.13s/it]                                                    {'loss': 0.73, 'learning_rate': 1.9753235170392253e-05, 'epoch': 0.1}
 10%|▉         | 141/1426 [44:32<6:28:16, 18.13s/it] 10%|▉         | 142/1426 [44:50<6:27:03, 18.09s/it]                                                    {'loss': 0.7643, 'learning_rate': 1.9748194808919577e-05, 'epoch': 0.1}
 10%|▉         | 142/1426 [44:50<6:27:03, 18.09s/it] 10%|█         | 143/1426 [45:08<6:28:57, 18.19s/it]                                                    {'loss': 0.7617, 'learning_rate': 1.9743104146125033e-05, 'epoch': 0.1}
 10%|█         | 143/1426 [45:08<6:28:57, 18.19s/it] 10%|█         | 144/1426 [45:25<6:20:02, 17.79s/it]                                                    {'loss': 0.7648, 'learning_rate': 1.973796320827678e-05, 'epoch': 0.1}
 10%|█         | 144/1426 [45:25<6:20:02, 17.79s/it] 10%|█         | 145/1426 [45:42<6:18:00, 17.71s/it]                                                    {'loss': 0.7377, 'learning_rate': 1.9732772021902383e-05, 'epoch': 0.1}
 10%|█         | 145/1426 [45:43<6:18:00, 17.71s/it] 10%|█         | 146/1426 [45:59<6:11:46, 17.43s/it]                                                    {'loss': 0.7546, 'learning_rate': 1.9727530613788716e-05, 'epoch': 0.1}
 10%|█         | 146/1426 [45:59<6:11:46, 17.43s/it] 10%|█         | 147/1426 [46:17<6:10:43, 17.39s/it]                                                    {'loss': 0.7648, 'learning_rate': 1.9722239010981778e-05, 'epoch': 0.1}
 10%|█         | 147/1426 [46:17<6:10:43, 17.39s/it] 10%|█         | 148/1426 [46:35<6:14:12, 17.57s/it]                                                    {'loss': 0.7331, 'learning_rate': 1.971689724078658e-05, 'epoch': 0.1}
 10%|█         | 148/1426 [46:35<6:14:12, 17.57s/it] 10%|█         | 149/1426 [46:52<6:11:51, 17.47s/it]                                                    {'loss': 0.7436, 'learning_rate': 1.971150533076702e-05, 'epoch': 0.1}
 10%|█         | 149/1426 [46:52<6:11:51, 17.47s/it] 11%|█         | 150/1426 [47:10<6:18:28, 17.80s/it]                                                    {'loss': 0.7535, 'learning_rate': 1.9706063308745694e-05, 'epoch': 0.11}
 11%|█         | 150/1426 [47:10<6:18:28, 17.80s/it] 11%|█         | 151/1426 [47:29<6:25:36, 18.15s/it]                                                    {'loss': 0.7691, 'learning_rate': 1.9700571202803797e-05, 'epoch': 0.11}
 11%|█         | 151/1426 [47:29<6:25:36, 18.15s/it] 11%|█         | 152/1426 [47:47<6:22:26, 18.01s/it]                                                    {'loss': 0.7478, 'learning_rate': 1.969502904128095e-05, 'epoch': 0.11}
 11%|█         | 152/1426 [47:47<6:22:26, 18.01s/it] 11%|█         | 153/1426 [48:05<6:19:20, 17.88s/it]                                                    {'loss': 0.7768, 'learning_rate': 1.9689436852775075e-05, 'epoch': 0.11}
 11%|█         | 153/1426 [48:05<6:19:20, 17.88s/it] 11%|█         | 154/1426 [48:22<6:16:10, 17.74s/it]                                                    {'loss': 0.7464, 'learning_rate': 1.968379466614222e-05, 'epoch': 0.11}
 11%|█         | 154/1426 [48:22<6:16:10, 17.74s/it] 11%|█         | 155/1426 [48:40<6:15:16, 17.72s/it]                                                    {'loss': 0.7573, 'learning_rate': 1.967810251049645e-05, 'epoch': 0.11}
 11%|█         | 155/1426 [48:40<6:15:16, 17.72s/it] 11%|█         | 156/1426 [48:57<6:12:26, 17.60s/it]                                                    {'loss': 0.7476, 'learning_rate': 1.9672360415209647e-05, 'epoch': 0.11}
 11%|█         | 156/1426 [48:57<6:12:26, 17.60s/it] 11%|█         | 157/1426 [49:14<6:09:53, 17.49s/it]                                                    {'loss': 0.731, 'learning_rate': 1.96665684099114e-05, 'epoch': 0.11}
 11%|█         | 157/1426 [49:14<6:09:53, 17.49s/it] 11%|█         | 158/1426 [49:31<6:04:42, 17.26s/it]                                                    {'loss': 0.7354, 'learning_rate': 1.9660726524488843e-05, 'epoch': 0.11}
 11%|█         | 158/1426 [49:31<6:04:42, 17.26s/it] 11%|█         | 159/1426 [49:49<6:09:58, 17.52s/it]                                                    {'loss': 0.7616, 'learning_rate': 1.9654834789086478e-05, 'epoch': 0.11}
 11%|█         | 159/1426 [49:49<6:09:58, 17.52s/it] 11%|█         | 160/1426 [50:06<6:04:21, 17.27s/it]                                                    {'loss': 0.733, 'learning_rate': 1.9648893234106044e-05, 'epoch': 0.11}
 11%|█         | 160/1426 [50:06<6:04:21, 17.27s/it] 11%|█▏        | 161/1426 [50:24<6:07:56, 17.45s/it]                                                    {'loss': 0.7429, 'learning_rate': 1.9642901890206363e-05, 'epoch': 0.11}
 11%|█▏        | 161/1426 [50:24<6:07:56, 17.45s/it] 11%|█▏        | 162/1426 [50:41<6:07:53, 17.46s/it]                                                    {'loss': 0.7649, 'learning_rate': 1.9636860788303148e-05, 'epoch': 0.11}
 11%|█▏        | 162/1426 [50:41<6:07:53, 17.46s/it]this iter is wrong in something... skip...
 11%|█▏        | 163/1426 [50:59<6:12:20, 17.69s/it]                                                    {'loss': 0.7667, 'learning_rate': 1.9630769959568888e-05, 'epoch': 0.11}
 11%|█▏        | 163/1426 [50:59<6:12:20, 17.69s/it] 12%|█▏        | 164/1426 [51:17<6:11:25, 17.66s/it]                                                    {'loss': 0.7385, 'learning_rate': 1.9624629435432653e-05, 'epoch': 0.11}
 12%|█▏        | 164/1426 [51:17<6:11:25, 17.66s/it]this iter is wrong in something... skip...
 12%|█▏        | 165/1426 [51:35<6:14:17, 17.81s/it]                                                    {'loss': 0.7248, 'learning_rate': 1.961843924757995e-05, 'epoch': 0.12}
 12%|█▏        | 165/1426 [51:35<6:14:17, 17.81s/it] 12%|█▏        | 166/1426 [51:55<6:25:53, 18.38s/it]                                                    {'loss': 0.7494, 'learning_rate': 1.9612199427952554e-05, 'epoch': 0.12}
 12%|█▏        | 166/1426 [51:55<6:25:53, 18.38s/it] 12%|█▏        | 167/1426 [52:12<6:21:21, 18.17s/it]                                                    {'loss': 0.7253, 'learning_rate': 1.9605910008748338e-05, 'epoch': 0.12}
 12%|█▏        | 167/1426 [52:13<6:21:21, 18.17s/it] 12%|█▏        | 168/1426 [52:30<6:15:31, 17.91s/it]                                                    {'loss': 0.7664, 'learning_rate': 1.9599571022421116e-05, 'epoch': 0.12}
 12%|█▏        | 168/1426 [52:30<6:15:31, 17.91s/it] 12%|█▏        | 169/1426 [52:50<6:28:45, 18.56s/it]                                                    {'loss': 0.7427, 'learning_rate': 1.9593182501680476e-05, 'epoch': 0.12}
 12%|█▏        | 169/1426 [52:50<6:28:45, 18.56s/it] 12%|█▏        | 170/1426 [53:08<6:26:13, 18.45s/it]                                                    {'loss': 0.723, 'learning_rate': 1.95867444794916e-05, 'epoch': 0.12}
 12%|█▏        | 170/1426 [53:08<6:26:13, 18.45s/it]this iter is wrong in something... skip...
 12%|█▏        | 171/1426 [53:27<6:28:37, 18.58s/it]                                                    {'loss': 0.7556, 'learning_rate': 1.9580256989075103e-05, 'epoch': 0.12}
 12%|█▏        | 171/1426 [53:27<6:28:37, 18.58s/it] 12%|█▏        | 172/1426 [53:46<6:32:26, 18.78s/it]                                                    {'loss': 0.7336, 'learning_rate': 1.9573720063906857e-05, 'epoch': 0.12}
 12%|█▏        | 172/1426 [53:46<6:32:26, 18.78s/it] 12%|█▏        | 173/1426 [54:06<6:35:33, 18.94s/it]                                                    {'loss': 0.7447, 'learning_rate': 1.9567133737717824e-05, 'epoch': 0.12}
 12%|█▏        | 173/1426 [54:06<6:35:33, 18.94s/it] 12%|█▏        | 174/1426 [54:26<6:45:25, 19.43s/it]                                                    {'loss': 0.7425, 'learning_rate': 1.956049804449388e-05, 'epoch': 0.12}
 12%|█▏        | 174/1426 [54:26<6:45:25, 19.43s/it] 12%|█▏        | 175/1426 [54:45<6:43:07, 19.33s/it]                                                    {'loss': 0.7695, 'learning_rate': 1.9553813018475633e-05, 'epoch': 0.12}
 12%|█▏        | 175/1426 [54:45<6:43:07, 19.33s/it] 12%|█▏        | 176/1426 [55:07<7:01:04, 20.21s/it]                                                    {'loss': 0.7775, 'learning_rate': 1.9547078694158253e-05, 'epoch': 0.12}
 12%|█▏        | 176/1426 [55:07<7:01:04, 20.21s/it] 12%|█▏        | 177/1426 [55:27<6:58:41, 20.11s/it]                                                    {'loss': 0.7362, 'learning_rate': 1.9540295106291294e-05, 'epoch': 0.12}
 12%|█▏        | 177/1426 [55:27<6:58:41, 20.11s/it] 12%|█▏        | 178/1426 [55:52<7:25:21, 21.41s/it]                                                    {'loss': 0.7521, 'learning_rate': 1.9533462289878518e-05, 'epoch': 0.12}
 12%|█▏        | 178/1426 [55:52<7:25:21, 21.41s/it] 13%|█▎        | 179/1426 [56:15<7:38:04, 22.04s/it]                                                    {'loss': 0.7526, 'learning_rate': 1.9526580280177695e-05, 'epoch': 0.13}
 13%|█▎        | 179/1426 [56:16<7:38:04, 22.04s/it] 13%|█▎        | 180/1426 [56:42<8:04:37, 23.34s/it]                                                    {'loss': 0.7617, 'learning_rate': 1.9519649112700447e-05, 'epoch': 0.13}
 13%|█▎        | 180/1426 [56:42<8:04:37, 23.34s/it] 13%|█▎        | 181/1426 [57:03<7:50:01, 22.65s/it]                                                    {'loss': 0.7423, 'learning_rate': 1.9512668823212056e-05, 'epoch': 0.13}
 13%|█▎        | 181/1426 [57:03<7:50:01, 22.65s/it] 13%|█▎        | 182/1426 [57:27<8:01:28, 23.22s/it]                                                    {'loss': 0.7626, 'learning_rate': 1.9505639447731264e-05, 'epoch': 0.13}
 13%|█▎        | 182/1426 [57:27<8:01:28, 23.22s/it] 13%|█▎        | 183/1426 [57:51<8:04:19, 23.38s/it]                                                    {'loss': 0.7474, 'learning_rate': 1.9498561022530114e-05, 'epoch': 0.13}
 13%|█▎        | 183/1426 [57:51<8:04:19, 23.38s/it] 13%|█▎        | 184/1426 [58:12<7:46:12, 22.52s/it]                                                    {'loss': 0.7546, 'learning_rate': 1.9491433584133736e-05, 'epoch': 0.13}
 13%|█▎        | 184/1426 [58:12<7:46:12, 22.52s/it] 13%|█▎        | 185/1426 [58:33<7:39:21, 22.21s/it]                                                    {'loss': 0.7635, 'learning_rate': 1.9484257169320187e-05, 'epoch': 0.13}
 13%|█▎        | 185/1426 [58:33<7:39:21, 22.21s/it] 13%|█▎        | 186/1426 [58:52<7:18:44, 21.23s/it]                                                    {'loss': 0.7518, 'learning_rate': 1.9477031815120227e-05, 'epoch': 0.13}
 13%|█▎        | 186/1426 [58:52<7:18:44, 21.23s/it] 13%|█▎        | 187/1426 [59:12<7:08:53, 20.77s/it]                                                    {'loss': 0.7523, 'learning_rate': 1.9469757558817163e-05, 'epoch': 0.13}
 13%|█▎        | 187/1426 [59:12<7:08:53, 20.77s/it] 13%|█▎        | 188/1426 [59:35<7:25:30, 21.59s/it]                                                    {'loss': 0.765, 'learning_rate': 1.9462434437946623e-05, 'epoch': 0.13}
 13%|█▎        | 188/1426 [59:35<7:25:30, 21.59s/it]this iter is wrong in something... skip...
 13%|█▎        | 189/1426 [1:00:01<7:52:47, 22.93s/it]                                                      {'loss': 0.7356, 'learning_rate': 1.9455062490296398e-05, 'epoch': 0.13}
 13%|█▎        | 189/1426 [1:00:01<7:52:47, 22.93s/it] 13%|█▎        | 190/1426 [1:00:21<7:30:46, 21.88s/it]                                                      {'loss': 0.7573, 'learning_rate': 1.9447641753906214e-05, 'epoch': 0.13}
 13%|█▎        | 190/1426 [1:00:21<7:30:46, 21.88s/it] 13%|█▎        | 191/1426 [1:00:40<7:15:47, 21.17s/it]                                                      {'loss': 0.7518, 'learning_rate': 1.9440172267067558e-05, 'epoch': 0.13}
 13%|█▎        | 191/1426 [1:00:40<7:15:47, 21.17s/it] 13%|█▎        | 192/1426 [1:00:58<6:55:28, 20.20s/it]                                                      {'loss': 0.7293, 'learning_rate': 1.9432654068323472e-05, 'epoch': 0.13}
 13%|█▎        | 192/1426 [1:00:58<6:55:28, 20.20s/it] 14%|█▎        | 193/1426 [1:01:15<6:36:14, 19.28s/it]                                                      {'loss': 0.7655, 'learning_rate': 1.9425087196468346e-05, 'epoch': 0.14}
 14%|█▎        | 193/1426 [1:01:15<6:36:14, 19.28s/it] 14%|█▎        | 194/1426 [1:01:33<6:25:45, 18.79s/it]                                                      {'loss': 0.7472, 'learning_rate': 1.941747169054774e-05, 'epoch': 0.14}
 14%|█▎        | 194/1426 [1:01:33<6:25:45, 18.79s/it] 14%|█▎        | 195/1426 [1:01:49<6:10:36, 18.06s/it]                                                      {'loss': 0.7811, 'learning_rate': 1.9409807589858157e-05, 'epoch': 0.14}
 14%|█▎        | 195/1426 [1:01:49<6:10:36, 18.06s/it] 14%|█▎        | 196/1426 [1:02:06<6:02:35, 17.69s/it]                                                      {'loss': 0.747, 'learning_rate': 1.9402094933946858e-05, 'epoch': 0.14}
 14%|█▎        | 196/1426 [1:02:06<6:02:35, 17.69s/it] 14%|█▍        | 197/1426 [1:02:24<6:02:41, 17.71s/it]                                                      {'loss': 0.7497, 'learning_rate': 1.9394333762611652e-05, 'epoch': 0.14}
 14%|█▍        | 197/1426 [1:02:24<6:02:41, 17.71s/it] 14%|█▍        | 198/1426 [1:02:41<5:56:36, 17.42s/it]                                                      {'loss': 0.748, 'learning_rate': 1.938652411590069e-05, 'epoch': 0.14}
 14%|█▍        | 198/1426 [1:02:41<5:56:36, 17.42s/it] 14%|█▍        | 199/1426 [1:02:57<5:51:23, 17.18s/it]                                                      {'loss': 0.7322, 'learning_rate': 1.937866603411226e-05, 'epoch': 0.14}
 14%|█▍        | 199/1426 [1:02:57<5:51:23, 17.18s/it] 14%|█▍        | 200/1426 [1:03:14<5:51:02, 17.18s/it]                                                      {'loss': 0.7686, 'learning_rate': 1.937075955779458e-05, 'epoch': 0.14}
 14%|█▍        | 200/1426 [1:03:14<5:51:02, 17.18s/it] 14%|█▍        | 201/1426 [1:03:31<5:48:02, 17.05s/it]                                                      {'loss': 0.7484, 'learning_rate': 1.9362804727745576e-05, 'epoch': 0.14}
 14%|█▍        | 201/1426 [1:03:31<5:48:02, 17.05s/it] 14%|█▍        | 202/1426 [1:03:49<5:55:23, 17.42s/it]                                                      {'loss': 0.7342, 'learning_rate': 1.9354801585012696e-05, 'epoch': 0.14}
 14%|█▍        | 202/1426 [1:03:49<5:55:23, 17.42s/it] 14%|█▍        | 203/1426 [1:04:06<5:50:13, 17.18s/it]                                                      {'loss': 0.7576, 'learning_rate': 1.9346750170892677e-05, 'epoch': 0.14}
 14%|█▍        | 203/1426 [1:04:06<5:50:13, 17.18s/it] 14%|█▍        | 204/1426 [1:04:24<5:56:22, 17.50s/it]                                                      {'loss': 0.7681, 'learning_rate': 1.9338650526931348e-05, 'epoch': 0.14}
 14%|█▍        | 204/1426 [1:04:24<5:56:22, 17.50s/it]this iter is wrong in something... skip...
 14%|█▍        | 205/1426 [1:04:42<5:55:14, 17.46s/it]                                                      {'loss': 0.7356, 'learning_rate': 1.933050269492339e-05, 'epoch': 0.14}
 14%|█▍        | 205/1426 [1:04:42<5:55:14, 17.46s/it] 14%|█▍        | 206/1426 [1:04:59<5:57:20, 17.57s/it]                                                      {'loss': 0.7195, 'learning_rate': 1.9322306716912155e-05, 'epoch': 0.14}
 14%|█▍        | 206/1426 [1:04:59<5:57:20, 17.57s/it] 15%|█▍        | 207/1426 [1:05:19<6:09:02, 18.16s/it]                                                      {'loss': 0.7445, 'learning_rate': 1.9314062635189426e-05, 'epoch': 0.15}
 15%|█▍        | 207/1426 [1:05:19<6:09:02, 18.16s/it] 15%|█▍        | 208/1426 [1:05:37<6:08:48, 18.17s/it]                                                      {'loss': 0.73, 'learning_rate': 1.93057704922952e-05, 'epoch': 0.15}
 15%|█▍        | 208/1426 [1:05:37<6:08:48, 18.17s/it] 15%|█▍        | 209/1426 [1:05:56<6:12:28, 18.36s/it]                                                      {'loss': 0.7631, 'learning_rate': 1.929743033101748e-05, 'epoch': 0.15}
 15%|█▍        | 209/1426 [1:05:56<6:12:28, 18.36s/it] 15%|█▍        | 210/1426 [1:06:14<6:08:41, 18.19s/it]                                                      {'loss': 0.7471, 'learning_rate': 1.9289042194392035e-05, 'epoch': 0.15}
 15%|█▍        | 210/1426 [1:06:14<6:08:41, 18.19s/it] 15%|█▍        | 211/1426 [1:06:33<6:11:49, 18.36s/it]                                                      {'loss': 0.7494, 'learning_rate': 1.9280606125702203e-05, 'epoch': 0.15}
 15%|█▍        | 211/1426 [1:06:33<6:11:49, 18.36s/it] 15%|█▍        | 212/1426 [1:06:50<6:07:43, 18.17s/it]                                                      {'loss': 0.7061, 'learning_rate': 1.927212216847865e-05, 'epoch': 0.15}
 15%|█▍        | 212/1426 [1:06:50<6:07:43, 18.17s/it] 15%|█▍        | 213/1426 [1:07:09<6:08:33, 18.23s/it]                                                      {'loss': 0.7269, 'learning_rate': 1.9263590366499148e-05, 'epoch': 0.15}
 15%|█▍        | 213/1426 [1:07:09<6:08:33, 18.23s/it] 15%|█▌        | 214/1426 [1:07:23<5:47:33, 17.21s/it]                                                      {'loss': 0.2617, 'learning_rate': 1.9255010763788354e-05, 'epoch': 0.15}
 15%|█▌        | 214/1426 [1:07:23<5:47:33, 17.21s/it] 15%|█▌        | 215/1426 [1:07:42<5:54:25, 17.56s/it]                                                      {'loss': 0.7541, 'learning_rate': 1.9246383404617576e-05, 'epoch': 0.15}
 15%|█▌        | 215/1426 [1:07:42<5:54:25, 17.56s/it] 15%|█▌        | 216/1426 [1:07:59<5:51:28, 17.43s/it]                                                      {'loss': 0.7466, 'learning_rate': 1.923770833350455e-05, 'epoch': 0.15}
 15%|█▌        | 216/1426 [1:07:59<5:51:28, 17.43s/it] 15%|█▌        | 217/1426 [1:08:17<5:53:25, 17.54s/it]                                                      {'loss': 0.7445, 'learning_rate': 1.9228985595213214e-05, 'epoch': 0.15}
 15%|█▌        | 217/1426 [1:08:17<5:53:25, 17.54s/it] 15%|█▌        | 218/1426 [1:08:35<5:55:26, 17.65s/it]                                                      {'loss': 0.7265, 'learning_rate': 1.9220215234753464e-05, 'epoch': 0.15}
 15%|█▌        | 218/1426 [1:08:35<5:55:26, 17.65s/it] 15%|█▌        | 219/1426 [1:08:51<5:47:48, 17.29s/it]                                                      {'loss': 0.7377, 'learning_rate': 1.9211397297380934e-05, 'epoch': 0.15}
 15%|█▌        | 219/1426 [1:08:51<5:47:48, 17.29s/it] 15%|█▌        | 220/1426 [1:09:06<5:30:46, 16.46s/it]                                                      {'loss': 0.2829, 'learning_rate': 1.9202531828596756e-05, 'epoch': 0.15}
 15%|█▌        | 220/1426 [1:09:06<5:30:46, 16.46s/it] 15%|█▌        | 221/1426 [1:09:23<5:33:09, 16.59s/it]                                                      {'loss': 0.731, 'learning_rate': 1.9193618874147327e-05, 'epoch': 0.15}
 15%|█▌        | 221/1426 [1:09:23<5:33:09, 16.59s/it] 16%|█▌        | 222/1426 [1:09:40<5:38:48, 16.88s/it]                                                      {'loss': 0.7747, 'learning_rate': 1.9184658480024076e-05, 'epoch': 0.16}
 16%|█▌        | 222/1426 [1:09:40<5:38:48, 16.88s/it] 16%|█▌        | 223/1426 [1:10:00<5:57:19, 17.82s/it]                                                      {'loss': 0.7342, 'learning_rate': 1.917565069246322e-05, 'epoch': 0.16}
 16%|█▌        | 223/1426 [1:10:00<5:57:19, 17.82s/it] 16%|█▌        | 224/1426 [1:10:20<6:07:19, 18.34s/it]                                                      {'loss': 0.7656, 'learning_rate': 1.9166595557945532e-05, 'epoch': 0.16}
 16%|█▌        | 224/1426 [1:10:20<6:07:19, 18.34s/it] 16%|█▌        | 225/1426 [1:10:39<6:10:40, 18.52s/it]                                                      {'loss': 0.7213, 'learning_rate': 1.91574931231961e-05, 'epoch': 0.16}
 16%|█▌        | 225/1426 [1:10:39<6:10:40, 18.52s/it] 16%|█▌        | 226/1426 [1:10:57<6:08:42, 18.44s/it]                                                      {'loss': 0.7568, 'learning_rate': 1.9148343435184077e-05, 'epoch': 0.16}
 16%|█▌        | 226/1426 [1:10:57<6:08:42, 18.44s/it] 16%|█▌        | 227/1426 [1:11:16<6:11:35, 18.60s/it]                                                      {'loss': 0.7591, 'learning_rate': 1.9139146541122455e-05, 'epoch': 0.16}
 16%|█▌        | 227/1426 [1:11:16<6:11:35, 18.60s/it] 16%|█▌        | 228/1426 [1:11:38<6:32:57, 19.68s/it]                                                      {'loss': 0.722, 'learning_rate': 1.9129902488467804e-05, 'epoch': 0.16}
 16%|█▌        | 228/1426 [1:11:38<6:32:57, 19.68s/it] 16%|█▌        | 229/1426 [1:11:58<6:34:15, 19.76s/it]                                                      {'loss': 0.7362, 'learning_rate': 1.912061132492004e-05, 'epoch': 0.16}
 16%|█▌        | 229/1426 [1:11:58<6:34:15, 19.76s/it] 16%|█▌        | 230/1426 [1:12:18<6:32:40, 19.70s/it]                                                      {'loss': 0.7616, 'learning_rate': 1.911127309842218e-05, 'epoch': 0.16}
 16%|█▌        | 230/1426 [1:12:18<6:32:40, 19.70s/it] 16%|█▌        | 231/1426 [1:12:43<7:05:28, 21.36s/it]                                                      {'loss': 0.7356, 'learning_rate': 1.910188785716008e-05, 'epoch': 0.16}
 16%|█▌        | 231/1426 [1:12:43<7:05:28, 21.36s/it] 16%|█▋        | 232/1426 [1:13:01<6:48:12, 20.51s/it]                                                      {'loss': 0.7732, 'learning_rate': 1.909245564956219e-05, 'epoch': 0.16}
 16%|█▋        | 232/1426 [1:13:01<6:48:12, 20.51s/it] 16%|█▋        | 233/1426 [1:13:20<6:36:12, 19.93s/it]                                                      {'loss': 0.756, 'learning_rate': 1.908297652429932e-05, 'epoch': 0.16}
 16%|█▋        | 233/1426 [1:13:20<6:36:12, 19.93s/it] 16%|█▋        | 234/1426 [1:13:42<6:47:01, 20.49s/it]                                                      {'loss': 0.7479, 'learning_rate': 1.9073450530284383e-05, 'epoch': 0.16}
 16%|█▋        | 234/1426 [1:13:42<6:47:01, 20.49s/it] 16%|█▋        | 235/1426 [1:14:01<6:38:18, 20.07s/it]                                                      {'loss': 0.7505, 'learning_rate': 1.9063877716672126e-05, 'epoch': 0.16}
 16%|█▋        | 235/1426 [1:14:01<6:38:18, 20.07s/it] 17%|█▋        | 236/1426 [1:14:21<6:38:42, 20.10s/it]                                                      {'loss': 0.7608, 'learning_rate': 1.905425813285889e-05, 'epoch': 0.17}
 17%|█▋        | 236/1426 [1:14:21<6:38:42, 20.10s/it] 17%|█▋        | 237/1426 [1:14:40<6:34:33, 19.91s/it]                                                      {'loss': 0.7578, 'learning_rate': 1.904459182848236e-05, 'epoch': 0.17}
 17%|█▋        | 237/1426 [1:14:40<6:34:33, 19.91s/it] 17%|█▋        | 238/1426 [1:15:04<6:57:00, 21.06s/it]                                                      {'loss': 0.7689, 'learning_rate': 1.9034878853421293e-05, 'epoch': 0.17}
 17%|█▋        | 238/1426 [1:15:04<6:57:00, 21.06s/it] 17%|█▋        | 239/1426 [1:15:26<7:03:00, 21.38s/it]                                                      {'loss': 0.772, 'learning_rate': 1.9025119257795278e-05, 'epoch': 0.17}
 17%|█▋        | 239/1426 [1:15:26<7:03:00, 21.38s/it] 17%|█▋        | 240/1426 [1:15:45<6:47:35, 20.62s/it]                                                      {'loss': 0.7346, 'learning_rate': 1.9015313091964467e-05, 'epoch': 0.17}
 17%|█▋        | 240/1426 [1:15:45<6:47:35, 20.62s/it] 17%|█▋        | 241/1426 [1:16:05<6:45:46, 20.55s/it]                                                      {'loss': 0.7597, 'learning_rate': 1.9005460406529313e-05, 'epoch': 0.17}
 17%|█▋        | 241/1426 [1:16:05<6:45:46, 20.55s/it] 17%|█▋        | 242/1426 [1:16:27<6:51:49, 20.87s/it]                                                      {'loss': 0.7411, 'learning_rate': 1.8995561252330317e-05, 'epoch': 0.17}
 17%|█▋        | 242/1426 [1:16:27<6:51:49, 20.87s/it] 17%|█▋        | 243/1426 [1:16:45<6:35:36, 20.06s/it]                                                      {'loss': 0.724, 'learning_rate': 1.898561568044776e-05, 'epoch': 0.17}
 17%|█▋        | 243/1426 [1:16:45<6:35:36, 20.06s/it] 17%|█▋        | 244/1426 [1:17:00<6:01:48, 18.37s/it]                                                      {'loss': 0.2503, 'learning_rate': 1.897562374220145e-05, 'epoch': 0.17}
 17%|█▋        | 244/1426 [1:17:00<6:01:48, 18.37s/it] 17%|█▋        | 245/1426 [1:17:19<6:04:46, 18.53s/it]                                                      {'loss': 0.7833, 'learning_rate': 1.896558548915043e-05, 'epoch': 0.17}
 17%|█▋        | 245/1426 [1:17:19<6:04:46, 18.53s/it] 17%|█▋        | 246/1426 [1:17:39<6:15:13, 19.08s/it]                                                      {'loss': 0.7266, 'learning_rate': 1.895550097309275e-05, 'epoch': 0.17}
 17%|█▋        | 246/1426 [1:17:39<6:15:13, 19.08s/it] 17%|█▋        | 247/1426 [1:18:02<6:36:15, 20.17s/it]                                                      {'loss': 0.7186, 'learning_rate': 1.8945370246065166e-05, 'epoch': 0.17}
 17%|█▋        | 247/1426 [1:18:02<6:36:15, 20.17s/it] 17%|█▋        | 248/1426 [1:18:24<6:46:42, 20.72s/it]                                                      {'loss': 0.7466, 'learning_rate': 1.893519336034289e-05, 'epoch': 0.17}
 17%|█▋        | 248/1426 [1:18:24<6:46:42, 20.72s/it] 17%|█▋        | 249/1426 [1:18:44<6:41:26, 20.46s/it]                                                      {'loss': 0.7447, 'learning_rate': 1.8924970368439326e-05, 'epoch': 0.17}
 17%|█▋        | 249/1426 [1:18:44<6:41:26, 20.46s/it] 18%|█▊        | 250/1426 [1:19:06<6:51:22, 20.99s/it]                                                      {'loss': 0.7239, 'learning_rate': 1.891470132310577e-05, 'epoch': 0.18}
 18%|█▊        | 250/1426 [1:19:06<6:51:22, 20.99s/it] 18%|█▊        | 251/1426 [1:19:24<6:37:36, 20.30s/it]                                                      {'loss': 0.7249, 'learning_rate': 1.8904386277331174e-05, 'epoch': 0.18}
 18%|█▊        | 251/1426 [1:19:24<6:37:36, 20.30s/it] 18%|█▊        | 252/1426 [1:19:42<6:22:09, 19.53s/it]                                                      {'loss': 0.7526, 'learning_rate': 1.8894025284341843e-05, 'epoch': 0.18}
 18%|█▊        | 252/1426 [1:19:42<6:22:09, 19.53s/it] 18%|█▊        | 253/1426 [1:20:01<6:18:56, 19.38s/it]                                                      {'loss': 0.7308, 'learning_rate': 1.8883618397601174e-05, 'epoch': 0.18}
 18%|█▊        | 253/1426 [1:20:01<6:18:56, 19.38s/it] 18%|█▊        | 254/1426 [1:20:19<6:09:30, 18.92s/it]                                                      {'loss': 0.7568, 'learning_rate': 1.887316567080939e-05, 'epoch': 0.18}
 18%|█▊        | 254/1426 [1:20:19<6:09:30, 18.92s/it] 18%|█▊        | 255/1426 [1:20:37<6:02:47, 18.59s/it]                                                      {'loss': 0.7312, 'learning_rate': 1.8862667157903246e-05, 'epoch': 0.18}
 18%|█▊        | 255/1426 [1:20:37<6:02:47, 18.59s/it] 18%|█▊        | 256/1426 [1:20:55<5:58:30, 18.39s/it]                                                      {'loss': 0.7354, 'learning_rate': 1.8852122913055742e-05, 'epoch': 0.18}
 18%|█▊        | 256/1426 [1:20:55<5:58:30, 18.39s/it] 18%|█▊        | 257/1426 [1:21:11<5:48:18, 17.88s/it]                                                      {'loss': 0.733, 'learning_rate': 1.884153299067588e-05, 'epoch': 0.18}
 18%|█▊        | 257/1426 [1:21:11<5:48:18, 17.88s/it] 18%|█▊        | 258/1426 [1:21:29<5:46:40, 17.81s/it]                                                      {'loss': 0.7204, 'learning_rate': 1.883089744540835e-05, 'epoch': 0.18}
 18%|█▊        | 258/1426 [1:21:29<5:46:40, 17.81s/it] 18%|█▊        | 259/1426 [1:21:47<5:44:16, 17.70s/it]                                                      {'loss': 0.7469, 'learning_rate': 1.8820216332133257e-05, 'epoch': 0.18}
 18%|█▊        | 259/1426 [1:21:47<5:44:16, 17.70s/it] 18%|█▊        | 260/1426 [1:22:04<5:44:55, 17.75s/it]                                                      {'loss': 0.7438, 'learning_rate': 1.8809489705965847e-05, 'epoch': 0.18}
 18%|█▊        | 260/1426 [1:22:04<5:44:55, 17.75s/it] 18%|█▊        | 261/1426 [1:22:22<5:42:24, 17.63s/it]                                                      {'loss': 0.7374, 'learning_rate': 1.8798717622256207e-05, 'epoch': 0.18}
 18%|█▊        | 261/1426 [1:22:22<5:42:24, 17.63s/it] 18%|█▊        | 262/1426 [1:22:39<5:39:03, 17.48s/it]                                                      {'loss': 0.7341, 'learning_rate': 1.8787900136588996e-05, 'epoch': 0.18}
 18%|█▊        | 262/1426 [1:22:39<5:39:03, 17.48s/it] 18%|█▊        | 263/1426 [1:22:53<5:17:38, 16.39s/it]                                                      {'loss': 0.2542, 'learning_rate': 1.8777037304783144e-05, 'epoch': 0.18}
 18%|█▊        | 263/1426 [1:22:53<5:17:38, 16.39s/it] 19%|█▊        | 264/1426 [1:23:11<5:29:25, 17.01s/it]                                                      {'loss': 0.7018, 'learning_rate': 1.8766129182891573e-05, 'epoch': 0.19}
 19%|█▊        | 264/1426 [1:23:11<5:29:25, 17.01s/it] 19%|█▊        | 265/1426 [1:23:29<5:32:29, 17.18s/it]                                                      {'loss': 0.7597, 'learning_rate': 1.875517582720091e-05, 'epoch': 0.19}
 19%|█▊        | 265/1426 [1:23:29<5:32:29, 17.18s/it] 19%|█▊        | 266/1426 [1:23:47<5:37:39, 17.47s/it]                                                      {'loss': 0.7301, 'learning_rate': 1.8744177294231175e-05, 'epoch': 0.19}
 19%|█▊        | 266/1426 [1:23:47<5:37:39, 17.47s/it] 19%|█▊        | 267/1426 [1:24:05<5:41:16, 17.67s/it]                                                      {'loss': 0.7554, 'learning_rate': 1.8733133640735527e-05, 'epoch': 0.19}
 19%|█▊        | 267/1426 [1:24:05<5:41:16, 17.67s/it] 19%|█▉        | 268/1426 [1:24:22<5:38:20, 17.53s/it]                                                      {'loss': 0.7507, 'learning_rate': 1.872204492369993e-05, 'epoch': 0.19}
 19%|█▉        | 268/1426 [1:24:22<5:38:20, 17.53s/it] 19%|█▉        | 269/1426 [1:24:40<5:38:18, 17.54s/it]                                                      {'loss': 0.7501, 'learning_rate': 1.8710911200342897e-05, 'epoch': 0.19}
 19%|█▉        | 269/1426 [1:24:40<5:38:18, 17.54s/it] 19%|█▉        | 270/1426 [1:24:57<5:35:46, 17.43s/it]                                                      {'loss': 0.7434, 'learning_rate': 1.8699732528115164e-05, 'epoch': 0.19}
 19%|█▉        | 270/1426 [1:24:57<5:35:46, 17.43s/it] 19%|█▉        | 271/1426 [1:25:15<5:36:34, 17.48s/it]                                                      {'loss': 0.7136, 'learning_rate': 1.8688508964699404e-05, 'epoch': 0.19}
 19%|█▉        | 271/1426 [1:25:15<5:36:34, 17.48s/it] 19%|█▉        | 272/1426 [1:25:34<5:46:42, 18.03s/it]                                                      {'loss': 0.7444, 'learning_rate': 1.8677240568009944e-05, 'epoch': 0.19}
 19%|█▉        | 272/1426 [1:25:34<5:46:42, 18.03s/it] 19%|█▉        | 273/1426 [1:25:54<6:00:10, 18.74s/it]                                                      {'loss': 0.7446, 'learning_rate': 1.866592739619245e-05, 'epoch': 0.19}
 19%|█▉        | 273/1426 [1:25:54<6:00:10, 18.74s/it] 19%|█▉        | 274/1426 [1:26:13<5:58:21, 18.66s/it]                                                      {'loss': 0.7561, 'learning_rate': 1.8654569507623618e-05, 'epoch': 0.19}
 19%|█▉        | 274/1426 [1:26:13<5:58:21, 18.66s/it] 19%|█▉        | 275/1426 [1:26:30<5:52:21, 18.37s/it]                                                      {'loss': 0.7042, 'learning_rate': 1.8643166960910898e-05, 'epoch': 0.19}
 19%|█▉        | 275/1426 [1:26:31<5:52:21, 18.37s/it] 19%|█▉        | 276/1426 [1:26:49<5:54:04, 18.47s/it]                                                      {'loss': 0.7045, 'learning_rate': 1.8631719814892182e-05, 'epoch': 0.19}
 19%|█▉        | 276/1426 [1:26:49<5:54:04, 18.47s/it] 19%|█▉        | 277/1426 [1:27:08<5:57:29, 18.67s/it]                                                      {'loss': 0.6983, 'learning_rate': 1.8620228128635484e-05, 'epoch': 0.19}
 19%|█▉        | 277/1426 [1:27:08<5:57:29, 18.67s/it] 19%|█▉        | 278/1426 [1:27:27<5:58:38, 18.74s/it]                                                      {'loss': 0.7247, 'learning_rate': 1.8608691961438657e-05, 'epoch': 0.19}
 19%|█▉        | 278/1426 [1:27:27<5:58:38, 18.74s/it] 20%|█▉        | 279/1426 [1:27:48<6:09:07, 19.31s/it]                                                      {'loss': 0.7383, 'learning_rate': 1.8597111372829082e-05, 'epoch': 0.2}
 20%|█▉        | 279/1426 [1:27:48<6:09:07, 19.31s/it] 20%|█▉        | 280/1426 [1:28:08<6:12:04, 19.48s/it]                                                      {'loss': 0.6896, 'learning_rate': 1.8585486422563343e-05, 'epoch': 0.2}
 20%|█▉        | 280/1426 [1:28:08<6:12:04, 19.48s/it] 20%|█▉        | 281/1426 [1:28:27<6:11:11, 19.45s/it]                                                      {'loss': 0.7138, 'learning_rate': 1.8573817170626952e-05, 'epoch': 0.2}
 20%|█▉        | 281/1426 [1:28:27<6:11:11, 19.45s/it] 20%|█▉        | 282/1426 [1:28:48<6:20:02, 19.93s/it]                                                      {'loss': 0.7496, 'learning_rate': 1.8562103677234007e-05, 'epoch': 0.2}
 20%|█▉        | 282/1426 [1:28:48<6:20:02, 19.93s/it] 20%|█▉        | 283/1426 [1:29:08<6:20:22, 19.97s/it]                                                      {'loss': 0.7168, 'learning_rate': 1.8550346002826898e-05, 'epoch': 0.2}
 20%|█▉        | 283/1426 [1:29:08<6:20:22, 19.97s/it] 20%|█▉        | 284/1426 [1:29:29<6:23:42, 20.16s/it]                                                      {'loss': 0.7363, 'learning_rate': 1.8538544208075997e-05, 'epoch': 0.2}
 20%|█▉        | 284/1426 [1:29:29<6:23:42, 20.16s/it] 20%|█▉        | 285/1426 [1:29:49<6:23:07, 20.15s/it]                                                      {'loss': 0.7306, 'learning_rate': 1.8526698353879328e-05, 'epoch': 0.2}
 20%|█▉        | 285/1426 [1:29:49<6:23:07, 20.15s/it] 20%|██        | 286/1426 [1:30:09<6:24:01, 20.21s/it]                                                      {'loss': 0.7038, 'learning_rate': 1.851480850136228e-05, 'epoch': 0.2}
 20%|██        | 286/1426 [1:30:09<6:24:01, 20.21s/it] 20%|██        | 287/1426 [1:30:28<6:14:08, 19.71s/it]                                                      {'loss': 0.735, 'learning_rate': 1.8502874711877264e-05, 'epoch': 0.2}
 20%|██        | 287/1426 [1:30:28<6:14:08, 19.71s/it] 20%|██        | 288/1426 [1:30:50<6:28:30, 20.48s/it]                                                      {'loss': 0.7205, 'learning_rate': 1.8490897047003418e-05, 'epoch': 0.2}
 20%|██        | 288/1426 [1:30:50<6:28:30, 20.48s/it] 20%|██        | 289/1426 [1:31:10<6:22:22, 20.18s/it]                                                      {'loss': 0.7315, 'learning_rate': 1.847887556854627e-05, 'epoch': 0.2}
 20%|██        | 289/1426 [1:31:10<6:22:22, 20.18s/it] 20%|██        | 290/1426 [1:31:29<6:16:08, 19.87s/it]                                                      {'loss': 0.7373, 'learning_rate': 1.8466810338537434e-05, 'epoch': 0.2}
 20%|██        | 290/1426 [1:31:29<6:16:08, 19.87s/it] 20%|██        | 291/1426 [1:31:50<6:25:53, 20.40s/it]                                                      {'loss': 0.7372, 'learning_rate': 1.845470141923429e-05, 'epoch': 0.2}
 20%|██        | 291/1426 [1:31:50<6:25:53, 20.40s/it] 20%|██        | 292/1426 [1:32:12<6:30:57, 20.69s/it]                                                      {'loss': 0.755, 'learning_rate': 1.8442548873119644e-05, 'epoch': 0.2}
 20%|██        | 292/1426 [1:32:12<6:30:57, 20.69s/it] 21%|██        | 293/1426 [1:32:32<6:30:44, 20.69s/it]                                                      {'loss': 0.7423, 'learning_rate': 1.8430352762901435e-05, 'epoch': 0.21}
 21%|██        | 293/1426 [1:32:32<6:30:44, 20.69s/it] 21%|██        | 294/1426 [1:32:52<6:24:38, 20.39s/it]                                                      {'loss': 0.7415, 'learning_rate': 1.8418113151512386e-05, 'epoch': 0.21}
 21%|██        | 294/1426 [1:32:52<6:24:38, 20.39s/it] 21%|██        | 295/1426 [1:33:12<6:23:42, 20.36s/it]                                                      {'loss': 0.699, 'learning_rate': 1.840583010210969e-05, 'epoch': 0.21}
 21%|██        | 295/1426 [1:33:12<6:23:42, 20.36s/it] 21%|██        | 296/1426 [1:33:26<5:47:40, 18.46s/it]                                                      {'loss': 0.2508, 'learning_rate': 1.8393503678074686e-05, 'epoch': 0.21}
 21%|██        | 296/1426 [1:33:26<5:47:40, 18.46s/it] 21%|██        | 297/1426 [1:33:47<5:58:25, 19.05s/it]                                                      {'loss': 0.7215, 'learning_rate': 1.8381133943012527e-05, 'epoch': 0.21}
 21%|██        | 297/1426 [1:33:47<5:58:25, 19.05s/it] 21%|██        | 298/1426 [1:34:08<6:08:56, 19.62s/it]                                                      {'loss': 0.7304, 'learning_rate': 1.8368720960751854e-05, 'epoch': 0.21}
 21%|██        | 298/1426 [1:34:08<6:08:56, 19.62s/it] 21%|██        | 299/1426 [1:34:31<6:31:14, 20.83s/it]                                                      {'loss': 0.7483, 'learning_rate': 1.835626479534446e-05, 'epoch': 0.21}
 21%|██        | 299/1426 [1:34:32<6:31:14, 20.83s/it] 21%|██        | 300/1426 [1:34:52<6:30:07, 20.79s/it]                                                      {'loss': 0.7335, 'learning_rate': 1.8343765511064982e-05, 'epoch': 0.21}
 21%|██        | 300/1426 [1:34:52<6:30:07, 20.79s/it] 21%|██        | 301/1426 [1:35:12<6:21:56, 20.37s/it]                                                      {'loss': 0.7117, 'learning_rate': 1.8331223172410535e-05, 'epoch': 0.21}
 21%|██        | 301/1426 [1:35:12<6:21:56, 20.37s/it] 21%|██        | 302/1426 [1:35:26<5:49:16, 18.64s/it]                                                      {'loss': 0.2432, 'learning_rate': 1.831863784410041e-05, 'epoch': 0.21}
 21%|██        | 302/1426 [1:35:26<5:49:16, 18.64s/it] 21%|██        | 303/1426 [1:35:40<5:24:13, 17.32s/it]                                                      {'loss': 0.2503, 'learning_rate': 1.8306009591075723e-05, 'epoch': 0.21}
 21%|██        | 303/1426 [1:35:40<5:24:13, 17.32s/it] 21%|██▏       | 304/1426 [1:35:58<5:26:40, 17.47s/it]                                                      {'loss': 0.7576, 'learning_rate': 1.8293338478499084e-05, 'epoch': 0.21}
 21%|██▏       | 304/1426 [1:35:58<5:26:40, 17.47s/it] 21%|██▏       | 305/1426 [1:36:15<5:23:57, 17.34s/it]                                                      {'loss': 0.7345, 'learning_rate': 1.8280624571754258e-05, 'epoch': 0.21}
 21%|██▏       | 305/1426 [1:36:15<5:23:57, 17.34s/it] 21%|██▏       | 306/1426 [1:36:29<5:04:40, 16.32s/it]                                                      {'loss': 0.2447, 'learning_rate': 1.8267867936445835e-05, 'epoch': 0.21}
 21%|██▏       | 306/1426 [1:36:29<5:04:40, 16.32s/it] 22%|██▏       | 307/1426 [1:36:51<5:33:32, 17.88s/it]                                                      {'loss': 0.756, 'learning_rate': 1.8255068638398893e-05, 'epoch': 0.22}
 22%|██▏       | 307/1426 [1:36:51<5:33:32, 17.88s/it]this iter is wrong in something... skip...
 22%|██▏       | 308/1426 [1:37:10<5:39:06, 18.20s/it]                                                      {'loss': 0.7585, 'learning_rate': 1.8242226743658638e-05, 'epoch': 0.22}
 22%|██▏       | 308/1426 [1:37:10<5:39:06, 18.20s/it] 22%|██▏       | 309/1426 [1:37:29<5:42:35, 18.40s/it]                                                      {'loss': 0.7292, 'learning_rate': 1.8229342318490087e-05, 'epoch': 0.22}
 22%|██▏       | 309/1426 [1:37:29<5:42:35, 18.40s/it] 22%|██▏       | 310/1426 [1:37:46<5:35:36, 18.04s/it]                                                      {'loss': 0.7374, 'learning_rate': 1.821641542937772e-05, 'epoch': 0.22}
 22%|██▏       | 310/1426 [1:37:46<5:35:36, 18.04s/it] 22%|██▏       | 311/1426 [1:38:06<5:45:44, 18.60s/it]                                                      {'loss': 0.7266, 'learning_rate': 1.8203446143025127e-05, 'epoch': 0.22}
 22%|██▏       | 311/1426 [1:38:06<5:45:44, 18.60s/it] 22%|██▏       | 312/1426 [1:38:23<5:39:43, 18.30s/it]                                                      {'loss': 0.7385, 'learning_rate': 1.8190434526354667e-05, 'epoch': 0.22}
 22%|██▏       | 312/1426 [1:38:23<5:39:43, 18.30s/it] 22%|██▏       | 313/1426 [1:38:42<5:44:15, 18.56s/it]                                                      {'loss': 0.7308, 'learning_rate': 1.817738064650714e-05, 'epoch': 0.22}
 22%|██▏       | 313/1426 [1:38:42<5:44:15, 18.56s/it] 22%|██▏       | 314/1426 [1:39:00<5:37:57, 18.24s/it]                                                      {'loss': 0.7312, 'learning_rate': 1.8164284570841417e-05, 'epoch': 0.22}
 22%|██▏       | 314/1426 [1:39:00<5:37:57, 18.24s/it] 22%|██▏       | 315/1426 [1:39:16<5:24:48, 17.54s/it]                                                      {'loss': 0.247, 'learning_rate': 1.8151146366934096e-05, 'epoch': 0.22}
 22%|██▏       | 315/1426 [1:39:16<5:24:48, 17.54s/it] 22%|██▏       | 316/1426 [1:39:34<5:30:23, 17.86s/it]                                                      {'loss': 0.7267, 'learning_rate': 1.8137966102579178e-05, 'epoch': 0.22}
 22%|██▏       | 316/1426 [1:39:34<5:30:23, 17.86s/it] 22%|██▏       | 317/1426 [1:39:52<5:28:05, 17.75s/it]                                                      {'loss': 0.7142, 'learning_rate': 1.812474384578768e-05, 'epoch': 0.22}
 22%|██▏       | 317/1426 [1:39:52<5:28:05, 17.75s/it] 22%|██▏       | 318/1426 [1:40:09<5:24:48, 17.59s/it]                                                      {'loss': 0.7484, 'learning_rate': 1.811147966478731e-05, 'epoch': 0.22}
 22%|██▏       | 318/1426 [1:40:09<5:24:48, 17.59s/it] 22%|██▏       | 319/1426 [1:40:26<5:22:25, 17.48s/it]                                                      {'loss': 0.7638, 'learning_rate': 1.8098173628022118e-05, 'epoch': 0.22}
 22%|██▏       | 319/1426 [1:40:26<5:22:25, 17.48s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (4901 > 3072). Running this sequence through the model will result in indexing errors
 22%|██▏       | 320/1426 [1:40:45<5:28:44, 17.83s/it]                                                      {'loss': 0.7203, 'learning_rate': 1.8084825804152113e-05, 'epoch': 0.22}
 22%|██▏       | 320/1426 [1:40:45<5:28:44, 17.83s/it] 23%|██▎       | 321/1426 [1:41:03<5:30:04, 17.92s/it]                                                      {'loss': 0.6906, 'learning_rate': 1.807143626205294e-05, 'epoch': 0.23}
 23%|██▎       | 321/1426 [1:41:03<5:30:04, 17.92s/it] 23%|██▎       | 322/1426 [1:41:17<5:09:56, 16.84s/it]                                                      {'loss': 0.2317, 'learning_rate': 1.8058005070815515e-05, 'epoch': 0.23}
 23%|██▎       | 322/1426 [1:41:18<5:09:56, 16.84s/it] 23%|██▎       | 323/1426 [1:41:37<5:23:22, 17.59s/it]                                                      {'loss': 0.7332, 'learning_rate': 1.804453229974566e-05, 'epoch': 0.23}
 23%|██▎       | 323/1426 [1:41:37<5:23:22, 17.59s/it] 23%|██▎       | 324/1426 [1:41:54<5:22:19, 17.55s/it]                                                      {'loss': 0.7523, 'learning_rate': 1.8031018018363745e-05, 'epoch': 0.23}
 23%|██▎       | 324/1426 [1:41:54<5:22:19, 17.55s/it] 23%|██▎       | 325/1426 [1:42:15<5:41:31, 18.61s/it]                                                      {'loss': 0.744, 'learning_rate': 1.8017462296404358e-05, 'epoch': 0.23}
 23%|██▎       | 325/1426 [1:42:15<5:41:31, 18.61s/it] 23%|██▎       | 326/1426 [1:42:33<5:38:24, 18.46s/it]                                                      {'loss': 0.7352, 'learning_rate': 1.8003865203815903e-05, 'epoch': 0.23}
 23%|██▎       | 326/1426 [1:42:33<5:38:24, 18.46s/it] 23%|██▎       | 327/1426 [1:42:53<5:41:38, 18.65s/it]                                                      {'loss': 0.7262, 'learning_rate': 1.7990226810760266e-05, 'epoch': 0.23}
 23%|██▎       | 327/1426 [1:42:53<5:41:38, 18.65s/it] 23%|██▎       | 328/1426 [1:43:10<5:36:23, 18.38s/it]                                                      {'loss': 0.7087, 'learning_rate': 1.7976547187612446e-05, 'epoch': 0.23}
 23%|██▎       | 328/1426 [1:43:10<5:36:23, 18.38s/it] 23%|██▎       | 329/1426 [1:43:28<5:29:46, 18.04s/it]                                                      {'loss': 0.7305, 'learning_rate': 1.7962826404960192e-05, 'epoch': 0.23}
 23%|██▎       | 329/1426 [1:43:28<5:29:46, 18.04s/it] 23%|██▎       | 330/1426 [1:43:46<5:29:36, 18.04s/it]                                                      {'loss': 0.739, 'learning_rate': 1.794906453360364e-05, 'epoch': 0.23}
 23%|██▎       | 330/1426 [1:43:46<5:29:36, 18.04s/it] 23%|██▎       | 331/1426 [1:44:03<5:25:03, 17.81s/it]                                                      {'loss': 0.7174, 'learning_rate': 1.7935261644554943e-05, 'epoch': 0.23}
 23%|██▎       | 331/1426 [1:44:03<5:25:03, 17.81s/it] 23%|██▎       | 332/1426 [1:44:20<5:21:35, 17.64s/it]                                                      {'loss': 0.7307, 'learning_rate': 1.792141780903791e-05, 'epoch': 0.23}
 23%|██▎       | 332/1426 [1:44:20<5:21:35, 17.64s/it] 23%|██▎       | 333/1426 [1:44:37<5:16:23, 17.37s/it]                                                      {'loss': 0.7309, 'learning_rate': 1.7907533098487636e-05, 'epoch': 0.23}
 23%|██▎       | 333/1426 [1:44:37<5:16:23, 17.37s/it] 23%|██▎       | 334/1426 [1:44:55<5:22:58, 17.75s/it]                                                      {'loss': 0.738, 'learning_rate': 1.789360758455014e-05, 'epoch': 0.23}
 23%|██▎       | 334/1426 [1:44:56<5:22:58, 17.75s/it] 23%|██▎       | 335/1426 [1:45:13<5:20:14, 17.61s/it]                                                      {'loss': 0.7172, 'learning_rate': 1.7879641339081976e-05, 'epoch': 0.23}
 23%|██▎       | 335/1426 [1:45:13<5:20:14, 17.61s/it] 24%|██▎       | 336/1426 [1:45:31<5:22:41, 17.76s/it]                                                      {'loss': 0.718, 'learning_rate': 1.7865634434149885e-05, 'epoch': 0.24}
 24%|██▎       | 336/1426 [1:45:31<5:22:41, 17.76s/it] 24%|██▎       | 337/1426 [1:45:48<5:20:44, 17.67s/it]                                                      {'loss': 0.7281, 'learning_rate': 1.785158694203041e-05, 'epoch': 0.24}
 24%|██▎       | 337/1426 [1:45:48<5:20:44, 17.67s/it] 24%|██▎       | 338/1426 [1:46:10<5:42:35, 18.89s/it]                                                      {'loss': 0.7325, 'learning_rate': 1.7837498935209536e-05, 'epoch': 0.24}
 24%|██▎       | 338/1426 [1:46:10<5:42:35, 18.89s/it] 24%|██▍       | 339/1426 [1:46:29<5:40:55, 18.82s/it]                                                      {'loss': 0.7162, 'learning_rate': 1.7823370486382284e-05, 'epoch': 0.24}
 24%|██▍       | 339/1426 [1:46:29<5:40:55, 18.82s/it] 24%|██▍       | 340/1426 [1:46:43<5:15:17, 17.42s/it]                                                      {'loss': 0.2388, 'learning_rate': 1.7809201668452386e-05, 'epoch': 0.24}
 24%|██▍       | 340/1426 [1:46:43<5:15:17, 17.42s/it] 24%|██▍       | 341/1426 [1:47:00<5:13:36, 17.34s/it]                                                      {'loss': 0.7629, 'learning_rate': 1.779499255453186e-05, 'epoch': 0.24}
 24%|██▍       | 341/1426 [1:47:00<5:13:36, 17.34s/it] 24%|██▍       | 342/1426 [1:47:17<5:12:37, 17.30s/it]                                                      {'loss': 0.7154, 'learning_rate': 1.7780743217940664e-05, 'epoch': 0.24}
 24%|██▍       | 342/1426 [1:47:17<5:12:37, 17.30s/it] 24%|██▍       | 343/1426 [1:47:40<5:40:02, 18.84s/it]                                                      {'loss': 0.7418, 'learning_rate': 1.7766453732206306e-05, 'epoch': 0.24}
 24%|██▍       | 343/1426 [1:47:40<5:40:02, 18.84s/it] 24%|██▍       | 344/1426 [1:48:01<5:52:54, 19.57s/it]                                                      {'loss': 0.7379, 'learning_rate': 1.7752124171063465e-05, 'epoch': 0.24}
 24%|██▍       | 344/1426 [1:48:01<5:52:54, 19.57s/it] 24%|██▍       | 345/1426 [1:48:21<5:54:57, 19.70s/it]                                                      {'loss': 0.7248, 'learning_rate': 1.773775460845361e-05, 'epoch': 0.24}
 24%|██▍       | 345/1426 [1:48:21<5:54:57, 19.70s/it] 24%|██▍       | 346/1426 [1:48:38<5:42:30, 19.03s/it]                                                      {'loss': 0.715, 'learning_rate': 1.7723345118524632e-05, 'epoch': 0.24}
 24%|██▍       | 346/1426 [1:48:38<5:42:30, 19.03s/it] 24%|██▍       | 347/1426 [1:48:58<5:44:52, 19.18s/it]                                                      {'loss': 0.7345, 'learning_rate': 1.7708895775630427e-05, 'epoch': 0.24}
 24%|██▍       | 347/1426 [1:48:58<5:44:52, 19.18s/it] 24%|██▍       | 348/1426 [1:49:17<5:44:31, 19.18s/it]                                                      {'loss': 0.725, 'learning_rate': 1.7694406654330557e-05, 'epoch': 0.24}
 24%|██▍       | 348/1426 [1:49:17<5:44:31, 19.18s/it] 24%|██▍       | 349/1426 [1:49:40<6:05:04, 20.34s/it]                                                      {'loss': 0.7087, 'learning_rate': 1.767987782938983e-05, 'epoch': 0.24}
 24%|██▍       | 349/1426 [1:49:40<6:05:04, 20.34s/it] 25%|██▍       | 350/1426 [1:50:00<6:03:18, 20.26s/it]                                                      {'loss': 0.7104, 'learning_rate': 1.7665309375777934e-05, 'epoch': 0.25}
 25%|██▍       | 350/1426 [1:50:00<6:03:18, 20.26s/it] 25%|██▍       | 351/1426 [1:50:20<5:58:18, 20.00s/it]                                                      {'loss': 0.7379, 'learning_rate': 1.765070136866904e-05, 'epoch': 0.25}
 25%|██▍       | 351/1426 [1:50:20<5:58:18, 20.00s/it] 25%|██▍       | 352/1426 [1:50:41<6:07:09, 20.51s/it]                                                      {'loss': 0.7248, 'learning_rate': 1.763605388344142e-05, 'epoch': 0.25}
 25%|██▍       | 352/1426 [1:50:41<6:07:09, 20.51s/it] 25%|██▍       | 353/1426 [1:51:00<5:55:35, 19.88s/it]                                                      {'loss': 0.723, 'learning_rate': 1.7621366995677052e-05, 'epoch': 0.25}
 25%|██▍       | 353/1426 [1:51:00<5:55:35, 19.88s/it] 25%|██▍       | 354/1426 [1:51:21<6:01:12, 20.22s/it]                                                      {'loss': 0.7323, 'learning_rate': 1.7606640781161236e-05, 'epoch': 0.25}
 25%|██▍       | 354/1426 [1:51:21<6:01:12, 20.22s/it] 25%|██▍       | 355/1426 [1:51:41<6:03:24, 20.36s/it]                                                      {'loss': 0.6855, 'learning_rate': 1.7591875315882203e-05, 'epoch': 0.25}
 25%|██▍       | 355/1426 [1:51:41<6:03:24, 20.36s/it] 25%|██▍       | 356/1426 [1:52:01<6:00:50, 20.23s/it]                                                      {'loss': 0.7008, 'learning_rate': 1.7577070676030717e-05, 'epoch': 0.25}
 25%|██▍       | 356/1426 [1:52:01<6:00:50, 20.23s/it] 25%|██▌       | 357/1426 [1:52:15<5:27:34, 18.39s/it]                                                      {'loss': 0.2472, 'learning_rate': 1.7562226937999685e-05, 'epoch': 0.25}
 25%|██▌       | 357/1426 [1:52:15<5:27:34, 18.39s/it] 25%|██▌       | 358/1426 [1:52:38<5:50:06, 19.67s/it]                                                      {'loss': 0.7288, 'learning_rate': 1.754734417838377e-05, 'epoch': 0.25}
 25%|██▌       | 358/1426 [1:52:38<5:50:06, 19.67s/it] 25%|██▌       | 359/1426 [1:52:58<5:52:00, 19.79s/it]                                                      {'loss': 0.7388, 'learning_rate': 1.753242247397898e-05, 'epoch': 0.25}
 25%|██▌       | 359/1426 [1:52:58<5:52:00, 19.79s/it] 25%|██▌       | 360/1426 [1:53:19<5:55:30, 20.01s/it]                                                      {'loss': 0.7503, 'learning_rate': 1.7517461901782283e-05, 'epoch': 0.25}
 25%|██▌       | 360/1426 [1:53:19<5:55:30, 20.01s/it] 25%|██▌       | 361/1426 [1:53:39<5:56:40, 20.09s/it]                                                      {'loss': 0.7372, 'learning_rate': 1.7502462538991208e-05, 'epoch': 0.25}
 25%|██▌       | 361/1426 [1:53:39<5:56:40, 20.09s/it] 25%|██▌       | 362/1426 [1:53:56<5:42:10, 19.30s/it]                                                      {'loss': 0.7442, 'learning_rate': 1.748742446300345e-05, 'epoch': 0.25}
 25%|██▌       | 362/1426 [1:53:56<5:42:10, 19.30s/it] 25%|██▌       | 363/1426 [1:54:16<5:41:47, 19.29s/it]                                                      {'loss': 0.7002, 'learning_rate': 1.7472347751416457e-05, 'epoch': 0.25}
 25%|██▌       | 363/1426 [1:54:16<5:41:47, 19.29s/it] 26%|██▌       | 364/1426 [1:54:38<5:59:03, 20.29s/it]                                                      {'loss': 0.7389, 'learning_rate': 1.7457232482027057e-05, 'epoch': 0.26}
 26%|██▌       | 364/1426 [1:54:38<5:59:03, 20.29s/it] 26%|██▌       | 365/1426 [1:54:58<5:55:26, 20.10s/it]                                                      {'loss': 0.7502, 'learning_rate': 1.744207873283102e-05, 'epoch': 0.26}
 26%|██▌       | 365/1426 [1:54:58<5:55:26, 20.10s/it] 26%|██▌       | 366/1426 [1:55:16<5:43:43, 19.46s/it]                                                      {'loss': 0.7763, 'learning_rate': 1.7426886582022677e-05, 'epoch': 0.26}
 26%|██▌       | 366/1426 [1:55:16<5:43:43, 19.46s/it] 26%|██▌       | 367/1426 [1:55:36<5:47:33, 19.69s/it]                                                      {'loss': 0.6969, 'learning_rate': 1.7411656107994523e-05, 'epoch': 0.26}
 26%|██▌       | 367/1426 [1:55:36<5:47:33, 19.69s/it] 26%|██▌       | 368/1426 [1:55:53<5:31:48, 18.82s/it]                                                      {'loss': 0.7253, 'learning_rate': 1.7396387389336803e-05, 'epoch': 0.26}
 26%|██▌       | 368/1426 [1:55:53<5:31:48, 18.82s/it] 26%|██▌       | 369/1426 [1:56:14<5:41:40, 19.39s/it]                                                      {'loss': 0.7206, 'learning_rate': 1.7381080504837095e-05, 'epoch': 0.26}
 26%|██▌       | 369/1426 [1:56:14<5:41:40, 19.39s/it] 26%|██▌       | 370/1426 [1:56:31<5:31:26, 18.83s/it]                                                      {'loss': 0.7002, 'learning_rate': 1.736573553347993e-05, 'epoch': 0.26}
 26%|██▌       | 370/1426 [1:56:31<5:31:26, 18.83s/it] 26%|██▌       | 371/1426 [1:56:50<5:31:08, 18.83s/it]                                                      {'loss': 0.7024, 'learning_rate': 1.7350352554446355e-05, 'epoch': 0.26}
 26%|██▌       | 371/1426 [1:56:50<5:31:08, 18.83s/it] 26%|██▌       | 372/1426 [1:57:08<5:24:25, 18.47s/it]                                                      {'loss': 0.75, 'learning_rate': 1.733493164711355e-05, 'epoch': 0.26}
 26%|██▌       | 372/1426 [1:57:08<5:24:25, 18.47s/it] 26%|██▌       | 373/1426 [1:57:26<5:24:29, 18.49s/it]                                                      {'loss': 0.6947, 'learning_rate': 1.7319472891054404e-05, 'epoch': 0.26}
 26%|██▌       | 373/1426 [1:57:26<5:24:29, 18.49s/it] 26%|██▌       | 374/1426 [1:57:45<5:24:25, 18.50s/it]                                                      {'loss': 0.7049, 'learning_rate': 1.7303976366037106e-05, 'epoch': 0.26}
 26%|██▌       | 374/1426 [1:57:45<5:24:25, 18.50s/it] 26%|██▋       | 375/1426 [1:58:03<5:23:20, 18.46s/it]                                                      {'loss': 0.7392, 'learning_rate': 1.7288442152024745e-05, 'epoch': 0.26}
 26%|██▋       | 375/1426 [1:58:03<5:23:20, 18.46s/it] 26%|██▋       | 376/1426 [1:58:21<5:21:40, 18.38s/it]                                                      {'loss': 0.7429, 'learning_rate': 1.7272870329174872e-05, 'epoch': 0.26}
 26%|██▋       | 376/1426 [1:58:21<5:21:40, 18.38s/it] 26%|██▋       | 377/1426 [1:58:39<5:19:38, 18.28s/it]                                                      {'loss': 0.7174, 'learning_rate': 1.725726097783911e-05, 'epoch': 0.26}
 26%|██▋       | 377/1426 [1:58:39<5:19:38, 18.28s/it] 27%|██▋       | 378/1426 [1:58:57<5:17:15, 18.16s/it]                                                      {'loss': 0.7413, 'learning_rate': 1.724161417856273e-05, 'epoch': 0.26}
 27%|██▋       | 378/1426 [1:58:57<5:17:15, 18.16s/it] 27%|██▋       | 379/1426 [1:59:14<5:09:58, 17.76s/it]                                                      {'loss': 0.7197, 'learning_rate': 1.7225930012084246e-05, 'epoch': 0.27}
 27%|██▋       | 379/1426 [1:59:14<5:09:58, 17.76s/it] 27%|██▋       | 380/1426 [1:59:33<5:16:37, 18.16s/it]                                                      {'loss': 0.7433, 'learning_rate': 1.7210208559334973e-05, 'epoch': 0.27}
 27%|██▋       | 380/1426 [1:59:33<5:16:37, 18.16s/it] 27%|██▋       | 381/1426 [1:59:51<5:14:02, 18.03s/it]                                                      {'loss': 0.727, 'learning_rate': 1.7194449901438636e-05, 'epoch': 0.27}
 27%|██▋       | 381/1426 [1:59:51<5:14:02, 18.03s/it] 27%|██▋       | 382/1426 [2:00:08<5:11:04, 17.88s/it]                                                      {'loss': 0.7338, 'learning_rate': 1.7178654119710946e-05, 'epoch': 0.27}
 27%|██▋       | 382/1426 [2:00:08<5:11:04, 17.88s/it] 27%|██▋       | 383/1426 [2:00:27<5:16:11, 18.19s/it]                                                      {'loss': 0.7377, 'learning_rate': 1.716282129565916e-05, 'epoch': 0.27}
 27%|██▋       | 383/1426 [2:00:27<5:16:11, 18.19s/it] 27%|██▋       | 384/1426 [2:00:44<5:08:55, 17.79s/it]                                                      {'loss': 0.732, 'learning_rate': 1.7146951510981685e-05, 'epoch': 0.27}
 27%|██▋       | 384/1426 [2:00:44<5:08:55, 17.79s/it] 27%|██▋       | 385/1426 [2:01:04<5:16:33, 18.25s/it]                                                      {'loss': 0.7154, 'learning_rate': 1.713104484756765e-05, 'epoch': 0.27}
 27%|██▋       | 385/1426 [2:01:04<5:16:33, 18.25s/it] 27%|██▋       | 386/1426 [2:01:21<5:13:49, 18.10s/it]                                                      {'loss': 0.7128, 'learning_rate': 1.7115101387496476e-05, 'epoch': 0.27}
 27%|██▋       | 386/1426 [2:01:21<5:13:49, 18.10s/it] 27%|██▋       | 387/1426 [2:01:39<5:11:52, 18.01s/it]                                                      {'loss': 0.7206, 'learning_rate': 1.7099121213037454e-05, 'epoch': 0.27}
 27%|██▋       | 387/1426 [2:01:39<5:11:52, 18.01s/it] 27%|██▋       | 388/1426 [2:01:57<5:10:29, 17.95s/it]                                                      {'loss': 0.7118, 'learning_rate': 1.7083104406649337e-05, 'epoch': 0.27}
 27%|██▋       | 388/1426 [2:01:57<5:10:29, 17.95s/it] 27%|██▋       | 389/1426 [2:02:15<5:12:21, 18.07s/it]                                                      {'loss': 0.7135, 'learning_rate': 1.7067051050979885e-05, 'epoch': 0.27}
 27%|██▋       | 389/1426 [2:02:15<5:12:21, 18.07s/it] 27%|██▋       | 390/1426 [2:02:34<5:15:44, 18.29s/it]                                                      {'loss': 0.7075, 'learning_rate': 1.7050961228865463e-05, 'epoch': 0.27}
 27%|██▋       | 390/1426 [2:02:34<5:15:44, 18.29s/it] 27%|██▋       | 391/1426 [2:02:52<5:13:20, 18.16s/it]                                                      {'loss': 0.6972, 'learning_rate': 1.7034835023330598e-05, 'epoch': 0.27}
 27%|██▋       | 391/1426 [2:02:52<5:13:20, 18.16s/it] 27%|██▋       | 392/1426 [2:03:12<5:22:32, 18.72s/it]                                                      {'loss': 0.7265, 'learning_rate': 1.7018672517587577e-05, 'epoch': 0.27}
 27%|██▋       | 392/1426 [2:03:12<5:22:32, 18.72s/it] 28%|██▊       | 393/1426 [2:03:29<5:13:48, 18.23s/it]                                                      {'loss': 0.706, 'learning_rate': 1.7002473795035975e-05, 'epoch': 0.28}
 28%|██▊       | 393/1426 [2:03:29<5:13:48, 18.23s/it] 28%|██▊       | 394/1426 [2:03:48<5:18:30, 18.52s/it]                                                      {'loss': 0.7339, 'learning_rate': 1.698623893926226e-05, 'epoch': 0.28}
 28%|██▊       | 394/1426 [2:03:48<5:18:30, 18.52s/it] 28%|██▊       | 395/1426 [2:04:11<5:42:38, 19.94s/it]                                                      {'loss': 0.7171, 'learning_rate': 1.696996803403935e-05, 'epoch': 0.28}
 28%|██▊       | 395/1426 [2:04:11<5:42:38, 19.94s/it] 28%|██▊       | 396/1426 [2:04:32<5:42:49, 19.97s/it]                                                      {'loss': 0.7227, 'learning_rate': 1.6953661163326184e-05, 'epoch': 0.28}
 28%|██▊       | 396/1426 [2:04:32<5:42:49, 19.97s/it] 28%|██▊       | 397/1426 [2:04:49<5:31:06, 19.31s/it]                                                      {'loss': 0.7425, 'learning_rate': 1.6937318411267266e-05, 'epoch': 0.28}
 28%|██▊       | 397/1426 [2:04:49<5:31:06, 19.31s/it] 28%|██▊       | 398/1426 [2:05:12<5:46:49, 20.24s/it]                                                      {'loss': 0.7175, 'learning_rate': 1.6920939862192275e-05, 'epoch': 0.28}
 28%|██▊       | 398/1426 [2:05:12<5:46:49, 20.24s/it] 28%|██▊       | 399/1426 [2:05:32<5:48:53, 20.38s/it]                                                      {'loss': 0.7089, 'learning_rate': 1.690452560061559e-05, 'epoch': 0.28}
 28%|██▊       | 399/1426 [2:05:32<5:48:53, 20.38s/it] 28%|██▊       | 400/1426 [2:05:51<5:41:37, 19.98s/it]                                                      {'loss': 0.7038, 'learning_rate': 1.6888075711235873e-05, 'epoch': 0.28}
 28%|██▊       | 400/1426 [2:05:51<5:41:37, 19.98s/it] 28%|██▊       | 401/1426 [2:06:12<5:43:24, 20.10s/it]                                                      {'loss': 0.7189, 'learning_rate': 1.687159027893563e-05, 'epoch': 0.28}
 28%|██▊       | 401/1426 [2:06:12<5:43:24, 20.10s/it] 28%|██▊       | 402/1426 [2:06:34<5:53:33, 20.72s/it]                                                      {'loss': 0.733, 'learning_rate': 1.6855069388780766e-05, 'epoch': 0.28}
 28%|██▊       | 402/1426 [2:06:34<5:53:33, 20.72s/it] 28%|██▊       | 403/1426 [2:06:56<6:01:20, 21.19s/it]                                                      {'loss': 0.7106, 'learning_rate': 1.6838513126020153e-05, 'epoch': 0.28}
 28%|██▊       | 403/1426 [2:06:56<6:01:20, 21.19s/it] 28%|██▊       | 404/1426 [2:07:17<5:59:47, 21.12s/it]                                                      {'loss': 0.7195, 'learning_rate': 1.6821921576085186e-05, 'epoch': 0.28}
 28%|██▊       | 404/1426 [2:07:17<5:59:47, 21.12s/it] 28%|██▊       | 405/1426 [2:07:38<5:57:35, 21.01s/it]                                                      {'loss': 0.7152, 'learning_rate': 1.6805294824589353e-05, 'epoch': 0.28}
 28%|██▊       | 405/1426 [2:07:38<5:57:35, 21.01s/it] 28%|██▊       | 406/1426 [2:07:58<5:54:31, 20.85s/it]                                                      {'loss': 0.7337, 'learning_rate': 1.6788632957327772e-05, 'epoch': 0.28}
 28%|██▊       | 406/1426 [2:07:59<5:54:31, 20.85s/it] 29%|██▊       | 407/1426 [2:08:18<5:49:02, 20.55s/it]                                                      {'loss': 0.7151, 'learning_rate': 1.6771936060276765e-05, 'epoch': 0.29}
 29%|██▊       | 407/1426 [2:08:18<5:49:02, 20.55s/it] 29%|██▊       | 408/1426 [2:08:40<5:53:00, 20.81s/it]                                                      {'loss': 0.7145, 'learning_rate': 1.675520421959342e-05, 'epoch': 0.29}
 29%|██▊       | 408/1426 [2:08:40<5:53:00, 20.81s/it] 29%|██▊       | 409/1426 [2:09:01<5:53:16, 20.84s/it]                                                      {'loss': 0.7119, 'learning_rate': 1.6738437521615117e-05, 'epoch': 0.29}
 29%|██▊       | 409/1426 [2:09:01<5:53:16, 20.84s/it] 29%|██▉       | 410/1426 [2:09:21<5:51:08, 20.74s/it]                                                      {'loss': 0.6992, 'learning_rate': 1.672163605285912e-05, 'epoch': 0.29}
 29%|██▉       | 410/1426 [2:09:21<5:51:08, 20.74s/it] 29%|██▉       | 411/1426 [2:09:45<6:04:29, 21.55s/it]                                                      {'loss': 0.7325, 'learning_rate': 1.67047999000221e-05, 'epoch': 0.29}
 29%|██▉       | 411/1426 [2:09:45<6:04:29, 21.55s/it] 29%|██▉       | 412/1426 [2:10:05<5:59:06, 21.25s/it]                                                      {'loss': 0.7232, 'learning_rate': 1.668792914997971e-05, 'epoch': 0.29}
 29%|██▉       | 412/1426 [2:10:05<5:59:06, 21.25s/it] 29%|██▉       | 413/1426 [2:10:25<5:51:12, 20.80s/it]                                                      {'loss': 0.6971, 'learning_rate': 1.6671023889786115e-05, 'epoch': 0.29}
 29%|██▉       | 413/1426 [2:10:25<5:51:12, 20.80s/it] 29%|██▉       | 414/1426 [2:10:48<6:02:08, 21.47s/it]                                                      {'loss': 0.7162, 'learning_rate': 1.6654084206673572e-05, 'epoch': 0.29}
 29%|██▉       | 414/1426 [2:10:48<6:02:08, 21.47s/it] 29%|██▉       | 415/1426 [2:11:02<5:25:23, 19.31s/it]                                                      {'loss': 0.2624, 'learning_rate': 1.6637110188051944e-05, 'epoch': 0.29}
 29%|██▉       | 415/1426 [2:11:02<5:25:23, 19.31s/it] 29%|██▉       | 416/1426 [2:11:22<5:26:26, 19.39s/it]                                                      {'loss': 0.7168, 'learning_rate': 1.6620101921508275e-05, 'epoch': 0.29}
 29%|██▉       | 416/1426 [2:11:22<5:26:26, 19.39s/it] 29%|██▉       | 417/1426 [2:11:40<5:19:58, 19.03s/it]                                                      {'loss': 0.7285, 'learning_rate': 1.660305949480634e-05, 'epoch': 0.29}
 29%|██▉       | 417/1426 [2:11:40<5:19:58, 19.03s/it] 29%|██▉       | 418/1426 [2:11:58<5:16:38, 18.85s/it]                                                      {'loss': 0.7086, 'learning_rate': 1.6585982995886165e-05, 'epoch': 0.29}
 29%|██▉       | 418/1426 [2:11:58<5:16:38, 18.85s/it] 29%|██▉       | 419/1426 [2:12:18<5:21:06, 19.13s/it]                                                      {'loss': 0.7163, 'learning_rate': 1.6568872512863603e-05, 'epoch': 0.29}
 29%|██▉       | 419/1426 [2:12:18<5:21:06, 19.13s/it] 29%|██▉       | 420/1426 [2:12:35<5:10:30, 18.52s/it]                                                      {'loss': 0.7148, 'learning_rate': 1.655172813402986e-05, 'epoch': 0.29}
 29%|██▉       | 420/1426 [2:12:35<5:10:30, 18.52s/it] 30%|██▉       | 421/1426 [2:12:57<5:24:56, 19.40s/it]                                                      {'loss': 0.7315, 'learning_rate': 1.653454994785106e-05, 'epoch': 0.3}
 30%|██▉       | 421/1426 [2:12:57<5:24:56, 19.40s/it] 30%|██▉       | 422/1426 [2:13:15<5:17:48, 18.99s/it]                                                      {'loss': 0.7027, 'learning_rate': 1.6517338042967758e-05, 'epoch': 0.3}
 30%|██▉       | 422/1426 [2:13:15<5:17:48, 18.99s/it] 30%|██▉       | 423/1426 [2:13:34<5:18:04, 19.03s/it]                                                      {'loss': 0.7094, 'learning_rate': 1.650009250819451e-05, 'epoch': 0.3}
 30%|██▉       | 423/1426 [2:13:34<5:18:04, 19.03s/it] 30%|██▉       | 424/1426 [2:13:56<5:33:59, 20.00s/it]                                                      {'loss': 0.7187, 'learning_rate': 1.64828134325194e-05, 'epoch': 0.3}
 30%|██▉       | 424/1426 [2:13:56<5:33:59, 20.00s/it] 30%|██▉       | 425/1426 [2:14:15<5:28:30, 19.69s/it]                                                      {'loss': 0.7191, 'learning_rate': 1.6465500905103585e-05, 'epoch': 0.3}
 30%|██▉       | 425/1426 [2:14:15<5:28:30, 19.69s/it] 30%|██▉       | 426/1426 [2:14:32<5:16:19, 18.98s/it]                                                      {'loss': 0.7205, 'learning_rate': 1.6448155015280843e-05, 'epoch': 0.3}
 30%|██▉       | 426/1426 [2:14:32<5:16:19, 18.98s/it] 30%|██▉       | 427/1426 [2:14:52<5:19:23, 19.18s/it]                                                      {'loss': 0.6987, 'learning_rate': 1.6430775852557096e-05, 'epoch': 0.3}
 30%|██▉       | 427/1426 [2:14:52<5:19:23, 19.18s/it] 30%|███       | 428/1426 [2:15:10<5:11:02, 18.70s/it]                                                      {'loss': 0.7053, 'learning_rate': 1.6413363506609955e-05, 'epoch': 0.3}
 30%|███       | 428/1426 [2:15:10<5:11:02, 18.70s/it] 30%|███       | 429/1426 [2:15:28<5:08:07, 18.54s/it]                                                      {'loss': 0.7174, 'learning_rate': 1.6395918067288266e-05, 'epoch': 0.3}
 30%|███       | 429/1426 [2:15:28<5:08:07, 18.54s/it] 30%|███       | 430/1426 [2:15:46<5:04:33, 18.35s/it]                                                      {'loss': 0.7355, 'learning_rate': 1.6378439624611635e-05, 'epoch': 0.3}
 30%|███       | 430/1426 [2:15:46<5:04:33, 18.35s/it] 30%|███       | 431/1426 [2:16:03<4:56:51, 17.90s/it]                                                      {'loss': 0.7124, 'learning_rate': 1.6360928268769966e-05, 'epoch': 0.3}
 30%|███       | 431/1426 [2:16:03<4:56:51, 17.90s/it] 30%|███       | 432/1426 [2:16:20<4:54:38, 17.78s/it]                                                      {'loss': 0.7013, 'learning_rate': 1.6343384090123e-05, 'epoch': 0.3}
 30%|███       | 432/1426 [2:16:20<4:54:38, 17.78s/it] 30%|███       | 433/1426 [2:16:38<4:53:31, 17.74s/it]                                                      {'loss': 0.7026, 'learning_rate': 1.632580717919985e-05, 'epoch': 0.3}
 30%|███       | 433/1426 [2:16:38<4:53:31, 17.74s/it] 30%|███       | 434/1426 [2:16:56<4:55:04, 17.85s/it]                                                      {'loss': 0.7059, 'learning_rate': 1.630819762669851e-05, 'epoch': 0.3}
 30%|███       | 434/1426 [2:16:56<4:55:04, 17.85s/it] 31%|███       | 435/1426 [2:17:15<5:01:08, 18.23s/it]                                                      {'loss': 0.6921, 'learning_rate': 1.6290555523485436e-05, 'epoch': 0.3}
 31%|███       | 435/1426 [2:17:15<5:01:08, 18.23s/it] 31%|███       | 436/1426 [2:17:34<5:02:26, 18.33s/it]                                                      {'loss': 0.7465, 'learning_rate': 1.6272880960595027e-05, 'epoch': 0.31}
 31%|███       | 436/1426 [2:17:34<5:02:26, 18.33s/it] 31%|███       | 437/1426 [2:17:52<5:01:42, 18.30s/it]                                                      {'loss': 0.7049, 'learning_rate': 1.6255174029229178e-05, 'epoch': 0.31}
 31%|███       | 437/1426 [2:17:52<5:01:42, 18.30s/it] 31%|███       | 438/1426 [2:18:10<4:59:19, 18.18s/it]                                                      {'loss': 0.7437, 'learning_rate': 1.6237434820756814e-05, 'epoch': 0.31}
 31%|███       | 438/1426 [2:18:10<4:59:19, 18.18s/it] 31%|███       | 439/1426 [2:18:29<5:03:51, 18.47s/it]                                                      {'loss': 0.6861, 'learning_rate': 1.6219663426713412e-05, 'epoch': 0.31}
 31%|███       | 439/1426 [2:18:29<5:03:51, 18.47s/it] 31%|███       | 440/1426 [2:18:46<4:58:46, 18.18s/it]                                                      {'loss': 0.7191, 'learning_rate': 1.6201859938800527e-05, 'epoch': 0.31}
 31%|███       | 440/1426 [2:18:46<4:58:46, 18.18s/it] 31%|███       | 441/1426 [2:19:04<4:55:16, 17.99s/it]                                                      {'loss': 0.7431, 'learning_rate': 1.6184024448885312e-05, 'epoch': 0.31}
 31%|███       | 441/1426 [2:19:04<4:55:16, 17.99s/it] 31%|███       | 442/1426 [2:19:21<4:49:34, 17.66s/it]                                                      {'loss': 0.73, 'learning_rate': 1.6166157049000067e-05, 'epoch': 0.31}
 31%|███       | 442/1426 [2:19:21<4:49:34, 17.66s/it] 31%|███       | 443/1426 [2:19:39<4:52:02, 17.83s/it]                                                      {'loss': 0.7003, 'learning_rate': 1.6148257831341738e-05, 'epoch': 0.31}
 31%|███       | 443/1426 [2:19:39<4:52:02, 17.83s/it] 31%|███       | 444/1426 [2:19:58<4:59:43, 18.31s/it]                                                      {'loss': 0.7456, 'learning_rate': 1.6130326888271456e-05, 'epoch': 0.31}
 31%|███       | 444/1426 [2:19:58<4:59:43, 18.31s/it] 31%|███       | 445/1426 [2:20:17<5:03:04, 18.54s/it]                                                      {'loss': 0.7391, 'learning_rate': 1.6112364312314065e-05, 'epoch': 0.31}
 31%|███       | 445/1426 [2:20:17<5:03:04, 18.54s/it] 31%|███▏      | 446/1426 [2:20:35<5:00:11, 18.38s/it]                                                      {'loss': 0.7128, 'learning_rate': 1.6094370196157622e-05, 'epoch': 0.31}
 31%|███▏      | 446/1426 [2:20:35<5:00:11, 18.38s/it]this iter is wrong in something... skip...
 31%|███▏      | 447/1426 [2:20:54<5:02:26, 18.54s/it]                                                      {'loss': 0.7312, 'learning_rate': 1.6076344632652948e-05, 'epoch': 0.31}
 31%|███▏      | 447/1426 [2:20:54<5:02:26, 18.54s/it] 31%|███▏      | 448/1426 [2:21:14<5:05:39, 18.75s/it]                                                      {'loss': 0.737, 'learning_rate': 1.6058287714813126e-05, 'epoch': 0.31}
 31%|███▏      | 448/1426 [2:21:14<5:05:39, 18.75s/it] 31%|███▏      | 449/1426 [2:21:34<5:10:48, 19.09s/it]                                                      {'loss': 0.6726, 'learning_rate': 1.604019953581303e-05, 'epoch': 0.31}
 31%|███▏      | 449/1426 [2:21:34<5:10:48, 19.09s/it] 32%|███▏      | 450/1426 [2:21:51<5:04:47, 18.74s/it]                                                      {'loss': 0.7015, 'learning_rate': 1.6022080188988845e-05, 'epoch': 0.32}
 32%|███▏      | 450/1426 [2:21:51<5:04:47, 18.74s/it] 32%|███▏      | 451/1426 [2:22:10<5:01:18, 18.54s/it]                                                      {'loss': 0.7397, 'learning_rate': 1.6003929767837586e-05, 'epoch': 0.32}
 32%|███▏      | 451/1426 [2:22:10<5:01:18, 18.54s/it] 32%|███▏      | 452/1426 [2:22:27<4:57:09, 18.31s/it]                                                      {'loss': 0.7444, 'learning_rate': 1.598574836601661e-05, 'epoch': 0.32}
 32%|███▏      | 452/1426 [2:22:27<4:57:09, 18.31s/it] 32%|███▏      | 453/1426 [2:22:47<5:01:51, 18.61s/it]                                                      {'loss': 0.7123, 'learning_rate': 1.596753607734314e-05, 'epoch': 0.32}
 32%|███▏      | 453/1426 [2:22:47<5:01:51, 18.61s/it] 32%|███▏      | 454/1426 [2:23:06<5:04:16, 18.78s/it]                                                      {'loss': 0.7152, 'learning_rate': 1.5949292995793767e-05, 'epoch': 0.32}
 32%|███▏      | 454/1426 [2:23:06<5:04:16, 18.78s/it] 32%|███▏      | 455/1426 [2:23:26<5:13:06, 19.35s/it]                                                      {'loss': 0.6961, 'learning_rate': 1.5931019215503992e-05, 'epoch': 0.32}
 32%|███▏      | 455/1426 [2:23:26<5:13:06, 19.35s/it] 32%|███▏      | 456/1426 [2:23:49<5:28:34, 20.32s/it]                                                      {'loss': 0.7148, 'learning_rate': 1.5912714830767715e-05, 'epoch': 0.32}
 32%|███▏      | 456/1426 [2:23:49<5:28:34, 20.32s/it] 32%|███▏      | 457/1426 [2:24:08<5:22:37, 19.98s/it]                                                      {'loss': 0.7224, 'learning_rate': 1.589437993603675e-05, 'epoch': 0.32}
 32%|███▏      | 457/1426 [2:24:08<5:22:37, 19.98s/it] 32%|███▏      | 458/1426 [2:24:30<5:30:21, 20.48s/it]                                                      {'loss': 0.7509, 'learning_rate': 1.5876014625920356e-05, 'epoch': 0.32}
 32%|███▏      | 458/1426 [2:24:30<5:30:21, 20.48s/it] 32%|███▏      | 459/1426 [2:24:51<5:32:24, 20.63s/it]                                                      {'loss': 0.7191, 'learning_rate': 1.5857618995184734e-05, 'epoch': 0.32}
 32%|███▏      | 459/1426 [2:24:51<5:32:24, 20.63s/it]this iter is wrong in something... skip...
 32%|███▏      | 460/1426 [2:25:10<5:26:36, 20.29s/it]                                                      {'loss': 0.685, 'learning_rate': 1.5839193138752538e-05, 'epoch': 0.32}
 32%|███▏      | 460/1426 [2:25:10<5:26:36, 20.29s/it] 32%|███▏      | 461/1426 [2:25:32<5:30:42, 20.56s/it]                                                      {'loss': 0.7302, 'learning_rate': 1.5820737151702385e-05, 'epoch': 0.32}
 32%|███▏      | 461/1426 [2:25:32<5:30:42, 20.56s/it] 32%|███▏      | 462/1426 [2:25:52<5:31:20, 20.62s/it]                                                      {'loss': 0.7517, 'learning_rate': 1.5802251129268384e-05, 'epoch': 0.32}
 32%|███▏      | 462/1426 [2:25:52<5:31:20, 20.62s/it] 32%|███▏      | 463/1426 [2:26:12<5:26:18, 20.33s/it]                                                      {'loss': 0.7016, 'learning_rate': 1.578373516683961e-05, 'epoch': 0.32}
 32%|███▏      | 463/1426 [2:26:12<5:26:18, 20.33s/it] 33%|███▎      | 464/1426 [2:26:35<5:38:52, 21.14s/it]                                                      {'loss': 0.7045, 'learning_rate': 1.576518935995964e-05, 'epoch': 0.33}
 33%|███▎      | 464/1426 [2:26:35<5:38:52, 21.14s/it] 33%|███▎      | 465/1426 [2:26:55<5:32:34, 20.76s/it]                                                      {'loss': 0.7178, 'learning_rate': 1.5746613804326052e-05, 'epoch': 0.33}
 33%|███▎      | 465/1426 [2:26:55<5:32:34, 20.76s/it] 33%|███▎      | 466/1426 [2:27:15<5:29:36, 20.60s/it]                                                      {'loss': 0.7068, 'learning_rate': 1.5728008595789925e-05, 'epoch': 0.33}
 33%|███▎      | 466/1426 [2:27:15<5:29:36, 20.60s/it] 33%|███▎      | 467/1426 [2:27:34<5:22:37, 20.18s/it]                                                      {'loss': 0.6972, 'learning_rate': 1.570937383035535e-05, 'epoch': 0.33}
 33%|███▎      | 467/1426 [2:27:34<5:22:37, 20.18s/it] 33%|███▎      | 468/1426 [2:27:55<5:24:40, 20.33s/it]                                                      {'loss': 0.7177, 'learning_rate': 1.5690709604178928e-05, 'epoch': 0.33}
 33%|███▎      | 468/1426 [2:27:55<5:24:40, 20.33s/it] 33%|███▎      | 469/1426 [2:28:14<5:15:51, 19.80s/it]                                                      {'loss': 0.7335, 'learning_rate': 1.5672016013569297e-05, 'epoch': 0.33}
 33%|███▎      | 469/1426 [2:28:14<5:15:51, 19.80s/it] 33%|███▎      | 470/1426 [2:28:33<5:16:17, 19.85s/it]                                                      {'loss': 0.7158, 'learning_rate': 1.5653293154986594e-05, 'epoch': 0.33}
 33%|███▎      | 470/1426 [2:28:33<5:16:17, 19.85s/it] 33%|███▎      | 471/1426 [2:28:52<5:07:44, 19.33s/it]                                                      {'loss': 0.6794, 'learning_rate': 1.5634541125041995e-05, 'epoch': 0.33}
 33%|███▎      | 471/1426 [2:28:52<5:07:44, 19.33s/it] 33%|███▎      | 472/1426 [2:29:14<5:24:15, 20.39s/it]                                                      {'loss': 0.6933, 'learning_rate': 1.5615760020497202e-05, 'epoch': 0.33}
 33%|███▎      | 472/1426 [2:29:14<5:24:15, 20.39s/it] 33%|███▎      | 473/1426 [2:29:34<5:20:19, 20.17s/it]                                                      {'loss': 0.734, 'learning_rate': 1.5596949938263942e-05, 'epoch': 0.33}
 33%|███▎      | 473/1426 [2:29:34<5:20:19, 20.17s/it] 33%|███▎      | 474/1426 [2:29:55<5:23:26, 20.38s/it]                                                      {'loss': 0.6949, 'learning_rate': 1.5578110975403467e-05, 'epoch': 0.33}
 33%|███▎      | 474/1426 [2:29:55<5:23:26, 20.38s/it] 33%|███▎      | 475/1426 [2:30:14<5:18:11, 20.08s/it]                                                      {'loss': 0.7187, 'learning_rate': 1.5559243229126047e-05, 'epoch': 0.33}
 33%|███▎      | 475/1426 [2:30:14<5:18:11, 20.08s/it] 33%|███▎      | 476/1426 [2:30:35<5:21:28, 20.30s/it]                                                      {'loss': 0.6989, 'learning_rate': 1.5540346796790497e-05, 'epoch': 0.33}
 33%|███▎      | 476/1426 [2:30:35<5:21:28, 20.30s/it] 33%|███▎      | 477/1426 [2:30:54<5:14:02, 19.86s/it]                                                      {'loss': 0.685, 'learning_rate': 1.552142177590364e-05, 'epoch': 0.33}
 33%|███▎      | 477/1426 [2:30:54<5:14:02, 19.86s/it] 34%|███▎      | 478/1426 [2:31:14<5:15:11, 19.95s/it]                                                      {'loss': 0.7043, 'learning_rate': 1.5502468264119813e-05, 'epoch': 0.34}
 34%|███▎      | 478/1426 [2:31:14<5:15:11, 19.95s/it] 34%|███▎      | 479/1426 [2:31:33<5:09:25, 19.60s/it]                                                      {'loss': 0.7264, 'learning_rate': 1.548348635924038e-05, 'epoch': 0.34}
 34%|███▎      | 479/1426 [2:31:33<5:09:25, 19.60s/it] 34%|███▎      | 480/1426 [2:31:52<5:06:27, 19.44s/it]                                                      {'loss': 0.7376, 'learning_rate': 1.5464476159213207e-05, 'epoch': 0.34}
 34%|███▎      | 480/1426 [2:31:52<5:06:27, 19.44s/it] 34%|███▎      | 481/1426 [2:32:11<5:04:42, 19.35s/it]                                                      {'loss': 0.7139, 'learning_rate': 1.5445437762132173e-05, 'epoch': 0.34}
 34%|███▎      | 481/1426 [2:32:11<5:04:42, 19.35s/it] 34%|███▍      | 482/1426 [2:32:29<4:58:17, 18.96s/it]                                                      {'loss': 0.7279, 'learning_rate': 1.5426371266236647e-05, 'epoch': 0.34}
 34%|███▍      | 482/1426 [2:32:29<4:58:17, 18.96s/it] 34%|███▍      | 483/1426 [2:32:48<4:55:59, 18.83s/it]                                                      {'loss': 0.7465, 'learning_rate': 1.540727676991099e-05, 'epoch': 0.34}
 34%|███▍      | 483/1426 [2:32:48<4:55:59, 18.83s/it] 34%|███▍      | 484/1426 [2:33:09<5:09:19, 19.70s/it]                                                      {'loss': 0.7244, 'learning_rate': 1.538815437168405e-05, 'epoch': 0.34}
 34%|███▍      | 484/1426 [2:33:09<5:09:19, 19.70s/it] 34%|███▍      | 485/1426 [2:33:27<5:00:59, 19.19s/it]                                                      {'loss': 0.7156, 'learning_rate': 1.5369004170228654e-05, 'epoch': 0.34}
 34%|███▍      | 485/1426 [2:33:28<5:00:59, 19.19s/it] 34%|███▍      | 486/1426 [2:33:42<4:38:19, 17.77s/it]                                                      {'loss': 0.2918, 'learning_rate': 1.5349826264361095e-05, 'epoch': 0.34}
 34%|███▍      | 486/1426 [2:33:42<4:38:19, 17.77s/it] 34%|███▍      | 487/1426 [2:34:00<4:37:23, 17.72s/it]                                                      {'loss': 0.7243, 'learning_rate': 1.5330620753040618e-05, 'epoch': 0.34}
 34%|███▍      | 487/1426 [2:34:00<4:37:23, 17.72s/it] 34%|███▍      | 488/1426 [2:34:18<4:38:35, 17.82s/it]                                                      {'loss': 0.7381, 'learning_rate': 1.531138773536891e-05, 'epoch': 0.34}
 34%|███▍      | 488/1426 [2:34:18<4:38:35, 17.82s/it] 34%|███▍      | 489/1426 [2:34:40<4:59:02, 19.15s/it]                                                      {'loss': 0.7368, 'learning_rate': 1.5292127310589603e-05, 'epoch': 0.34}
 34%|███▍      | 489/1426 [2:34:40<4:59:02, 19.15s/it] 34%|███▍      | 490/1426 [2:34:58<4:55:38, 18.95s/it]                                                      {'loss': 0.7305, 'learning_rate': 1.527283957808775e-05, 'epoch': 0.34}
 34%|███▍      | 490/1426 [2:34:58<4:55:38, 18.95s/it] 34%|███▍      | 491/1426 [2:35:16<4:49:52, 18.60s/it]                                                      {'loss': 0.6997, 'learning_rate': 1.5253524637389302e-05, 'epoch': 0.34}
 34%|███▍      | 491/1426 [2:35:16<4:49:52, 18.60s/it] 35%|███▍      | 492/1426 [2:35:34<4:47:02, 18.44s/it]                                                      {'loss': 0.7618, 'learning_rate': 1.523418258816062e-05, 'epoch': 0.34}
 35%|███▍      | 492/1426 [2:35:34<4:47:02, 18.44s/it] 35%|███▍      | 493/1426 [2:35:52<4:44:58, 18.33s/it]                                                      {'loss': 0.7016, 'learning_rate': 1.5214813530207943e-05, 'epoch': 0.35}
 35%|███▍      | 493/1426 [2:35:52<4:44:58, 18.33s/it] 35%|███▍      | 494/1426 [2:36:10<4:40:08, 18.03s/it]                                                      {'loss': 0.7157, 'learning_rate': 1.519541756347687e-05, 'epoch': 0.35}
 35%|███▍      | 494/1426 [2:36:10<4:40:08, 18.03s/it] 35%|███▍      | 495/1426 [2:36:24<4:22:30, 16.92s/it]                                                      {'loss': 0.2478, 'learning_rate': 1.517599478805186e-05, 'epoch': 0.35}
 35%|███▍      | 495/1426 [2:36:24<4:22:30, 16.92s/it] 35%|███▍      | 496/1426 [2:36:42<4:27:25, 17.25s/it]                                                      {'loss': 0.7249, 'learning_rate': 1.5156545304155699e-05, 'epoch': 0.35}
 35%|███▍      | 496/1426 [2:36:42<4:27:25, 17.25s/it] 35%|███▍      | 497/1426 [2:37:01<4:33:38, 17.67s/it]                                                      {'loss': 0.7098, 'learning_rate': 1.5137069212148998e-05, 'epoch': 0.35}
 35%|███▍      | 497/1426 [2:37:01<4:33:38, 17.67s/it] 35%|███▍      | 498/1426 [2:37:22<4:52:21, 18.90s/it]                                                      {'loss': 0.7086, 'learning_rate': 1.5117566612529661e-05, 'epoch': 0.35}
 35%|███▍      | 498/1426 [2:37:22<4:52:21, 18.90s/it] 35%|███▍      | 499/1426 [2:37:40<4:46:36, 18.55s/it]                                                      {'loss': 0.7297, 'learning_rate': 1.5098037605932376e-05, 'epoch': 0.35}
 35%|███▍      | 499/1426 [2:37:40<4:46:36, 18.55s/it] 35%|███▌      | 500/1426 [2:37:59<4:47:25, 18.62s/it]                                                      {'loss': 0.7074, 'learning_rate': 1.5078482293128093e-05, 'epoch': 0.35}
 35%|███▌      | 500/1426 [2:37:59<4:47:25, 18.62s/it] 35%|███▌      | 501/1426 [2:38:19<4:52:44, 18.99s/it]                                                      {'loss': 0.7442, 'learning_rate': 1.5058900775023506e-05, 'epoch': 0.35}
 35%|███▌      | 501/1426 [2:38:19<4:52:44, 18.99s/it] 35%|███▌      | 502/1426 [2:38:38<4:51:39, 18.94s/it]                                                      {'loss': 0.7235, 'learning_rate': 1.5039293152660525e-05, 'epoch': 0.35}
 35%|███▌      | 502/1426 [2:38:38<4:51:39, 18.94s/it] 35%|███▌      | 503/1426 [2:38:58<4:56:41, 19.29s/it]                                                      {'loss': 0.7023, 'learning_rate': 1.5019659527215757e-05, 'epoch': 0.35}
 35%|███▌      | 503/1426 [2:38:58<4:56:41, 19.29s/it] 35%|███▌      | 504/1426 [2:39:12<4:33:22, 17.79s/it]                                                      {'loss': 0.2566, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.35}
 35%|███▌      | 504/1426 [2:39:12<4:33:22, 17.79s/it] 35%|███▌      | 505/1426 [2:39:30<4:36:24, 18.01s/it]                                                      {'loss': 0.696, 'learning_rate': 1.4980314672457696e-05, 'epoch': 0.35}
 35%|███▌      | 505/1426 [2:39:30<4:36:24, 18.01s/it] 35%|███▌      | 506/1426 [2:39:45<4:17:48, 16.81s/it]                                                      {'loss': 0.2441, 'learning_rate': 1.4960603646166416e-05, 'epoch': 0.35}
 35%|███▌      | 506/1426 [2:39:45<4:17:48, 16.81s/it] 36%|███▌      | 507/1426 [2:40:02<4:20:06, 16.98s/it]                                                      {'loss': 0.7226, 'learning_rate': 1.4940867022836348e-05, 'epoch': 0.36}
 36%|███▌      | 507/1426 [2:40:02<4:20:06, 16.98s/it]this iter is wrong in something... skip...
 36%|███▌      | 508/1426 [2:40:16<4:05:42, 16.06s/it]                                                      {'loss': 0.2477, 'learning_rate': 1.4921104904309755e-05, 'epoch': 0.36}
 36%|███▌      | 508/1426 [2:40:16<4:05:42, 16.06s/it] 36%|███▌      | 509/1426 [2:40:34<4:13:04, 16.56s/it]                                                      {'loss': 0.7219, 'learning_rate': 1.490131739256046e-05, 'epoch': 0.36}
 36%|███▌      | 509/1426 [2:40:34<4:13:04, 16.56s/it] 36%|███▌      | 510/1426 [2:40:55<4:37:14, 18.16s/it]                                                      {'loss': 0.7007, 'learning_rate': 1.4881504589693317e-05, 'epoch': 0.36}
 36%|███▌      | 510/1426 [2:40:55<4:37:14, 18.16s/it] 36%|███▌      | 511/1426 [2:41:15<4:45:03, 18.69s/it]                                                      {'loss': 0.7115, 'learning_rate': 1.486166659794368e-05, 'epoch': 0.36}
 36%|███▌      | 511/1426 [2:41:15<4:45:03, 18.69s/it] 36%|███▌      | 512/1426 [2:41:37<4:58:17, 19.58s/it]                                                      {'loss': 0.7165, 'learning_rate': 1.4841803519676882e-05, 'epoch': 0.36}
 36%|███▌      | 512/1426 [2:41:37<4:58:17, 19.58s/it] 36%|███▌      | 513/1426 [2:41:58<5:04:44, 20.03s/it]                                                      {'loss': 0.7037, 'learning_rate': 1.4821915457387712e-05, 'epoch': 0.36}
 36%|███▌      | 513/1426 [2:41:58<5:04:44, 20.03s/it] 36%|███▌      | 514/1426 [2:42:20<5:14:50, 20.71s/it]                                                      {'loss': 0.6912, 'learning_rate': 1.480200251369986e-05, 'epoch': 0.36}
 36%|███▌      | 514/1426 [2:42:20<5:14:50, 20.71s/it] 36%|███▌      | 515/1426 [2:42:40<5:08:51, 20.34s/it]                                                      {'loss': 0.7351, 'learning_rate': 1.4782064791365419e-05, 'epoch': 0.36}
 36%|███▌      | 515/1426 [2:42:40<5:08:51, 20.34s/it] 36%|███▌      | 516/1426 [2:42:56<4:51:34, 19.22s/it]                                                      {'loss': 0.232, 'learning_rate': 1.4762102393264344e-05, 'epoch': 0.36}
 36%|███▌      | 516/1426 [2:42:56<4:51:34, 19.22s/it] 36%|███▋      | 517/1426 [2:43:17<4:59:10, 19.75s/it]                                                      {'loss': 0.7067, 'learning_rate': 1.4742115422403904e-05, 'epoch': 0.36}
 36%|███▋      | 517/1426 [2:43:17<4:59:10, 19.75s/it] 36%|███▋      | 518/1426 [2:43:38<5:02:10, 19.97s/it]                                                      {'loss': 0.6983, 'learning_rate': 1.472210398191818e-05, 'epoch': 0.36}
 36%|███▋      | 518/1426 [2:43:38<5:02:10, 19.97s/it] 36%|███▋      | 519/1426 [2:43:58<5:00:44, 19.90s/it]                                                      {'loss': 0.6773, 'learning_rate': 1.4702068175067509e-05, 'epoch': 0.36}
 36%|███▋      | 519/1426 [2:43:58<5:00:44, 19.90s/it] 36%|███▋      | 520/1426 [2:44:16<4:55:00, 19.54s/it]                                                      {'loss': 0.7024, 'learning_rate': 1.4682008105237967e-05, 'epoch': 0.36}
 36%|███▋      | 520/1426 [2:44:16<4:55:00, 19.54s/it] 37%|███▋      | 521/1426 [2:44:36<4:53:05, 19.43s/it]                                                      {'loss': 0.6863, 'learning_rate': 1.4661923875940819e-05, 'epoch': 0.37}
 37%|███▋      | 521/1426 [2:44:36<4:53:05, 19.43s/it] 37%|███▋      | 522/1426 [2:44:58<5:05:13, 20.26s/it]                                                      {'loss': 0.7172, 'learning_rate': 1.4641815590812003e-05, 'epoch': 0.37}
 37%|███▋      | 522/1426 [2:44:58<5:05:13, 20.26s/it] 37%|███▋      | 523/1426 [2:45:18<5:04:57, 20.26s/it]                                                      {'loss': 0.6978, 'learning_rate': 1.4621683353611583e-05, 'epoch': 0.37}
 37%|███▋      | 523/1426 [2:45:18<5:04:57, 20.26s/it] 37%|███▋      | 524/1426 [2:45:41<5:15:44, 21.00s/it]                                                      {'loss': 0.6881, 'learning_rate': 1.4601527268223214e-05, 'epoch': 0.37}
 37%|███▋      | 524/1426 [2:45:41<5:15:44, 21.00s/it] 37%|███▋      | 525/1426 [2:45:59<5:03:59, 20.24s/it]                                                      {'loss': 0.7058, 'learning_rate': 1.4581347438653624e-05, 'epoch': 0.37}
 37%|███▋      | 525/1426 [2:45:59<5:03:59, 20.24s/it] 37%|███▋      | 526/1426 [2:46:19<5:01:27, 20.10s/it]                                                      {'loss': 0.7101, 'learning_rate': 1.4561143969032039e-05, 'epoch': 0.37}
 37%|███▋      | 526/1426 [2:46:19<5:01:27, 20.10s/it] 37%|███▋      | 527/1426 [2:46:39<5:01:22, 20.11s/it]                                                      {'loss': 0.7043, 'learning_rate': 1.4540916963609687e-05, 'epoch': 0.37}
 37%|███▋      | 527/1426 [2:46:39<5:01:22, 20.11s/it] 37%|███▋      | 528/1426 [2:46:58<4:55:33, 19.75s/it]                                                      {'loss': 0.7278, 'learning_rate': 1.452066652675924e-05, 'epoch': 0.37}
 37%|███▋      | 528/1426 [2:46:58<4:55:33, 19.75s/it] 37%|███▋      | 529/1426 [2:47:16<4:47:45, 19.25s/it]                                                      {'loss': 0.7193, 'learning_rate': 1.4500392762974269e-05, 'epoch': 0.37}
 37%|███▋      | 529/1426 [2:47:16<4:47:45, 19.25s/it] 37%|███▋      | 530/1426 [2:47:39<5:03:21, 20.31s/it]                                                      {'loss': 0.7235, 'learning_rate': 1.4480095776868724e-05, 'epoch': 0.37}
 37%|███▋      | 530/1426 [2:47:39<5:03:21, 20.31s/it] 37%|███▋      | 531/1426 [2:47:59<5:01:59, 20.25s/it]                                                      {'loss': 0.6782, 'learning_rate': 1.4459775673176375e-05, 'epoch': 0.37}
 37%|███▋      | 531/1426 [2:47:59<5:01:59, 20.25s/it] 37%|███▋      | 532/1426 [2:48:21<5:09:00, 20.74s/it]                                                      {'loss': 0.7044, 'learning_rate': 1.4439432556750288e-05, 'epoch': 0.37}
 37%|███▋      | 532/1426 [2:48:21<5:09:00, 20.74s/it] 37%|███▋      | 533/1426 [2:48:40<5:00:55, 20.22s/it]                                                      {'loss': 0.7027, 'learning_rate': 1.4419066532562266e-05, 'epoch': 0.37}
 37%|███▋      | 533/1426 [2:48:40<5:00:55, 20.22s/it] 37%|███▋      | 534/1426 [2:49:01<5:03:36, 20.42s/it]                                                      {'loss': 0.6994, 'learning_rate': 1.4398677705702332e-05, 'epoch': 0.37}
 37%|███▋      | 534/1426 [2:49:01<5:03:36, 20.42s/it] 38%|███▊      | 535/1426 [2:49:21<5:01:31, 20.31s/it]                                                      {'loss': 0.7119, 'learning_rate': 1.437826618137816e-05, 'epoch': 0.38}
 38%|███▊      | 535/1426 [2:49:21<5:01:31, 20.31s/it] 38%|███▊      | 536/1426 [2:49:41<4:59:01, 20.16s/it]                                                      {'loss': 0.7047, 'learning_rate': 1.4357832064914548e-05, 'epoch': 0.38}
 38%|███▊      | 536/1426 [2:49:41<4:59:01, 20.16s/it] 38%|███▊      | 537/1426 [2:50:01<5:00:29, 20.28s/it]                                                      {'loss': 0.7198, 'learning_rate': 1.4337375461752872e-05, 'epoch': 0.38}
 38%|███▊      | 537/1426 [2:50:01<5:00:29, 20.28s/it] 38%|███▊      | 538/1426 [2:50:22<5:04:33, 20.58s/it]                                                      {'loss': 0.6982, 'learning_rate': 1.4316896477450542e-05, 'epoch': 0.38}
 38%|███▊      | 538/1426 [2:50:22<5:04:33, 20.58s/it] 38%|███▊      | 539/1426 [2:50:40<4:49:17, 19.57s/it]                                                      {'loss': 0.706, 'learning_rate': 1.4296395217680453e-05, 'epoch': 0.38}
 38%|███▊      | 539/1426 [2:50:40<4:49:17, 19.57s/it] 38%|███▊      | 540/1426 [2:50:59<4:48:09, 19.51s/it]                                                      {'loss': 0.6704, 'learning_rate': 1.4275871788230443e-05, 'epoch': 0.38}
 38%|███▊      | 540/1426 [2:50:59<4:48:09, 19.51s/it] 38%|███▊      | 541/1426 [2:51:18<4:45:57, 19.39s/it]                                                      {'loss': 0.682, 'learning_rate': 1.4255326295002754e-05, 'epoch': 0.38}
 38%|███▊      | 541/1426 [2:51:18<4:45:57, 19.39s/it] 38%|███▊      | 542/1426 [2:51:36<4:39:27, 18.97s/it]                                                      {'loss': 0.6998, 'learning_rate': 1.4234758844013463e-05, 'epoch': 0.38}
 38%|███▊      | 542/1426 [2:51:36<4:39:27, 18.97s/it] 38%|███▊      | 543/1426 [2:51:55<4:38:40, 18.94s/it]                                                      {'loss': 0.6672, 'learning_rate': 1.4214169541391971e-05, 'epoch': 0.38}
 38%|███▊      | 543/1426 [2:51:55<4:38:40, 18.94s/it] 38%|███▊      | 544/1426 [2:52:12<4:32:02, 18.51s/it]                                                      {'loss': 0.7199, 'learning_rate': 1.4193558493380418e-05, 'epoch': 0.38}
 38%|███▊      | 544/1426 [2:52:12<4:32:02, 18.51s/it] 38%|███▊      | 545/1426 [2:52:30<4:25:39, 18.09s/it]                                                      {'loss': 0.7193, 'learning_rate': 1.4172925806333158e-05, 'epoch': 0.38}
 38%|███▊      | 545/1426 [2:52:30<4:25:39, 18.09s/it] 38%|███▊      | 546/1426 [2:52:48<4:28:44, 18.32s/it]                                                      {'loss': 0.7124, 'learning_rate': 1.4152271586716205e-05, 'epoch': 0.38}
 38%|███▊      | 546/1426 [2:52:48<4:28:44, 18.32s/it] 38%|███▊      | 547/1426 [2:53:07<4:30:16, 18.45s/it]                                                      {'loss': 0.7305, 'learning_rate': 1.4131595941106681e-05, 'epoch': 0.38}
 38%|███▊      | 547/1426 [2:53:07<4:30:16, 18.45s/it] 38%|███▊      | 548/1426 [2:53:25<4:25:12, 18.12s/it]                                                      {'loss': 0.7143, 'learning_rate': 1.4110898976192267e-05, 'epoch': 0.38}
 38%|███▊      | 548/1426 [2:53:25<4:25:12, 18.12s/it] 38%|███▊      | 549/1426 [2:53:43<4:24:53, 18.12s/it]                                                      {'loss': 0.7119, 'learning_rate': 1.4090180798770656e-05, 'epoch': 0.38}
 38%|███▊      | 549/1426 [2:53:43<4:24:53, 18.12s/it] 39%|███▊      | 550/1426 [2:54:02<4:29:42, 18.47s/it]                                                      {'loss': 0.702, 'learning_rate': 1.4069441515748993e-05, 'epoch': 0.39}
 39%|███▊      | 550/1426 [2:54:02<4:29:42, 18.47s/it] 39%|███▊      | 551/1426 [2:54:21<4:29:33, 18.48s/it]                                                      {'loss': 0.6923, 'learning_rate': 1.4048681234143337e-05, 'epoch': 0.39}
 39%|███▊      | 551/1426 [2:54:21<4:29:33, 18.48s/it] 39%|███▊      | 552/1426 [2:54:40<4:35:37, 18.92s/it]                                                      {'loss': 0.72, 'learning_rate': 1.4027900061078097e-05, 'epoch': 0.39}
 39%|███▊      | 552/1426 [2:54:40<4:35:37, 18.92s/it] 39%|███▉      | 553/1426 [2:54:59<4:32:55, 18.76s/it]                                                      {'loss': 0.7031, 'learning_rate': 1.4007098103785476e-05, 'epoch': 0.39}
 39%|███▉      | 553/1426 [2:54:59<4:32:55, 18.76s/it] 39%|███▉      | 554/1426 [2:55:17<4:28:56, 18.51s/it]                                                      {'loss': 0.7029, 'learning_rate': 1.3986275469604937e-05, 'epoch': 0.39}
 39%|███▉      | 554/1426 [2:55:17<4:28:56, 18.51s/it] 39%|███▉      | 555/1426 [2:55:37<4:37:09, 19.09s/it]                                                      {'loss': 0.7074, 'learning_rate': 1.3965432265982638e-05, 'epoch': 0.39}
 39%|███▉      | 555/1426 [2:55:37<4:37:09, 19.09s/it] 39%|███▉      | 556/1426 [2:55:58<4:43:23, 19.54s/it]                                                      {'loss': 0.6942, 'learning_rate': 1.394456860047086e-05, 'epoch': 0.39}
 39%|███▉      | 556/1426 [2:55:58<4:43:23, 19.54s/it] 39%|███▉      | 557/1426 [2:56:19<4:49:50, 20.01s/it]                                                      {'loss': 0.7001, 'learning_rate': 1.3923684580727488e-05, 'epoch': 0.39}
 39%|███▉      | 557/1426 [2:56:19<4:49:50, 20.01s/it] 39%|███▉      | 558/1426 [2:56:36<4:38:02, 19.22s/it]                                                      {'loss': 0.7124, 'learning_rate': 1.3902780314515424e-05, 'epoch': 0.39}
 39%|███▉      | 558/1426 [2:56:36<4:38:02, 19.22s/it] 39%|███▉      | 559/1426 [2:56:55<4:33:43, 18.94s/it]                                                      {'loss': 0.7236, 'learning_rate': 1.3881855909702049e-05, 'epoch': 0.39}
 39%|███▉      | 559/1426 [2:56:55<4:33:43, 18.94s/it] 39%|███▉      | 560/1426 [2:57:12<4:27:44, 18.55s/it]                                                      {'loss': 0.719, 'learning_rate': 1.3860911474258653e-05, 'epoch': 0.39}
 39%|███▉      | 560/1426 [2:57:12<4:27:44, 18.55s/it] 39%|███▉      | 561/1426 [2:57:32<4:33:48, 18.99s/it]                                                      {'loss': 0.7046, 'learning_rate': 1.38399471162599e-05, 'epoch': 0.39}
 39%|███▉      | 561/1426 [2:57:32<4:33:48, 18.99s/it] 39%|███▉      | 562/1426 [2:57:53<4:41:45, 19.57s/it]                                                      {'loss': 0.6911, 'learning_rate': 1.3818962943883239e-05, 'epoch': 0.39}
 39%|███▉      | 562/1426 [2:57:53<4:41:45, 19.57s/it] 39%|███▉      | 563/1426 [2:58:10<4:29:50, 18.76s/it]                                                      {'loss': 0.6734, 'learning_rate': 1.3797959065408372e-05, 'epoch': 0.39}
 39%|███▉      | 563/1426 [2:58:10<4:29:50, 18.76s/it] 40%|███▉      | 564/1426 [2:58:30<4:33:34, 19.04s/it]                                                      {'loss': 0.7094, 'learning_rate': 1.3776935589216687e-05, 'epoch': 0.4}
 40%|███▉      | 564/1426 [2:58:30<4:33:34, 19.04s/it] 40%|███▉      | 565/1426 [2:58:52<4:47:27, 20.03s/it]                                                      {'loss': 0.6945, 'learning_rate': 1.3755892623790688e-05, 'epoch': 0.4}
 40%|███▉      | 565/1426 [2:58:52<4:47:27, 20.03s/it] 40%|███▉      | 566/1426 [2:59:12<4:45:36, 19.93s/it]                                                      {'loss': 0.6864, 'learning_rate': 1.3734830277713457e-05, 'epoch': 0.4}
 40%|███▉      | 566/1426 [2:59:12<4:45:36, 19.93s/it] 40%|███▉      | 567/1426 [2:59:33<4:51:25, 20.36s/it]                                                      {'loss': 0.6979, 'learning_rate': 1.3713748659668067e-05, 'epoch': 0.4}
 40%|███▉      | 567/1426 [2:59:33<4:51:25, 20.36s/it] 40%|███▉      | 568/1426 [2:59:52<4:45:30, 19.97s/it]                                                      {'loss': 0.7115, 'learning_rate': 1.3692647878437053e-05, 'epoch': 0.4}
 40%|███▉      | 568/1426 [2:59:52<4:45:30, 19.97s/it] 40%|███▉      | 569/1426 [3:00:06<4:19:33, 18.17s/it]                                                      {'loss': 0.2541, 'learning_rate': 1.367152804290182e-05, 'epoch': 0.4}
 40%|███▉      | 569/1426 [3:00:06<4:19:33, 18.17s/it] 40%|███▉      | 570/1426 [3:00:25<4:23:39, 18.48s/it]                                                      {'loss': 0.7215, 'learning_rate': 1.3650389262042101e-05, 'epoch': 0.4}
 40%|███▉      | 570/1426 [3:00:25<4:23:39, 18.48s/it] 40%|████      | 571/1426 [3:00:45<4:27:44, 18.79s/it]                                                      {'loss': 0.6872, 'learning_rate': 1.3629231644935382e-05, 'epoch': 0.4}
 40%|████      | 571/1426 [3:00:45<4:27:44, 18.79s/it] 40%|████      | 572/1426 [3:01:05<4:32:41, 19.16s/it]                                                      {'loss': 0.719, 'learning_rate': 1.360805530075635e-05, 'epoch': 0.4}
 40%|████      | 572/1426 [3:01:05<4:32:41, 19.16s/it] 40%|████      | 573/1426 [3:01:26<4:42:43, 19.89s/it]                                                      {'loss': 0.713, 'learning_rate': 1.3586860338776329e-05, 'epoch': 0.4}
 40%|████      | 573/1426 [3:01:26<4:42:43, 19.89s/it] 40%|████      | 574/1426 [3:01:46<4:42:39, 19.91s/it]                                                      {'loss': 0.6878, 'learning_rate': 1.35656468683627e-05, 'epoch': 0.4}
 40%|████      | 574/1426 [3:01:46<4:42:39, 19.91s/it] 40%|████      | 575/1426 [3:02:06<4:40:45, 19.79s/it]                                                      {'loss': 0.6971, 'learning_rate': 1.3544414998978361e-05, 'epoch': 0.4}
 40%|████      | 575/1426 [3:02:06<4:40:45, 19.79s/it] 40%|████      | 576/1426 [3:02:24<4:31:58, 19.20s/it]                                                      {'loss': 0.6854, 'learning_rate': 1.3523164840181143e-05, 'epoch': 0.4}
 40%|████      | 576/1426 [3:02:24<4:31:58, 19.20s/it] 40%|████      | 577/1426 [3:02:47<4:48:13, 20.37s/it]                                                      {'loss': 0.6865, 'learning_rate': 1.3501896501623253e-05, 'epoch': 0.4}
 40%|████      | 577/1426 [3:02:47<4:48:13, 20.37s/it] 41%|████      | 578/1426 [3:03:07<4:48:33, 20.42s/it]                                                      {'loss': 0.6737, 'learning_rate': 1.3480610093050703e-05, 'epoch': 0.41}
 41%|████      | 578/1426 [3:03:07<4:48:33, 20.42s/it] 41%|████      | 579/1426 [3:03:27<4:45:11, 20.20s/it]                                                      {'loss': 0.6874, 'learning_rate': 1.3459305724302758e-05, 'epoch': 0.41}
 41%|████      | 579/1426 [3:03:27<4:45:11, 20.20s/it]this iter is wrong in something... skip...
 41%|████      | 580/1426 [3:03:48<4:49:21, 20.52s/it]                                                      {'loss': 0.6966, 'learning_rate': 1.343798350531135e-05, 'epoch': 0.41}
 41%|████      | 580/1426 [3:03:48<4:49:21, 20.52s/it] 41%|████      | 581/1426 [3:04:08<4:43:17, 20.12s/it]                                                      {'loss': 0.7009, 'learning_rate': 1.341664354610052e-05, 'epoch': 0.41}
 41%|████      | 581/1426 [3:04:08<4:43:17, 20.12s/it] 41%|████      | 582/1426 [3:04:25<4:32:50, 19.40s/it]                                                      {'loss': 0.7252, 'learning_rate': 1.3395285956785855e-05, 'epoch': 0.41}
 41%|████      | 582/1426 [3:04:25<4:32:50, 19.40s/it] 41%|████      | 583/1426 [3:04:49<4:51:46, 20.77s/it]                                                      {'loss': 0.6898, 'learning_rate': 1.3373910847573903e-05, 'epoch': 0.41}
 41%|████      | 583/1426 [3:04:49<4:51:46, 20.77s/it] 41%|████      | 584/1426 [3:05:08<4:42:31, 20.13s/it]                                                      {'loss': 0.6893, 'learning_rate': 1.3352518328761633e-05, 'epoch': 0.41}
 41%|████      | 584/1426 [3:05:08<4:42:31, 20.13s/it] 41%|████      | 585/1426 [3:05:26<4:31:49, 19.39s/it]                                                      {'loss': 0.7034, 'learning_rate': 1.3331108510735833e-05, 'epoch': 0.41}
 41%|████      | 585/1426 [3:05:26<4:31:49, 19.39s/it] 41%|████      | 586/1426 [3:05:45<4:30:04, 19.29s/it]                                                      {'loss': 0.7036, 'learning_rate': 1.3309681503972564e-05, 'epoch': 0.41}
 41%|████      | 586/1426 [3:05:45<4:30:04, 19.29s/it] 41%|████      | 587/1426 [3:06:02<4:23:52, 18.87s/it]                                                      {'loss': 0.7047, 'learning_rate': 1.328823741903658e-05, 'epoch': 0.41}
 41%|████      | 587/1426 [3:06:02<4:23:52, 18.87s/it] 41%|████      | 588/1426 [3:06:23<4:31:37, 19.45s/it]                                                      {'loss': 0.693, 'learning_rate': 1.326677636658076e-05, 'epoch': 0.41}
 41%|████      | 588/1426 [3:06:23<4:31:37, 19.45s/it] 41%|████▏     | 589/1426 [3:06:41<4:24:39, 18.97s/it]                                                      {'loss': 0.707, 'learning_rate': 1.3245298457345536e-05, 'epoch': 0.41}
 41%|████▏     | 589/1426 [3:06:41<4:24:39, 18.97s/it] 41%|████▏     | 590/1426 [3:07:00<4:24:41, 19.00s/it]                                                      {'loss': 0.7372, 'learning_rate': 1.3223803802158324e-05, 'epoch': 0.41}
 41%|████▏     | 590/1426 [3:07:00<4:24:41, 19.00s/it] 41%|████▏     | 591/1426 [3:07:21<4:31:02, 19.48s/it]                                                      {'loss': 0.6841, 'learning_rate': 1.3202292511932948e-05, 'epoch': 0.41}
 41%|████▏     | 591/1426 [3:07:21<4:31:02, 19.48s/it] 42%|████▏     | 592/1426 [3:07:39<4:24:36, 19.04s/it]                                                      {'loss': 0.675, 'learning_rate': 1.3180764697669066e-05, 'epoch': 0.42}
 42%|████▏     | 592/1426 [3:07:39<4:24:36, 19.04s/it] 42%|████▏     | 593/1426 [3:07:59<4:28:43, 19.36s/it]                                                      {'loss': 0.7119, 'learning_rate': 1.315922047045161e-05, 'epoch': 0.42}
 42%|████▏     | 593/1426 [3:07:59<4:28:43, 19.36s/it] 42%|████▏     | 594/1426 [3:08:17<4:22:29, 18.93s/it]                                                      {'loss': 0.6757, 'learning_rate': 1.3137659941450203e-05, 'epoch': 0.42}
 42%|████▏     | 594/1426 [3:08:17<4:22:29, 18.93s/it]this iter is wrong in something... skip...
 42%|████▏     | 595/1426 [3:08:34<4:15:48, 18.47s/it]                                                      {'loss': 0.7076, 'learning_rate': 1.3116083221918575e-05, 'epoch': 0.42}
 42%|████▏     | 595/1426 [3:08:34<4:15:48, 18.47s/it] 42%|████▏     | 596/1426 [3:08:52<4:13:22, 18.32s/it]                                                      {'loss': 0.709, 'learning_rate': 1.3094490423194012e-05, 'epoch': 0.42}
 42%|████▏     | 596/1426 [3:08:52<4:13:22, 18.32s/it] 42%|████▏     | 597/1426 [3:09:09<4:07:37, 17.92s/it]                                                      {'loss': 0.7001, 'learning_rate': 1.3072881656696766e-05, 'epoch': 0.42}
 42%|████▏     | 597/1426 [3:09:09<4:07:37, 17.92s/it] 42%|████▏     | 598/1426 [3:09:27<4:08:24, 18.00s/it]                                                      {'loss': 0.7076, 'learning_rate': 1.3051257033929482e-05, 'epoch': 0.42}
 42%|████▏     | 598/1426 [3:09:27<4:08:24, 18.00s/it] 42%|████▏     | 599/1426 [3:09:47<4:15:58, 18.57s/it]                                                      {'loss': 0.6996, 'learning_rate': 1.3029616666476626e-05, 'epoch': 0.42}
 42%|████▏     | 599/1426 [3:09:47<4:15:58, 18.57s/it] 42%|████▏     | 600/1426 [3:10:06<4:16:55, 18.66s/it]                                                      {'loss': 0.708, 'learning_rate': 1.3007960666003907e-05, 'epoch': 0.42}
 42%|████▏     | 600/1426 [3:10:06<4:16:55, 18.66s/it] 42%|████▏     | 601/1426 [3:10:26<4:20:53, 18.97s/it]                                                      {'loss': 0.6817, 'learning_rate': 1.2986289144257705e-05, 'epoch': 0.42}
 42%|████▏     | 601/1426 [3:10:26<4:20:53, 18.97s/it] 42%|████▏     | 602/1426 [3:10:44<4:16:36, 18.68s/it]                                                      {'loss': 0.7012, 'learning_rate': 1.2964602213064482e-05, 'epoch': 0.42}
 42%|████▏     | 602/1426 [3:10:44<4:16:36, 18.68s/it] 42%|████▏     | 603/1426 [3:10:58<3:56:43, 17.26s/it]                                                      {'loss': 0.2449, 'learning_rate': 1.2942899984330225e-05, 'epoch': 0.42}
 42%|████▏     | 603/1426 [3:10:58<3:56:43, 17.26s/it] 42%|████▏     | 604/1426 [3:11:16<3:58:48, 17.43s/it]                                                      {'loss': 0.6937, 'learning_rate': 1.2921182570039848e-05, 'epoch': 0.42}
 42%|████▏     | 604/1426 [3:11:16<3:58:48, 17.43s/it] 42%|████▏     | 605/1426 [3:11:34<4:03:18, 17.78s/it]                                                      {'loss': 0.7113, 'learning_rate': 1.2899450082256629e-05, 'epoch': 0.42}
 42%|████▏     | 605/1426 [3:11:34<4:03:18, 17.78s/it] 42%|████▏     | 606/1426 [3:11:54<4:11:13, 18.38s/it]                                                      {'loss': 0.6921, 'learning_rate': 1.2877702633121623e-05, 'epoch': 0.42}
 42%|████▏     | 606/1426 [3:11:54<4:11:13, 18.38s/it] 43%|████▎     | 607/1426 [3:12:11<4:06:58, 18.09s/it]                                                      {'loss': 0.705, 'learning_rate': 1.2855940334853086e-05, 'epoch': 0.43}
 43%|████▎     | 607/1426 [3:12:11<4:06:58, 18.09s/it] 43%|████▎     | 608/1426 [3:12:30<4:07:14, 18.13s/it]                                                      {'loss': 0.69, 'learning_rate': 1.2834163299745904e-05, 'epoch': 0.43}
 43%|████▎     | 608/1426 [3:12:30<4:07:14, 18.13s/it] 43%|████▎     | 609/1426 [3:12:47<4:05:10, 18.01s/it]                                                      {'loss': 0.7036, 'learning_rate': 1.2812371640170993e-05, 'epoch': 0.43}
 43%|████▎     | 609/1426 [3:12:47<4:05:10, 18.01s/it] 43%|████▎     | 610/1426 [3:13:02<3:50:03, 16.92s/it]                                                      {'loss': 0.2338, 'learning_rate': 1.2790565468574747e-05, 'epoch': 0.43}
 43%|████▎     | 610/1426 [3:13:02<3:50:03, 16.92s/it] 43%|████▎     | 611/1426 [3:13:19<3:49:44, 16.91s/it]                                                      {'loss': 0.6899, 'learning_rate': 1.276874489747843e-05, 'epoch': 0.43}
 43%|████▎     | 611/1426 [3:13:19<3:49:44, 16.91s/it] 43%|████▎     | 612/1426 [3:13:36<3:52:33, 17.14s/it]                                                      {'loss': 0.6842, 'learning_rate': 1.274691003947762e-05, 'epoch': 0.43}
 43%|████▎     | 612/1426 [3:13:36<3:52:33, 17.14s/it] 43%|████▎     | 613/1426 [3:13:55<4:00:07, 17.72s/it]                                                      {'loss': 0.656, 'learning_rate': 1.2725061007241601e-05, 'epoch': 0.43}
 43%|████▎     | 613/1426 [3:13:55<4:00:07, 17.72s/it] 43%|████▎     | 614/1426 [3:14:13<4:00:44, 17.79s/it]                                                      {'loss': 0.6931, 'learning_rate': 1.2703197913512817e-05, 'epoch': 0.43}
 43%|████▎     | 614/1426 [3:14:13<4:00:44, 17.79s/it] 43%|████▎     | 615/1426 [3:14:30<3:57:46, 17.59s/it]                                                      {'loss': 0.7072, 'learning_rate': 1.268132087110625e-05, 'epoch': 0.43}
 43%|████▎     | 615/1426 [3:14:30<3:57:46, 17.59s/it] 43%|████▎     | 616/1426 [3:14:49<4:03:21, 18.03s/it]                                                      {'loss': 0.7064, 'learning_rate': 1.2659429992908872e-05, 'epoch': 0.43}
 43%|████▎     | 616/1426 [3:14:50<4:03:21, 18.03s/it] 43%|████▎     | 617/1426 [3:15:09<4:07:10, 18.33s/it]                                                      {'loss': 0.6721, 'learning_rate': 1.2637525391879048e-05, 'epoch': 0.43}
 43%|████▎     | 617/1426 [3:15:09<4:07:10, 18.33s/it] 43%|████▎     | 618/1426 [3:15:26<4:02:13, 17.99s/it]                                                      {'loss': 0.7084, 'learning_rate': 1.2615607181045943e-05, 'epoch': 0.43}
 43%|████▎     | 618/1426 [3:15:26<4:02:13, 17.99s/it] 43%|████▎     | 619/1426 [3:15:44<4:01:41, 17.97s/it]                                                      {'loss': 0.7094, 'learning_rate': 1.2593675473508959e-05, 'epoch': 0.43}
 43%|████▎     | 619/1426 [3:15:44<4:01:41, 17.97s/it] 43%|████▎     | 620/1426 [3:16:03<4:07:02, 18.39s/it]                                                      {'loss': 0.6953, 'learning_rate': 1.2571730382437142e-05, 'epoch': 0.43}
 43%|████▎     | 620/1426 [3:16:03<4:07:02, 18.39s/it] 44%|████▎     | 621/1426 [3:16:24<4:18:51, 19.29s/it]                                                      {'loss': 0.6996, 'learning_rate': 1.2549772021068594e-05, 'epoch': 0.44}
 44%|████▎     | 621/1426 [3:16:24<4:18:51, 19.29s/it] 44%|████▎     | 622/1426 [3:16:46<4:27:33, 19.97s/it]                                                      {'loss': 0.6699, 'learning_rate': 1.2527800502709887e-05, 'epoch': 0.44}
 44%|████▎     | 622/1426 [3:16:46<4:27:33, 19.97s/it] 44%|████▎     | 623/1426 [3:17:05<4:22:02, 19.58s/it]                                                      {'loss': 0.6939, 'learning_rate': 1.25058159407355e-05, 'epoch': 0.44}
 44%|████▎     | 623/1426 [3:17:05<4:22:02, 19.58s/it] 44%|████▍     | 624/1426 [3:17:24<4:19:49, 19.44s/it]                                                      {'loss': 0.706, 'learning_rate': 1.2483818448587204e-05, 'epoch': 0.44}
 44%|████▍     | 624/1426 [3:17:24<4:19:49, 19.44s/it] 44%|████▍     | 625/1426 [3:17:41<4:08:53, 18.64s/it]                                                      {'loss': 0.7034, 'learning_rate': 1.2461808139773494e-05, 'epoch': 0.44}
 44%|████▍     | 625/1426 [3:17:41<4:08:53, 18.64s/it] 44%|████▍     | 626/1426 [3:18:00<4:12:39, 18.95s/it]                                                      {'loss': 0.7075, 'learning_rate': 1.2439785127869007e-05, 'epoch': 0.44}
 44%|████▍     | 626/1426 [3:18:00<4:12:39, 18.95s/it] 44%|████▍     | 627/1426 [3:18:17<4:05:46, 18.46s/it]                                                      {'loss': 0.6808, 'learning_rate': 1.2417749526513915e-05, 'epoch': 0.44}
 44%|████▍     | 627/1426 [3:18:18<4:05:46, 18.46s/it] 44%|████▍     | 628/1426 [3:18:38<4:13:40, 19.07s/it]                                                      {'loss': 0.6773, 'learning_rate': 1.2395701449413364e-05, 'epoch': 0.44}
 44%|████▍     | 628/1426 [3:18:38<4:13:40, 19.07s/it] 44%|████▍     | 629/1426 [3:18:58<4:16:23, 19.30s/it]                                                      {'loss': 0.6943, 'learning_rate': 1.2373641010336875e-05, 'epoch': 0.44}
 44%|████▍     | 629/1426 [3:18:58<4:16:23, 19.30s/it] 44%|████▍     | 630/1426 [3:19:17<4:15:15, 19.24s/it]                                                      {'loss': 0.6776, 'learning_rate': 1.2351568323117748e-05, 'epoch': 0.44}
 44%|████▍     | 630/1426 [3:19:17<4:15:15, 19.24s/it] 44%|████▍     | 631/1426 [3:19:34<4:07:10, 18.66s/it]                                                      {'loss': 0.6934, 'learning_rate': 1.2329483501652493e-05, 'epoch': 0.44}
 44%|████▍     | 631/1426 [3:19:34<4:07:10, 18.66s/it]this iter is wrong in something... skip...
 44%|████▍     | 632/1426 [3:19:56<4:17:24, 19.45s/it]                                                      {'loss': 0.686, 'learning_rate': 1.2307386659900233e-05, 'epoch': 0.44}
 44%|████▍     | 632/1426 [3:19:56<4:17:24, 19.45s/it] 44%|████▍     | 633/1426 [3:20:16<4:19:58, 19.67s/it]                                                      {'loss': 0.695, 'learning_rate': 1.2285277911882114e-05, 'epoch': 0.44}
 44%|████▍     | 633/1426 [3:20:16<4:19:58, 19.67s/it] 44%|████▍     | 634/1426 [3:20:37<4:24:10, 20.01s/it]                                                      {'loss': 0.7245, 'learning_rate': 1.2263157371680716e-05, 'epoch': 0.44}
 44%|████▍     | 634/1426 [3:20:37<4:24:10, 20.01s/it] 45%|████▍     | 635/1426 [3:20:57<4:25:01, 20.10s/it]                                                      {'loss': 0.6823, 'learning_rate': 1.2241025153439477e-05, 'epoch': 0.45}
 45%|████▍     | 635/1426 [3:20:57<4:25:01, 20.10s/it] 45%|████▍     | 636/1426 [3:21:16<4:19:12, 19.69s/it]                                                      {'loss': 0.7019, 'learning_rate': 1.2218881371362086e-05, 'epoch': 0.45}
 45%|████▍     | 636/1426 [3:21:16<4:19:12, 19.69s/it] 45%|████▍     | 637/1426 [3:21:34<4:13:24, 19.27s/it]                                                      {'loss': 0.6886, 'learning_rate': 1.21967261397119e-05, 'epoch': 0.45}
 45%|████▍     | 637/1426 [3:21:34<4:13:24, 19.27s/it] 45%|████▍     | 638/1426 [3:21:55<4:19:33, 19.76s/it]                                                      {'loss': 0.6828, 'learning_rate': 1.2174559572811368e-05, 'epoch': 0.45}
 45%|████▍     | 638/1426 [3:21:55<4:19:33, 19.76s/it] 45%|████▍     | 639/1426 [3:22:13<4:13:27, 19.32s/it]                                                      {'loss': 0.6883, 'learning_rate': 1.2152381785041424e-05, 'epoch': 0.45}
 45%|████▍     | 639/1426 [3:22:13<4:13:27, 19.32s/it] 45%|████▍     | 640/1426 [3:22:32<4:10:31, 19.12s/it]                                                      {'loss': 0.7076, 'learning_rate': 1.2130192890840893e-05, 'epoch': 0.45}
 45%|████▍     | 640/1426 [3:22:32<4:10:31, 19.12s/it] 45%|████▍     | 641/1426 [3:22:54<4:21:04, 19.95s/it]                                                      {'loss': 0.661, 'learning_rate': 1.2107993004705924e-05, 'epoch': 0.45}
 45%|████▍     | 641/1426 [3:22:54<4:21:04, 19.95s/it] 45%|████▌     | 642/1426 [3:23:16<4:28:29, 20.55s/it]                                                      {'loss': 0.6997, 'learning_rate': 1.208578224118938e-05, 'epoch': 0.45}
 45%|████▌     | 642/1426 [3:23:16<4:28:29, 20.55s/it] 45%|████▌     | 643/1426 [3:23:34<4:21:16, 20.02s/it]                                                      {'loss': 0.7309, 'learning_rate': 1.2063560714900249e-05, 'epoch': 0.45}
 45%|████▌     | 643/1426 [3:23:34<4:21:16, 20.02s/it] 45%|████▌     | 644/1426 [3:23:53<4:15:36, 19.61s/it]                                                      {'loss': 0.7286, 'learning_rate': 1.204132854050306e-05, 'epoch': 0.45}
 45%|████▌     | 644/1426 [3:23:53<4:15:36, 19.61s/it] 45%|████▌     | 645/1426 [3:24:13<4:14:55, 19.58s/it]                                                      {'loss': 0.6901, 'learning_rate': 1.2019085832717281e-05, 'epoch': 0.45}
 45%|████▌     | 645/1426 [3:24:13<4:14:55, 19.58s/it] 45%|████▌     | 646/1426 [3:24:30<4:05:53, 18.91s/it]                                                      {'loss': 0.7107, 'learning_rate': 1.1996832706316739e-05, 'epoch': 0.45}
 45%|████▌     | 646/1426 [3:24:30<4:05:53, 18.91s/it] 45%|████▌     | 647/1426 [3:24:51<4:14:29, 19.60s/it]                                                      {'loss': 0.6823, 'learning_rate': 1.1974569276129023e-05, 'epoch': 0.45}
 45%|████▌     | 647/1426 [3:24:51<4:14:29, 19.60s/it] 45%|████▌     | 648/1426 [3:25:14<4:27:53, 20.66s/it]                                                      {'loss': 0.6931, 'learning_rate': 1.195229565703488e-05, 'epoch': 0.45}
 45%|████▌     | 648/1426 [3:25:14<4:27:53, 20.66s/it] 46%|████▌     | 649/1426 [3:25:33<4:21:23, 20.19s/it]                                                      {'loss': 0.6759, 'learning_rate': 1.1930011963967644e-05, 'epoch': 0.45}
 46%|████▌     | 649/1426 [3:25:33<4:21:23, 20.19s/it] 46%|████▌     | 650/1426 [3:25:51<4:12:52, 19.55s/it]                                                      {'loss': 0.691, 'learning_rate': 1.1907718311912628e-05, 'epoch': 0.46}
 46%|████▌     | 650/1426 [3:25:51<4:12:52, 19.55s/it] 46%|████▌     | 651/1426 [3:26:09<4:06:53, 19.11s/it]                                                      {'loss': 0.703, 'learning_rate': 1.1885414815906531e-05, 'epoch': 0.46}
 46%|████▌     | 651/1426 [3:26:09<4:06:53, 19.11s/it] 46%|████▌     | 652/1426 [3:26:28<4:03:04, 18.84s/it]                                                      {'loss': 0.6783, 'learning_rate': 1.1863101591036844e-05, 'epoch': 0.46}
 46%|████▌     | 652/1426 [3:26:28<4:03:04, 18.84s/it] 46%|████▌     | 653/1426 [3:26:46<4:00:12, 18.64s/it]                                                      {'loss': 0.6981, 'learning_rate': 1.1840778752441268e-05, 'epoch': 0.46}
 46%|████▌     | 653/1426 [3:26:46<4:00:12, 18.64s/it] 46%|████▌     | 654/1426 [3:27:05<4:02:53, 18.88s/it]                                                      {'loss': 0.6629, 'learning_rate': 1.1818446415307113e-05, 'epoch': 0.46}
 46%|████▌     | 654/1426 [3:27:05<4:02:53, 18.88s/it] 46%|████▌     | 655/1426 [3:27:23<3:58:00, 18.52s/it]                                                      {'loss': 0.7107, 'learning_rate': 1.1796104694870686e-05, 'epoch': 0.46}
 46%|████▌     | 655/1426 [3:27:23<3:58:00, 18.52s/it] 46%|████▌     | 656/1426 [3:27:41<3:55:00, 18.31s/it]                                                      {'loss': 0.6679, 'learning_rate': 1.1773753706416739e-05, 'epoch': 0.46}
 46%|████▌     | 656/1426 [3:27:41<3:55:00, 18.31s/it] 46%|████▌     | 657/1426 [3:28:00<3:57:03, 18.50s/it]                                                      {'loss': 0.6859, 'learning_rate': 1.175139356527782e-05, 'epoch': 0.46}
 46%|████▌     | 657/1426 [3:28:00<3:57:03, 18.50s/it] 46%|████▌     | 658/1426 [3:28:18<3:55:36, 18.41s/it]                                                      {'loss': 0.6859, 'learning_rate': 1.1729024386833721e-05, 'epoch': 0.46}
 46%|████▌     | 658/1426 [3:28:18<3:55:36, 18.41s/it] 46%|████▌     | 659/1426 [3:28:35<3:50:43, 18.05s/it]                                                      {'loss': 0.7171, 'learning_rate': 1.1706646286510863e-05, 'epoch': 0.46}
 46%|████▌     | 659/1426 [3:28:35<3:50:43, 18.05s/it] 46%|████▋     | 660/1426 [3:28:55<3:57:30, 18.60s/it]                                                      {'loss': 0.6864, 'learning_rate': 1.1684259379781707e-05, 'epoch': 0.46}
 46%|████▋     | 660/1426 [3:28:55<3:57:30, 18.60s/it] 46%|████▋     | 661/1426 [3:29:12<3:51:41, 18.17s/it]                                                      {'loss': 0.6747, 'learning_rate': 1.1661863782164154e-05, 'epoch': 0.46}
 46%|████▋     | 661/1426 [3:29:12<3:51:41, 18.17s/it] 46%|████▋     | 662/1426 [3:29:31<3:54:49, 18.44s/it]                                                      {'loss': 0.7153, 'learning_rate': 1.1639459609220946e-05, 'epoch': 0.46}
 46%|████▋     | 662/1426 [3:29:31<3:54:49, 18.44s/it] 46%|████▋     | 663/1426 [3:29:49<3:50:18, 18.11s/it]                                                      {'loss': 0.7043, 'learning_rate': 1.1617046976559086e-05, 'epoch': 0.46}
 46%|████▋     | 663/1426 [3:29:49<3:50:18, 18.11s/it] 47%|████▋     | 664/1426 [3:30:09<3:57:49, 18.73s/it]                                                      {'loss': 0.6882, 'learning_rate': 1.1594625999829215e-05, 'epoch': 0.47}
 47%|████▋     | 664/1426 [3:30:09<3:57:49, 18.73s/it] 47%|████▋     | 665/1426 [3:30:28<3:58:51, 18.83s/it]                                                      {'loss': 0.7314, 'learning_rate': 1.157219679472504e-05, 'epoch': 0.47}
 47%|████▋     | 665/1426 [3:30:28<3:58:51, 18.83s/it] 47%|████▋     | 666/1426 [3:30:46<3:56:02, 18.64s/it]                                                      {'loss': 0.6485, 'learning_rate': 1.1549759476982727e-05, 'epoch': 0.47}
 47%|████▋     | 666/1426 [3:30:46<3:56:02, 18.64s/it] 47%|████▋     | 667/1426 [3:31:04<3:51:43, 18.32s/it]                                                      {'loss': 0.6848, 'learning_rate': 1.15273141623803e-05, 'epoch': 0.47}
 47%|████▋     | 667/1426 [3:31:04<3:51:43, 18.32s/it] 47%|████▋     | 668/1426 [3:31:18<3:35:21, 17.05s/it]                                                      {'loss': 0.2425, 'learning_rate': 1.1504860966737042e-05, 'epoch': 0.47}
 47%|████▋     | 668/1426 [3:31:18<3:35:21, 17.05s/it] 47%|████▋     | 669/1426 [3:31:37<3:42:02, 17.60s/it]                                                      {'loss': 0.6917, 'learning_rate': 1.1482400005912915e-05, 'epoch': 0.47}
 47%|████▋     | 669/1426 [3:31:37<3:42:02, 17.60s/it] 47%|████▋     | 670/1426 [3:31:54<3:42:03, 17.62s/it]                                                      {'loss': 0.7148, 'learning_rate': 1.1459931395807944e-05, 'epoch': 0.47}
 47%|████▋     | 670/1426 [3:31:54<3:42:03, 17.62s/it] 47%|████▋     | 671/1426 [3:32:13<3:45:33, 17.92s/it]                                                      {'loss': 0.6795, 'learning_rate': 1.1437455252361622e-05, 'epoch': 0.47}
 47%|████▋     | 671/1426 [3:32:13<3:45:33, 17.92s/it] 47%|████▋     | 672/1426 [3:32:33<3:54:08, 18.63s/it]                                                      {'loss': 0.6923, 'learning_rate': 1.1414971691552319e-05, 'epoch': 0.47}
 47%|████▋     | 672/1426 [3:32:33<3:54:08, 18.63s/it] 47%|████▋     | 673/1426 [3:32:49<3:42:48, 17.75s/it]                                                      {'loss': 0.2455, 'learning_rate': 1.1392480829396674e-05, 'epoch': 0.47}
 47%|████▋     | 673/1426 [3:32:49<3:42:48, 17.75s/it] 47%|████▋     | 674/1426 [3:33:06<3:41:11, 17.65s/it]                                                      {'loss': 0.7097, 'learning_rate': 1.1369982781949011e-05, 'epoch': 0.47}
 47%|████▋     | 674/1426 [3:33:06<3:41:11, 17.65s/it] 47%|████▋     | 675/1426 [3:33:24<3:39:51, 17.56s/it]                                                      {'loss': 0.7042, 'learning_rate': 1.1347477665300717e-05, 'epoch': 0.47}
 47%|████▋     | 675/1426 [3:33:24<3:39:51, 17.56s/it] 47%|████▋     | 676/1426 [3:33:42<3:42:38, 17.81s/it]                                                      {'loss': 0.7244, 'learning_rate': 1.1324965595579667e-05, 'epoch': 0.47}
 47%|████▋     | 676/1426 [3:33:42<3:42:38, 17.81s/it] 47%|████▋     | 677/1426 [3:34:03<3:54:25, 18.78s/it]                                                      {'loss': 0.6803, 'learning_rate': 1.130244668894961e-05, 'epoch': 0.47}
 47%|████▋     | 677/1426 [3:34:03<3:54:25, 18.78s/it] 48%|████▊     | 678/1426 [3:34:24<4:01:58, 19.41s/it]                                                      {'loss': 0.6788, 'learning_rate': 1.1279921061609576e-05, 'epoch': 0.48}
 48%|████▊     | 678/1426 [3:34:24<4:01:58, 19.41s/it] 48%|████▊     | 679/1426 [3:34:42<3:56:25, 18.99s/it]                                                      {'loss': 0.695, 'learning_rate': 1.1257388829793274e-05, 'epoch': 0.48}
 48%|████▊     | 679/1426 [3:34:42<3:56:25, 18.99s/it] 48%|████▊     | 680/1426 [3:35:01<3:56:57, 19.06s/it]                                                      {'loss': 0.6811, 'learning_rate': 1.1234850109768488e-05, 'epoch': 0.48}
 48%|████▊     | 680/1426 [3:35:01<3:56:57, 19.06s/it] 48%|████▊     | 681/1426 [3:35:19<3:52:19, 18.71s/it]                                                      {'loss': 0.7054, 'learning_rate': 1.1212305017836491e-05, 'epoch': 0.48}
 48%|████▊     | 681/1426 [3:35:19<3:52:19, 18.71s/it] 48%|████▊     | 682/1426 [3:35:39<3:55:00, 18.95s/it]                                                      {'loss': 0.6976, 'learning_rate': 1.1189753670331424e-05, 'epoch': 0.48}
 48%|████▊     | 682/1426 [3:35:39<3:55:00, 18.95s/it] 48%|████▊     | 683/1426 [3:35:58<3:57:07, 19.15s/it]                                                      {'loss': 0.6812, 'learning_rate': 1.1167196183619719e-05, 'epoch': 0.48}
 48%|████▊     | 683/1426 [3:35:58<3:57:07, 19.15s/it] 48%|████▊     | 684/1426 [3:36:23<4:18:40, 20.92s/it]                                                      {'loss': 0.699, 'learning_rate': 1.1144632674099474e-05, 'epoch': 0.48}
 48%|████▊     | 684/1426 [3:36:23<4:18:40, 20.92s/it] 48%|████▊     | 685/1426 [3:36:37<3:52:33, 18.83s/it]                                                      {'loss': 0.2341, 'learning_rate': 1.1122063258199878e-05, 'epoch': 0.48}
 48%|████▊     | 685/1426 [3:36:37<3:52:33, 18.83s/it]this iter is wrong in something... skip...
 48%|████▊     | 686/1426 [3:36:59<4:02:39, 19.67s/it]                                                      {'loss': 0.7144, 'learning_rate': 1.1099488052380587e-05, 'epoch': 0.48}
 48%|████▊     | 686/1426 [3:36:59<4:02:39, 19.67s/it] 48%|████▊     | 687/1426 [3:37:18<4:01:08, 19.58s/it]                                                      {'loss': 0.6615, 'learning_rate': 1.1076907173131139e-05, 'epoch': 0.48}
 48%|████▊     | 687/1426 [3:37:18<4:01:08, 19.58s/it] 48%|████▊     | 688/1426 [3:37:39<4:06:33, 20.05s/it]                                                      {'loss': 0.6833, 'learning_rate': 1.1054320736970343e-05, 'epoch': 0.48}
 48%|████▊     | 688/1426 [3:37:39<4:06:33, 20.05s/it] 48%|████▊     | 689/1426 [3:38:00<4:07:26, 20.14s/it]                                                      {'loss': 0.6996, 'learning_rate': 1.103172886044569e-05, 'epoch': 0.48}
 48%|████▊     | 689/1426 [3:38:00<4:07:26, 20.14s/it] 48%|████▊     | 690/1426 [3:38:21<4:11:17, 20.49s/it]                                                      {'loss': 0.6979, 'learning_rate': 1.1009131660132733e-05, 'epoch': 0.48}
 48%|████▊     | 690/1426 [3:38:21<4:11:17, 20.49s/it] 48%|████▊     | 691/1426 [3:38:41<4:07:46, 20.23s/it]                                                      {'loss': 0.6934, 'learning_rate': 1.0986529252634503e-05, 'epoch': 0.48}
 48%|████▊     | 691/1426 [3:38:41<4:07:46, 20.23s/it] 49%|████▊     | 692/1426 [3:39:06<4:26:21, 21.77s/it]                                                      {'loss': 0.6952, 'learning_rate': 1.0963921754580901e-05, 'epoch': 0.49}
 49%|████▊     | 692/1426 [3:39:06<4:26:21, 21.77s/it] 49%|████▊     | 693/1426 [3:39:28<4:28:29, 21.98s/it]                                                      {'loss': 0.7347, 'learning_rate': 1.0941309282628092e-05, 'epoch': 0.49}
 49%|████▊     | 693/1426 [3:39:28<4:28:29, 21.98s/it] 49%|████▊     | 694/1426 [3:39:49<4:24:41, 21.70s/it]                                                      {'loss': 0.7045, 'learning_rate': 1.0918691953457907e-05, 'epoch': 0.49}
 49%|████▊     | 694/1426 [3:39:49<4:24:41, 21.70s/it] 49%|████▊     | 695/1426 [3:40:09<4:14:33, 20.89s/it]                                                      {'loss': 0.6843, 'learning_rate': 1.0896069883777246e-05, 'epoch': 0.49}
 49%|████▊     | 695/1426 [3:40:09<4:14:33, 20.89s/it] 49%|████▉     | 696/1426 [3:40:27<4:03:55, 20.05s/it]                                                      {'loss': 0.708, 'learning_rate': 1.0873443190317457e-05, 'epoch': 0.49}
 49%|████▉     | 696/1426 [3:40:27<4:03:55, 20.05s/it] 49%|████▉     | 697/1426 [3:40:47<4:05:42, 20.22s/it]                                                      {'loss': 0.6729, 'learning_rate': 1.0850811989833767e-05, 'epoch': 0.49}
 49%|████▉     | 697/1426 [3:40:47<4:05:42, 20.22s/it] 49%|████▉     | 698/1426 [3:41:05<3:57:48, 19.60s/it]                                                      {'loss': 0.6894, 'learning_rate': 1.082817639910464e-05, 'epoch': 0.49}
 49%|████▉     | 698/1426 [3:41:05<3:57:48, 19.60s/it] 49%|████▉     | 699/1426 [3:41:25<3:57:42, 19.62s/it]                                                      {'loss': 0.7078, 'learning_rate': 1.0805536534931207e-05, 'epoch': 0.49}
 49%|████▉     | 699/1426 [3:41:25<3:57:42, 19.62s/it] 49%|████▉     | 700/1426 [3:41:43<3:51:17, 19.11s/it]                                                      {'loss': 0.6704, 'learning_rate': 1.0782892514136642e-05, 'epoch': 0.49}
 49%|████▉     | 700/1426 [3:41:43<3:51:17, 19.11s/it] 49%|████▉     | 701/1426 [3:42:00<3:43:32, 18.50s/it]                                                      {'loss': 0.6829, 'learning_rate': 1.0760244453565577e-05, 'epoch': 0.49}
 49%|████▉     | 701/1426 [3:42:00<3:43:32, 18.50s/it] 49%|████▉     | 702/1426 [3:42:20<3:49:55, 19.05s/it]                                                      {'loss': 0.6913, 'learning_rate': 1.0737592470083477e-05, 'epoch': 0.49}
 49%|████▉     | 702/1426 [3:42:20<3:49:55, 19.05s/it] 49%|████▉     | 703/1426 [3:42:38<3:43:26, 18.54s/it]                                                      {'loss': 0.6902, 'learning_rate': 1.0714936680576062e-05, 'epoch': 0.49}
 49%|████▉     | 703/1426 [3:42:38<3:43:26, 18.54s/it] 49%|████▉     | 704/1426 [3:42:57<3:45:25, 18.73s/it]                                                      {'loss': 0.6947, 'learning_rate': 1.0692277201948684e-05, 'epoch': 0.49}
 49%|████▉     | 704/1426 [3:42:57<3:45:25, 18.73s/it] 49%|████▉     | 705/1426 [3:43:15<3:43:02, 18.56s/it]                                                      {'loss': 0.6744, 'learning_rate': 1.0669614151125726e-05, 'epoch': 0.49}
 49%|████▉     | 705/1426 [3:43:15<3:43:02, 18.56s/it] 50%|████▉     | 706/1426 [3:43:33<3:42:00, 18.50s/it]                                                      {'loss': 0.6871, 'learning_rate': 1.0646947645050023e-05, 'epoch': 0.49}
 50%|████▉     | 706/1426 [3:43:33<3:42:00, 18.50s/it] 50%|████▉     | 707/1426 [3:43:50<3:36:34, 18.07s/it]                                                      {'loss': 0.6826, 'learning_rate': 1.0624277800682216e-05, 'epoch': 0.5}
 50%|████▉     | 707/1426 [3:43:51<3:36:34, 18.07s/it] 50%|████▉     | 708/1426 [3:44:08<3:35:32, 18.01s/it]                                                      {'loss': 0.7145, 'learning_rate': 1.0601604735000187e-05, 'epoch': 0.5}
 50%|████▉     | 708/1426 [3:44:08<3:35:32, 18.01s/it] 50%|████▉     | 709/1426 [3:44:27<3:36:56, 18.15s/it]                                                      {'loss': 0.6878, 'learning_rate': 1.0578928564998431e-05, 'epoch': 0.5}
 50%|████▉     | 709/1426 [3:44:27<3:36:56, 18.15s/it] 50%|████▉     | 710/1426 [3:44:47<3:44:08, 18.78s/it]                                                      {'loss': 0.6721, 'learning_rate': 1.0556249407687476e-05, 'epoch': 0.5}
 50%|████▉     | 710/1426 [3:44:47<3:44:08, 18.78s/it] 50%|████▉     | 711/1426 [3:45:07<3:46:58, 19.05s/it]                                                      {'loss': 0.6769, 'learning_rate': 1.0533567380093243e-05, 'epoch': 0.5}
 50%|████▉     | 711/1426 [3:45:07<3:46:58, 19.05s/it] 50%|████▉     | 712/1426 [3:45:25<3:45:21, 18.94s/it]                                                      {'loss': 0.6617, 'learning_rate': 1.0510882599256486e-05, 'epoch': 0.5}
 50%|████▉     | 712/1426 [3:45:25<3:45:21, 18.94s/it] 50%|█████     | 713/1426 [3:45:44<3:45:16, 18.96s/it]                                                      {'loss': 0.6718, 'learning_rate': 1.0488195182232153e-05, 'epoch': 0.5}
 50%|█████     | 713/1426 [3:45:44<3:45:16, 18.96s/it] 50%|█████     | 714/1426 [3:46:02<3:39:30, 18.50s/it]                                                      {'loss': 0.7103, 'learning_rate': 1.0465505246088789e-05, 'epoch': 0.5}
 50%|█████     | 714/1426 [3:46:02<3:39:30, 18.50s/it] 50%|█████     | 715/1426 [3:46:20<3:39:16, 18.50s/it]                                                      {'loss': 0.7046, 'learning_rate': 1.044281290790796e-05, 'epoch': 0.5}
 50%|█████     | 715/1426 [3:46:20<3:39:16, 18.50s/it] 50%|█████     | 716/1426 [3:46:39<3:40:19, 18.62s/it]                                                      {'loss': 0.6751, 'learning_rate': 1.0420118284783603e-05, 'epoch': 0.5}
 50%|█████     | 716/1426 [3:46:39<3:40:19, 18.62s/it] 50%|█████     | 717/1426 [3:47:01<3:50:20, 19.49s/it]                                                      {'loss': 0.6807, 'learning_rate': 1.0397421493821456e-05, 'epoch': 0.5}
 50%|█████     | 717/1426 [3:47:01<3:50:20, 19.49s/it]this iter is wrong in something... skip...
 50%|█████     | 718/1426 [3:47:19<3:43:41, 18.96s/it]                                                      {'loss': 0.6951, 'learning_rate': 1.037472265213845e-05, 'epoch': 0.5}
 50%|█████     | 718/1426 [3:47:19<3:43:41, 18.96s/it] 50%|█████     | 719/1426 [3:47:36<3:36:39, 18.39s/it]                                                      {'loss': 0.6772, 'learning_rate': 1.0352021876862084e-05, 'epoch': 0.5}
 50%|█████     | 719/1426 [3:47:36<3:36:39, 18.39s/it] 50%|█████     | 720/1426 [3:47:54<3:37:50, 18.51s/it]                                                      {'loss': 0.6877, 'learning_rate': 1.0329319285129842e-05, 'epoch': 0.5}
 50%|█████     | 720/1426 [3:47:54<3:37:50, 18.51s/it] 51%|█████     | 721/1426 [3:48:15<3:43:18, 19.01s/it]                                                      {'loss': 0.6813, 'learning_rate': 1.0306614994088582e-05, 'epoch': 0.51}
 51%|█████     | 721/1426 [3:48:15<3:43:18, 19.01s/it] 51%|█████     | 722/1426 [3:48:32<3:38:30, 18.62s/it]                                                      {'loss': 0.7137, 'learning_rate': 1.0283909120893929e-05, 'epoch': 0.51}
 51%|█████     | 722/1426 [3:48:32<3:38:30, 18.62s/it] 51%|█████     | 723/1426 [3:48:50<3:33:56, 18.26s/it]                                                      {'loss': 0.7233, 'learning_rate': 1.0261201782709667e-05, 'epoch': 0.51}
 51%|█████     | 723/1426 [3:48:50<3:33:56, 18.26s/it] 51%|█████     | 724/1426 [3:49:11<3:45:41, 19.29s/it]                                                      {'loss': 0.6874, 'learning_rate': 1.0238493096707149e-05, 'epoch': 0.51}
 51%|█████     | 724/1426 [3:49:11<3:45:41, 19.29s/it] 51%|█████     | 725/1426 [3:49:28<3:34:26, 18.35s/it]                                                      {'loss': 0.6865, 'learning_rate': 1.0215783180064678e-05, 'epoch': 0.51}
 51%|█████     | 725/1426 [3:49:28<3:34:26, 18.35s/it] 51%|█████     | 726/1426 [3:49:46<3:33:44, 18.32s/it]                                                      {'loss': 0.7033, 'learning_rate': 1.0193072149966901e-05, 'epoch': 0.51}
 51%|█████     | 726/1426 [3:49:46<3:33:44, 18.32s/it] 51%|█████     | 727/1426 [3:50:04<3:32:48, 18.27s/it]                                                      {'loss': 0.7051, 'learning_rate': 1.0170360123604224e-05, 'epoch': 0.51}
 51%|█████     | 727/1426 [3:50:04<3:32:48, 18.27s/it] 51%|█████     | 728/1426 [3:50:24<3:38:27, 18.78s/it]                                                      {'loss': 0.6865, 'learning_rate': 1.0147647218172183e-05, 'epoch': 0.51}
 51%|█████     | 728/1426 [3:50:24<3:38:27, 18.78s/it]WARNING: tokenization mismatch: 1 vs. 64. (ignored)
 51%|█████     | 729/1426 [3:50:45<3:46:29, 19.50s/it]                                                      {'loss': 0.6682, 'learning_rate': 1.0124933550870853e-05, 'epoch': 0.51}
 51%|█████     | 729/1426 [3:50:45<3:46:29, 19.50s/it] 51%|█████     | 730/1426 [3:51:06<3:52:22, 20.03s/it]                                                      {'loss': 0.703, 'learning_rate': 1.0102219238904238e-05, 'epoch': 0.51}
 51%|█████     | 730/1426 [3:51:06<3:52:22, 20.03s/it] 51%|█████▏    | 731/1426 [3:51:25<3:47:26, 19.64s/it]                                                      {'loss': 0.6803, 'learning_rate': 1.0079504399479677e-05, 'epoch': 0.51}
 51%|█████▏    | 731/1426 [3:51:25<3:47:26, 19.64s/it] 51%|█████▏    | 732/1426 [3:51:45<3:49:26, 19.84s/it]                                                      {'loss': 0.6927, 'learning_rate': 1.0056789149807223e-05, 'epoch': 0.51}
 51%|█████▏    | 732/1426 [3:51:45<3:49:26, 19.84s/it] 51%|█████▏    | 733/1426 [3:52:05<3:49:34, 19.88s/it]                                                      {'loss': 0.6581, 'learning_rate': 1.0034073607099046e-05, 'epoch': 0.51}
 51%|█████▏    | 733/1426 [3:52:05<3:49:34, 19.88s/it] 51%|█████▏    | 734/1426 [3:52:24<3:44:46, 19.49s/it]                                                      {'loss': 0.6944, 'learning_rate': 1.0011357888568834e-05, 'epoch': 0.51}
 51%|█████▏    | 734/1426 [3:52:24<3:44:46, 19.49s/it] 52%|█████▏    | 735/1426 [3:52:43<3:43:57, 19.45s/it]                                                      {'loss': 0.6964, 'learning_rate': 9.988642111431171e-06, 'epoch': 0.52}
 52%|█████▏    | 735/1426 [3:52:43<3:43:57, 19.45s/it] 52%|█████▏    | 736/1426 [3:53:02<3:41:29, 19.26s/it]                                                      {'loss': 0.6974, 'learning_rate': 9.965926392900956e-06, 'epoch': 0.52}
 52%|█████▏    | 736/1426 [3:53:02<3:41:29, 19.26s/it] 52%|█████▏    | 737/1426 [3:53:20<3:37:12, 18.91s/it]                                                      {'loss': 0.6799, 'learning_rate': 9.94321085019278e-06, 'epoch': 0.52}
 52%|█████▏    | 737/1426 [3:53:20<3:37:12, 18.91s/it] 52%|█████▏    | 738/1426 [3:53:41<3:41:54, 19.35s/it]                                                      {'loss': 0.7124, 'learning_rate': 9.920495600520322e-06, 'epoch': 0.52}
 52%|█████▏    | 738/1426 [3:53:41<3:41:54, 19.35s/it] 52%|█████▏    | 739/1426 [3:54:04<3:56:54, 20.69s/it]                                                      {'loss': 0.6906, 'learning_rate': 9.897780761095762e-06, 'epoch': 0.52}
 52%|█████▏    | 739/1426 [3:54:04<3:56:54, 20.69s/it] 52%|█████▏    | 740/1426 [3:54:24<3:54:19, 20.50s/it]                                                      {'loss': 0.7031, 'learning_rate': 9.875066449129154e-06, 'epoch': 0.52}
 52%|█████▏    | 740/1426 [3:54:24<3:54:19, 20.50s/it] 52%|█████▏    | 741/1426 [3:54:45<3:53:22, 20.44s/it]                                                      {'loss': 0.6807, 'learning_rate': 9.85235278182782e-06, 'epoch': 0.52}
 52%|█████▏    | 741/1426 [3:54:45<3:53:22, 20.44s/it] 52%|█████▏    | 742/1426 [3:55:07<4:00:10, 21.07s/it]                                                      {'loss': 0.6905, 'learning_rate': 9.829639876395778e-06, 'epoch': 0.52}
 52%|█████▏    | 742/1426 [3:55:07<4:00:10, 21.07s/it] 52%|█████▏    | 743/1426 [3:55:25<3:48:35, 20.08s/it]                                                      {'loss': 0.7076, 'learning_rate': 9.8069278500331e-06, 'epoch': 0.52}
 52%|█████▏    | 743/1426 [3:55:25<3:48:35, 20.08s/it]this iter is wrong in something... skip...
 52%|█████▏    | 744/1426 [3:55:45<3:46:29, 19.93s/it]                                                      {'loss': 0.6738, 'learning_rate': 9.784216819935327e-06, 'epoch': 0.52}
 52%|█████▏    | 744/1426 [3:55:45<3:46:29, 19.93s/it] 52%|█████▏    | 745/1426 [3:56:06<3:51:29, 20.40s/it]                                                      {'loss': 0.6813, 'learning_rate': 9.761506903292855e-06, 'epoch': 0.52}
 52%|█████▏    | 745/1426 [3:56:06<3:51:29, 20.40s/it] 52%|█████▏    | 746/1426 [3:56:27<3:52:04, 20.48s/it]                                                      {'loss': 0.6915, 'learning_rate': 9.738798217290336e-06, 'epoch': 0.52}
 52%|█████▏    | 746/1426 [3:56:27<3:52:04, 20.48s/it] 52%|█████▏    | 747/1426 [3:56:48<3:52:51, 20.58s/it]                                                      {'loss': 0.6998, 'learning_rate': 9.716090879106074e-06, 'epoch': 0.52}
 52%|█████▏    | 747/1426 [3:56:48<3:52:51, 20.58s/it]this iter is wrong in something... skip...
 52%|█████▏    | 748/1426 [3:57:07<3:49:18, 20.29s/it]                                                      {'loss': 0.6606, 'learning_rate': 9.69338500591142e-06, 'epoch': 0.52}
 52%|█████▏    | 748/1426 [3:57:07<3:49:18, 20.29s/it] 53%|█████▎    | 749/1426 [3:57:28<3:51:16, 20.50s/it]                                                      {'loss': 0.679, 'learning_rate': 9.670680714870162e-06, 'epoch': 0.53}
 53%|█████▎    | 749/1426 [3:57:28<3:51:16, 20.50s/it] 53%|█████▎    | 750/1426 [3:57:46<3:41:26, 19.65s/it]                                                      {'loss': 0.6843, 'learning_rate': 9.647978123137919e-06, 'epoch': 0.53}
 53%|█████▎    | 750/1426 [3:57:46<3:41:26, 19.65s/it] 53%|█████▎    | 751/1426 [3:58:05<3:40:57, 19.64s/it]                                                      {'loss': 0.678, 'learning_rate': 9.625277347861554e-06, 'epoch': 0.53}
 53%|█████▎    | 751/1426 [3:58:05<3:40:57, 19.64s/it] 53%|█████▎    | 752/1426 [3:58:27<3:46:52, 20.20s/it]                                                      {'loss': 0.698, 'learning_rate': 9.602578506178546e-06, 'epoch': 0.53}
 53%|█████▎    | 752/1426 [3:58:27<3:46:52, 20.20s/it] 53%|█████▎    | 753/1426 [3:58:47<3:47:00, 20.24s/it]                                                      {'loss': 0.6999, 'learning_rate': 9.579881715216404e-06, 'epoch': 0.53}
 53%|█████▎    | 753/1426 [3:58:47<3:47:00, 20.24s/it] 53%|█████▎    | 754/1426 [3:59:07<3:44:53, 20.08s/it]                                                      {'loss': 0.6963, 'learning_rate': 9.557187092092046e-06, 'epoch': 0.53}
 53%|█████▎    | 754/1426 [3:59:07<3:44:53, 20.08s/it] 53%|█████▎    | 755/1426 [3:59:25<3:38:05, 19.50s/it]                                                      {'loss': 0.697, 'learning_rate': 9.534494753911214e-06, 'epoch': 0.53}
 53%|█████▎    | 755/1426 [3:59:25<3:38:05, 19.50s/it] 53%|█████▎    | 756/1426 [3:59:46<3:41:49, 19.86s/it]                                                      {'loss': 0.6814, 'learning_rate': 9.51180481776785e-06, 'epoch': 0.53}
 53%|█████▎    | 756/1426 [3:59:46<3:41:49, 19.86s/it] 53%|█████▎    | 757/1426 [4:00:05<3:37:20, 19.49s/it]                                                      {'loss': 0.7015, 'learning_rate': 9.489117400743515e-06, 'epoch': 0.53}
 53%|█████▎    | 757/1426 [4:00:05<3:37:20, 19.49s/it]WARNING: tokenization mismatch: 1 vs. 70. (ignored)
 53%|█████▎    | 758/1426 [4:00:19<3:19:11, 17.89s/it]                                                      {'loss': 0.2375, 'learning_rate': 9.466432619906758e-06, 'epoch': 0.53}
 53%|█████▎    | 758/1426 [4:00:19<3:19:11, 17.89s/it] 53%|█████▎    | 759/1426 [4:00:38<3:22:21, 18.20s/it]                                                      {'loss': 0.6936, 'learning_rate': 9.443750592312528e-06, 'epoch': 0.53}
 53%|█████▎    | 759/1426 [4:00:38<3:22:21, 18.20s/it] 53%|█████▎    | 760/1426 [4:00:57<3:24:48, 18.45s/it]                                                      {'loss': 0.686, 'learning_rate': 9.42107143500157e-06, 'epoch': 0.53}
 53%|█████▎    | 760/1426 [4:00:57<3:24:48, 18.45s/it] 53%|█████▎    | 761/1426 [4:01:17<3:30:47, 19.02s/it]                                                      {'loss': 0.6774, 'learning_rate': 9.398395264999817e-06, 'epoch': 0.53}
 53%|█████▎    | 761/1426 [4:01:17<3:30:47, 19.02s/it] 53%|█████▎    | 762/1426 [4:01:36<3:31:05, 19.07s/it]                                                      {'loss': 0.6928, 'learning_rate': 9.375722199317785e-06, 'epoch': 0.53}
 53%|█████▎    | 762/1426 [4:01:36<3:31:05, 19.07s/it] 54%|█████▎    | 763/1426 [4:01:53<3:24:24, 18.50s/it]                                                      {'loss': 0.6997, 'learning_rate': 9.353052354949982e-06, 'epoch': 0.53}
 54%|█████▎    | 763/1426 [4:01:53<3:24:24, 18.50s/it] 54%|█████▎    | 764/1426 [4:02:11<3:22:18, 18.34s/it]                                                      {'loss': 0.7115, 'learning_rate': 9.330385848874275e-06, 'epoch': 0.54}
 54%|█████▎    | 764/1426 [4:02:11<3:22:18, 18.34s/it]this iter is wrong in something... skip...
 54%|█████▎    | 765/1426 [4:02:29<3:18:31, 18.02s/it]                                                      {'loss': 0.6717, 'learning_rate': 9.307722798051318e-06, 'epoch': 0.54}
 54%|█████▎    | 765/1426 [4:02:29<3:18:31, 18.02s/it] 54%|█████▎    | 766/1426 [4:02:49<3:25:48, 18.71s/it]                                                      {'loss': 0.681, 'learning_rate': 9.28506331942394e-06, 'epoch': 0.54}
 54%|█████▎    | 766/1426 [4:02:49<3:25:48, 18.71s/it] 54%|█████▍    | 767/1426 [4:03:09<3:29:51, 19.11s/it]                                                      {'loss': 0.6792, 'learning_rate': 9.262407529916525e-06, 'epoch': 0.54}
 54%|█████▍    | 767/1426 [4:03:09<3:29:51, 19.11s/it] 54%|█████▍    | 768/1426 [4:03:31<3:38:17, 19.91s/it]                                                      {'loss': 0.7041, 'learning_rate': 9.239755546434427e-06, 'epoch': 0.54}
 54%|█████▍    | 768/1426 [4:03:31<3:38:17, 19.91s/it] 54%|█████▍    | 769/1426 [4:03:48<3:29:36, 19.14s/it]                                                      {'loss': 0.6932, 'learning_rate': 9.21710748586336e-06, 'epoch': 0.54}
 54%|█████▍    | 769/1426 [4:03:48<3:29:36, 19.14s/it] 54%|█████▍    | 770/1426 [4:04:07<3:28:15, 19.05s/it]                                                      {'loss': 0.6734, 'learning_rate': 9.194463465068797e-06, 'epoch': 0.54}
 54%|█████▍    | 770/1426 [4:04:07<3:28:15, 19.05s/it] 54%|█████▍    | 771/1426 [4:04:25<3:24:26, 18.73s/it]                                                      {'loss': 0.6835, 'learning_rate': 9.171823600895361e-06, 'epoch': 0.54}
 54%|█████▍    | 771/1426 [4:04:25<3:24:26, 18.73s/it] 54%|█████▍    | 772/1426 [4:04:44<3:26:34, 18.95s/it]                                                      {'loss': 0.6728, 'learning_rate': 9.14918801016624e-06, 'epoch': 0.54}
 54%|█████▍    | 772/1426 [4:04:44<3:26:34, 18.95s/it] 54%|█████▍    | 773/1426 [4:05:03<3:24:53, 18.83s/it]                                                      {'loss': 0.6955, 'learning_rate': 9.126556809682546e-06, 'epoch': 0.54}
 54%|█████▍    | 773/1426 [4:05:03<3:24:53, 18.83s/it] 54%|█████▍    | 774/1426 [4:05:23<3:30:01, 19.33s/it]                                                      {'loss': 0.6702, 'learning_rate': 9.103930116222758e-06, 'epoch': 0.54}
 54%|█████▍    | 774/1426 [4:05:23<3:30:01, 19.33s/it] 54%|█████▍    | 775/1426 [4:05:43<3:30:35, 19.41s/it]                                                      {'loss': 0.6844, 'learning_rate': 9.081308046542095e-06, 'epoch': 0.54}
 54%|█████▍    | 775/1426 [4:05:43<3:30:35, 19.41s/it] 54%|█████▍    | 776/1426 [4:06:00<3:22:03, 18.65s/it]                                                      {'loss': 0.6704, 'learning_rate': 9.058690717371911e-06, 'epoch': 0.54}
 54%|█████▍    | 776/1426 [4:06:00<3:22:03, 18.65s/it] 54%|█████▍    | 777/1426 [4:06:18<3:21:05, 18.59s/it]                                                      {'loss': 0.7056, 'learning_rate': 9.036078245419102e-06, 'epoch': 0.54}
 54%|█████▍    | 777/1426 [4:06:18<3:21:05, 18.59s/it] 55%|█████▍    | 778/1426 [4:06:36<3:18:16, 18.36s/it]                                                      {'loss': 0.6995, 'learning_rate': 9.0134707473655e-06, 'epoch': 0.55}
 55%|█████▍    | 778/1426 [4:06:36<3:18:16, 18.36s/it] 55%|█████▍    | 779/1426 [4:06:53<3:13:47, 17.97s/it]                                                      {'loss': 0.7167, 'learning_rate': 8.99086833986727e-06, 'epoch': 0.55}
 55%|█████▍    | 779/1426 [4:06:53<3:13:47, 17.97s/it] 55%|█████▍    | 780/1426 [4:07:13<3:20:44, 18.64s/it]                                                      {'loss': 0.6895, 'learning_rate': 8.968271139554312e-06, 'epoch': 0.55}
 55%|█████▍    | 780/1426 [4:07:13<3:20:44, 18.64s/it] 55%|█████▍    | 781/1426 [4:07:28<3:05:50, 17.29s/it]                                                      {'loss': 0.2502, 'learning_rate': 8.94567926302966e-06, 'epoch': 0.55}
 55%|█████▍    | 781/1426 [4:07:28<3:05:50, 17.29s/it] 55%|█████▍    | 782/1426 [4:07:44<3:03:27, 17.09s/it]                                                      {'loss': 0.6775, 'learning_rate': 8.923092826868865e-06, 'epoch': 0.55}
 55%|█████▍    | 782/1426 [4:07:44<3:03:27, 17.09s/it] 55%|█████▍    | 783/1426 [4:08:05<3:16:13, 18.31s/it]                                                      {'loss': 0.709, 'learning_rate': 8.900511947619415e-06, 'epoch': 0.55}
 55%|█████▍    | 783/1426 [4:08:05<3:16:13, 18.31s/it] 55%|█████▍    | 784/1426 [4:08:26<3:21:58, 18.88s/it]                                                      {'loss': 0.6965, 'learning_rate': 8.877936741800124e-06, 'epoch': 0.55}
 55%|█████▍    | 784/1426 [4:08:26<3:21:58, 18.88s/it] 55%|█████▌    | 785/1426 [4:08:45<3:25:13, 19.21s/it]                                                      {'loss': 0.6622, 'learning_rate': 8.855367325900526e-06, 'epoch': 0.55}
 55%|█████▌    | 785/1426 [4:08:46<3:25:13, 19.21s/it] 55%|█████▌    | 786/1426 [4:09:04<3:21:05, 18.85s/it]                                                      {'loss': 0.7086, 'learning_rate': 8.832803816380285e-06, 'epoch': 0.55}
 55%|█████▌    | 786/1426 [4:09:04<3:21:05, 18.85s/it] 55%|█████▌    | 787/1426 [4:09:24<3:25:49, 19.33s/it]                                                      {'loss': 0.7211, 'learning_rate': 8.810246329668577e-06, 'epoch': 0.55}
 55%|█████▌    | 787/1426 [4:09:24<3:25:49, 19.33s/it] 55%|█████▌    | 788/1426 [4:09:43<3:23:02, 19.09s/it]                                                      {'loss': 0.6949, 'learning_rate': 8.78769498216351e-06, 'epoch': 0.55}
 55%|█████▌    | 788/1426 [4:09:43<3:23:02, 19.09s/it] 55%|█████▌    | 789/1426 [4:10:00<3:16:36, 18.52s/it]                                                      {'loss': 0.6999, 'learning_rate': 8.765149890231512e-06, 'epoch': 0.55}
 55%|█████▌    | 789/1426 [4:10:00<3:16:36, 18.52s/it] 55%|█████▌    | 790/1426 [4:10:22<3:29:04, 19.72s/it]                                                      {'loss': 0.6657, 'learning_rate': 8.742611170206732e-06, 'epoch': 0.55}
 55%|█████▌    | 790/1426 [4:10:22<3:29:04, 19.72s/it] 55%|█████▌    | 791/1426 [4:10:42<3:28:00, 19.65s/it]                                                      {'loss': 0.6945, 'learning_rate': 8.720078938390426e-06, 'epoch': 0.55}
 55%|█████▌    | 791/1426 [4:10:42<3:28:00, 19.65s/it] 56%|█████▌    | 792/1426 [4:11:02<3:29:39, 19.84s/it]                                                      {'loss': 0.7023, 'learning_rate': 8.697553311050393e-06, 'epoch': 0.56}
 56%|█████▌    | 792/1426 [4:11:02<3:29:39, 19.84s/it] 56%|█████▌    | 793/1426 [4:11:16<3:11:09, 18.12s/it]                                                      {'loss': 0.2432, 'learning_rate': 8.675034404420337e-06, 'epoch': 0.56}
 56%|█████▌    | 793/1426 [4:11:16<3:11:09, 18.12s/it] 56%|█████▌    | 794/1426 [4:11:37<3:19:08, 18.91s/it]                                                      {'loss': 0.7081, 'learning_rate': 8.652522334699286e-06, 'epoch': 0.56}
 56%|█████▌    | 794/1426 [4:11:37<3:19:08, 18.91s/it] 56%|█████▌    | 795/1426 [4:11:51<3:03:12, 17.42s/it]                                                      {'loss': 0.2432, 'learning_rate': 8.630017218050994e-06, 'epoch': 0.56}
 56%|█████▌    | 795/1426 [4:11:51<3:03:12, 17.42s/it] 56%|█████▌    | 796/1426 [4:12:12<3:15:17, 18.60s/it]                                                      {'loss': 0.7088, 'learning_rate': 8.607519170603329e-06, 'epoch': 0.56}
 56%|█████▌    | 796/1426 [4:12:12<3:15:17, 18.60s/it] 56%|█████▌    | 797/1426 [4:12:32<3:18:11, 18.91s/it]                                                      {'loss': 0.6997, 'learning_rate': 8.585028308447685e-06, 'epoch': 0.56}
 56%|█████▌    | 797/1426 [4:12:32<3:18:11, 18.91s/it] 56%|█████▌    | 798/1426 [4:12:49<3:13:25, 18.48s/it]                                                      {'loss': 0.6828, 'learning_rate': 8.56254474763838e-06, 'epoch': 0.56}
 56%|█████▌    | 798/1426 [4:12:49<3:13:25, 18.48s/it] 56%|█████▌    | 799/1426 [4:13:10<3:20:24, 19.18s/it]                                                      {'loss': 0.6664, 'learning_rate': 8.540068604192061e-06, 'epoch': 0.56}
 56%|█████▌    | 799/1426 [4:13:10<3:20:24, 19.18s/it] 56%|█████▌    | 800/1426 [4:13:29<3:19:03, 19.08s/it]                                                      {'loss': 0.6676, 'learning_rate': 8.517599994087086e-06, 'epoch': 0.56}
 56%|█████▌    | 800/1426 [4:13:29<3:19:03, 19.08s/it] 56%|█████▌    | 801/1426 [4:13:48<3:18:37, 19.07s/it]                                                      {'loss': 0.6796, 'learning_rate': 8.49513903326296e-06, 'epoch': 0.56}
 56%|█████▌    | 801/1426 [4:13:48<3:18:37, 19.07s/it] 56%|█████▌    | 802/1426 [4:14:09<3:25:59, 19.81s/it]                                                      {'loss': 0.6801, 'learning_rate': 8.472685837619706e-06, 'epoch': 0.56}
 56%|█████▌    | 802/1426 [4:14:09<3:25:59, 19.81s/it] 56%|█████▋    | 803/1426 [4:14:30<3:27:39, 20.00s/it]                                                      {'loss': 0.7169, 'learning_rate': 8.450240523017275e-06, 'epoch': 0.56}
 56%|█████▋    | 803/1426 [4:14:30<3:27:39, 20.00s/it] 56%|█████▋    | 804/1426 [4:14:53<3:37:55, 21.02s/it]                                                      {'loss': 0.7108, 'learning_rate': 8.427803205274963e-06, 'epoch': 0.56}
 56%|█████▋    | 804/1426 [4:14:53<3:37:55, 21.02s/it] 56%|█████▋    | 805/1426 [4:15:12<3:31:17, 20.41s/it]                                                      {'loss': 0.6894, 'learning_rate': 8.40537400017079e-06, 'epoch': 0.56}
 56%|█████▋    | 805/1426 [4:15:12<3:31:17, 20.41s/it] 57%|█████▋    | 806/1426 [4:15:32<3:29:39, 20.29s/it]                                                      {'loss': 0.6788, 'learning_rate': 8.382953023440916e-06, 'epoch': 0.57}
 57%|█████▋    | 806/1426 [4:15:32<3:29:39, 20.29s/it] 57%|█████▋    | 807/1426 [4:15:50<3:22:25, 19.62s/it]                                                      {'loss': 0.6905, 'learning_rate': 8.360540390779055e-06, 'epoch': 0.57}
 57%|█████▋    | 807/1426 [4:15:50<3:22:25, 19.62s/it] 57%|█████▋    | 808/1426 [4:16:10<3:21:15, 19.54s/it]                                                      {'loss': 0.6546, 'learning_rate': 8.338136217835848e-06, 'epoch': 0.57}
 57%|█████▋    | 808/1426 [4:16:10<3:21:15, 19.54s/it] 57%|█████▋    | 809/1426 [4:16:28<3:17:28, 19.20s/it]                                                      {'loss': 0.6887, 'learning_rate': 8.315740620218295e-06, 'epoch': 0.57}
 57%|█████▋    | 809/1426 [4:16:28<3:17:28, 19.20s/it] 57%|█████▋    | 810/1426 [4:16:47<3:16:07, 19.10s/it]                                                      {'loss': 0.6761, 'learning_rate': 8.293353713489139e-06, 'epoch': 0.57}
 57%|█████▋    | 810/1426 [4:16:47<3:16:07, 19.10s/it] 57%|█████▋    | 811/1426 [4:17:09<3:23:37, 19.87s/it]                                                      {'loss': 0.6891, 'learning_rate': 8.270975613166282e-06, 'epoch': 0.57}
 57%|█████▋    | 811/1426 [4:17:09<3:23:37, 19.87s/it] 57%|█████▋    | 812/1426 [4:17:28<3:21:53, 19.73s/it]                                                      {'loss': 0.6772, 'learning_rate': 8.248606434722183e-06, 'epoch': 0.57}
 57%|█████▋    | 812/1426 [4:17:28<3:21:53, 19.73s/it] 57%|█████▋    | 813/1426 [4:17:49<3:24:26, 20.01s/it]                                                      {'loss': 0.706, 'learning_rate': 8.226246293583266e-06, 'epoch': 0.57}
 57%|█████▋    | 813/1426 [4:17:49<3:24:26, 20.01s/it] 57%|█████▋    | 814/1426 [4:18:07<3:17:28, 19.36s/it]                                                      {'loss': 0.6732, 'learning_rate': 8.203895305129315e-06, 'epoch': 0.57}
 57%|█████▋    | 814/1426 [4:18:07<3:17:28, 19.36s/it] 57%|█████▋    | 815/1426 [4:18:26<3:16:25, 19.29s/it]                                                      {'loss': 0.6761, 'learning_rate': 8.18155358469289e-06, 'epoch': 0.57}
 57%|█████▋    | 815/1426 [4:18:26<3:16:25, 19.29s/it] 57%|█████▋    | 816/1426 [4:18:44<3:14:26, 19.13s/it]                                                      {'loss': 0.651, 'learning_rate': 8.159221247558734e-06, 'epoch': 0.57}
 57%|█████▋    | 816/1426 [4:18:44<3:14:26, 19.13s/it] 57%|█████▋    | 817/1426 [4:19:04<3:16:46, 19.39s/it]                                                      {'loss': 0.6891, 'learning_rate': 8.136898408963158e-06, 'epoch': 0.57}
 57%|█████▋    | 817/1426 [4:19:04<3:16:46, 19.39s/it] 57%|█████▋    | 818/1426 [4:19:24<3:16:46, 19.42s/it]                                                      {'loss': 0.7039, 'learning_rate': 8.114585184093476e-06, 'epoch': 0.57}
 57%|█████▋    | 818/1426 [4:19:24<3:16:46, 19.42s/it] 57%|█████▋    | 819/1426 [4:19:43<3:15:15, 19.30s/it]                                                      {'loss': 0.6782, 'learning_rate': 8.092281688087376e-06, 'epoch': 0.57}
 57%|█████▋    | 819/1426 [4:19:43<3:15:15, 19.30s/it] 58%|█████▊    | 820/1426 [4:20:02<3:12:53, 19.10s/it]                                                      {'loss': 0.6906, 'learning_rate': 8.069988036032358e-06, 'epoch': 0.57}
 58%|█████▊    | 820/1426 [4:20:02<3:12:53, 19.10s/it] 58%|█████▊    | 821/1426 [4:20:19<3:06:53, 18.53s/it]                                                      {'loss': 0.6637, 'learning_rate': 8.047704342965121e-06, 'epoch': 0.58}
 58%|█████▊    | 821/1426 [4:20:19<3:06:53, 18.53s/it] 58%|█████▊    | 822/1426 [4:20:40<3:15:43, 19.44s/it]                                                      {'loss': 0.6666, 'learning_rate': 8.025430723870982e-06, 'epoch': 0.58}
 58%|█████▊    | 822/1426 [4:20:40<3:15:43, 19.44s/it] 58%|█████▊    | 823/1426 [4:20:58<3:10:47, 18.98s/it]                                                      {'loss': 0.679, 'learning_rate': 8.003167293683265e-06, 'epoch': 0.58}
 58%|█████▊    | 823/1426 [4:20:58<3:10:47, 18.98s/it] 58%|█████▊    | 824/1426 [4:21:17<3:09:17, 18.87s/it]                                                      {'loss': 0.7025, 'learning_rate': 7.980914167282722e-06, 'epoch': 0.58}
 58%|█████▊    | 824/1426 [4:21:17<3:09:17, 18.87s/it] 58%|█████▊    | 825/1426 [4:21:37<3:13:04, 19.27s/it]                                                      {'loss': 0.695, 'learning_rate': 7.958671459496944e-06, 'epoch': 0.58}
 58%|█████▊    | 825/1426 [4:21:37<3:13:04, 19.27s/it] 58%|█████▊    | 826/1426 [4:21:56<3:13:01, 19.30s/it]                                                      {'loss': 0.68, 'learning_rate': 7.936439285099753e-06, 'epoch': 0.58}
 58%|█████▊    | 826/1426 [4:21:56<3:13:01, 19.30s/it] 58%|█████▊    | 827/1426 [4:22:15<3:11:26, 19.18s/it]                                                      {'loss': 0.6613, 'learning_rate': 7.914217758810625e-06, 'epoch': 0.58}
 58%|█████▊    | 827/1426 [4:22:15<3:11:26, 19.18s/it] 58%|█████▊    | 828/1426 [4:22:34<3:10:11, 19.08s/it]                                                      {'loss': 0.6667, 'learning_rate': 7.892006995294079e-06, 'epoch': 0.58}
 58%|█████▊    | 828/1426 [4:22:34<3:10:11, 19.08s/it] 58%|█████▊    | 829/1426 [4:22:53<3:10:27, 19.14s/it]                                                      {'loss': 0.6772, 'learning_rate': 7.86980710915911e-06, 'epoch': 0.58}
 58%|█████▊    | 829/1426 [4:22:54<3:10:27, 19.14s/it] 58%|█████▊    | 830/1426 [4:23:13<3:11:37, 19.29s/it]                                                      {'loss': 0.684, 'learning_rate': 7.847618214958582e-06, 'epoch': 0.58}
 58%|█████▊    | 830/1426 [4:23:13<3:11:37, 19.29s/it] 58%|█████▊    | 831/1426 [4:23:32<3:10:46, 19.24s/it]                                                      {'loss': 0.6901, 'learning_rate': 7.825440427188635e-06, 'epoch': 0.58}
 58%|█████▊    | 831/1426 [4:23:32<3:10:46, 19.24s/it] 58%|█████▊    | 832/1426 [4:23:52<3:11:39, 19.36s/it]                                                      {'loss': 0.6604, 'learning_rate': 7.803273860288105e-06, 'epoch': 0.58}
 58%|█████▊    | 832/1426 [4:23:52<3:11:39, 19.36s/it] 58%|█████▊    | 833/1426 [4:24:10<3:07:25, 18.96s/it]                                                      {'loss': 0.6895, 'learning_rate': 7.781118628637918e-06, 'epoch': 0.58}
 58%|█████▊    | 833/1426 [4:24:10<3:07:25, 18.96s/it] 58%|█████▊    | 834/1426 [4:24:27<3:01:13, 18.37s/it]                                                      {'loss': 0.6442, 'learning_rate': 7.758974846560525e-06, 'epoch': 0.58}
 58%|█████▊    | 834/1426 [4:24:27<3:01:13, 18.37s/it] 59%|█████▊    | 835/1426 [4:24:47<3:06:21, 18.92s/it]                                                      {'loss': 0.7029, 'learning_rate': 7.736842628319286e-06, 'epoch': 0.59}
 59%|█████▊    | 835/1426 [4:24:47<3:06:21, 18.92s/it] 59%|█████▊    | 836/1426 [4:25:09<3:15:19, 19.86s/it]                                                      {'loss': 0.6911, 'learning_rate': 7.71472208811789e-06, 'epoch': 0.59}
 59%|█████▊    | 836/1426 [4:25:09<3:15:19, 19.86s/it] 59%|█████▊    | 837/1426 [4:25:27<3:08:23, 19.19s/it]                                                      {'loss': 0.6741, 'learning_rate': 7.69261334009977e-06, 'epoch': 0.59}
 59%|█████▊    | 837/1426 [4:25:27<3:08:23, 19.19s/it] 59%|█████▉    | 838/1426 [4:25:45<3:03:57, 18.77s/it]                                                      {'loss': 0.6825, 'learning_rate': 7.67051649834751e-06, 'epoch': 0.59}
 59%|█████▉    | 838/1426 [4:25:45<3:03:57, 18.77s/it] 59%|█████▉    | 839/1426 [4:26:05<3:07:43, 19.19s/it]                                                      {'loss': 0.6816, 'learning_rate': 7.648431676882254e-06, 'epoch': 0.59}
 59%|█████▉    | 839/1426 [4:26:05<3:07:43, 19.19s/it] 59%|█████▉    | 840/1426 [4:26:23<3:03:31, 18.79s/it]                                                      {'loss': 0.661, 'learning_rate': 7.626358989663127e-06, 'epoch': 0.59}
 59%|█████▉    | 840/1426 [4:26:23<3:03:31, 18.79s/it] 59%|█████▉    | 841/1426 [4:26:40<3:00:14, 18.49s/it]                                                      {'loss': 0.6566, 'learning_rate': 7.604298550586639e-06, 'epoch': 0.59}
 59%|█████▉    | 841/1426 [4:26:40<3:00:14, 18.49s/it] 59%|█████▉    | 842/1426 [4:27:00<3:04:19, 18.94s/it]                                                      {'loss': 0.6697, 'learning_rate': 7.582250473486087e-06, 'epoch': 0.59}
 59%|█████▉    | 842/1426 [4:27:00<3:04:19, 18.94s/it]this iter is wrong in something... skip...
 59%|█████▉    | 843/1426 [4:27:20<3:06:28, 19.19s/it]                                                      {'loss': 0.6684, 'learning_rate': 7.560214872130997e-06, 'epoch': 0.59}
 59%|█████▉    | 843/1426 [4:27:20<3:06:28, 19.19s/it] 59%|█████▉    | 844/1426 [4:27:41<3:11:25, 19.73s/it]                                                      {'loss': 0.6604, 'learning_rate': 7.5381918602265066e-06, 'epoch': 0.59}
 59%|█████▉    | 844/1426 [4:27:41<3:11:25, 19.73s/it] 59%|█████▉    | 845/1426 [4:28:00<3:09:13, 19.54s/it]                                                      {'loss': 0.6592, 'learning_rate': 7.5161815514128e-06, 'epoch': 0.59}
 59%|█████▉    | 845/1426 [4:28:00<3:09:13, 19.54s/it] 59%|█████▉    | 846/1426 [4:28:21<3:11:26, 19.80s/it]                                                      {'loss': 0.6503, 'learning_rate': 7.494184059264505e-06, 'epoch': 0.59}
 59%|█████▉    | 846/1426 [4:28:21<3:11:26, 19.80s/it] 59%|█████▉    | 847/1426 [4:28:38<3:04:24, 19.11s/it]                                                      {'loss': 0.6865, 'learning_rate': 7.472199497290116e-06, 'epoch': 0.59}
 59%|█████▉    | 847/1426 [4:28:38<3:04:24, 19.11s/it] 59%|█████▉    | 848/1426 [4:28:59<3:08:12, 19.54s/it]                                                      {'loss': 0.7037, 'learning_rate': 7.45022797893141e-06, 'epoch': 0.59}
 59%|█████▉    | 848/1426 [4:28:59<3:08:12, 19.54s/it] 60%|█████▉    | 849/1426 [4:29:17<3:04:29, 19.18s/it]                                                      {'loss': 0.7019, 'learning_rate': 7.428269617562859e-06, 'epoch': 0.6}
 60%|█████▉    | 849/1426 [4:29:17<3:04:29, 19.18s/it] 60%|█████▉    | 850/1426 [4:29:37<3:07:17, 19.51s/it]                                                      {'loss': 0.6748, 'learning_rate': 7.4063245264910444e-06, 'epoch': 0.6}
 60%|█████▉    | 850/1426 [4:29:37<3:07:17, 19.51s/it] 60%|█████▉    | 851/1426 [4:29:56<3:03:27, 19.14s/it]                                                      {'loss': 0.673, 'learning_rate': 7.384392818954059e-06, 'epoch': 0.6}
 60%|█████▉    | 851/1426 [4:29:56<3:03:27, 19.14s/it] 60%|█████▉    | 852/1426 [4:30:17<3:10:17, 19.89s/it]                                                      {'loss': 0.6615, 'learning_rate': 7.362474608120955e-06, 'epoch': 0.6}
 60%|█████▉    | 852/1426 [4:30:17<3:10:17, 19.89s/it] 60%|█████▉    | 853/1426 [4:30:37<3:10:08, 19.91s/it]                                                      {'loss': 0.6696, 'learning_rate': 7.340570007091128e-06, 'epoch': 0.6}
 60%|█████▉    | 853/1426 [4:30:37<3:10:08, 19.91s/it] 60%|█████▉    | 854/1426 [4:30:56<3:06:32, 19.57s/it]                                                      {'loss': 0.6764, 'learning_rate': 7.318679128893754e-06, 'epoch': 0.6}
 60%|█████▉    | 854/1426 [4:30:56<3:06:32, 19.57s/it] 60%|█████▉    | 855/1426 [4:31:15<3:04:52, 19.43s/it]                                                      {'loss': 0.6857, 'learning_rate': 7.296802086487189e-06, 'epoch': 0.6}
 60%|█████▉    | 855/1426 [4:31:15<3:04:52, 19.43s/it]this iter is wrong in something... skip...
 60%|██████    | 856/1426 [4:31:34<3:03:17, 19.29s/it]                                                      {'loss': 0.6697, 'learning_rate': 7.274938992758402e-06, 'epoch': 0.6}
 60%|██████    | 856/1426 [4:31:34<3:03:17, 19.29s/it] 60%|██████    | 857/1426 [4:31:55<3:08:32, 19.88s/it]                                                      {'loss': 0.6888, 'learning_rate': 7.253089960522384e-06, 'epoch': 0.6}
 60%|██████    | 857/1426 [4:31:55<3:08:32, 19.88s/it] 60%|██████    | 858/1426 [4:32:18<3:16:32, 20.76s/it]                                                      {'loss': 0.6646, 'learning_rate': 7.23125510252157e-06, 'epoch': 0.6}
 60%|██████    | 858/1426 [4:32:18<3:16:32, 20.76s/it] 60%|██████    | 859/1426 [4:32:38<3:13:16, 20.45s/it]                                                      {'loss': 0.6739, 'learning_rate': 7.209434531425257e-06, 'epoch': 0.6}
 60%|██████    | 859/1426 [4:32:38<3:13:16, 20.45s/it] 60%|██████    | 860/1426 [4:32:55<3:04:19, 19.54s/it]                                                      {'loss': 0.6651, 'learning_rate': 7.187628359829009e-06, 'epoch': 0.6}
 60%|██████    | 860/1426 [4:32:55<3:04:19, 19.54s/it] 60%|██████    | 861/1426 [4:33:15<3:05:36, 19.71s/it]                                                      {'loss': 0.6895, 'learning_rate': 7.1658367002541e-06, 'epoch': 0.6}
 60%|██████    | 861/1426 [4:33:15<3:05:36, 19.71s/it] 60%|██████    | 862/1426 [4:33:34<3:03:11, 19.49s/it]                                                      {'loss': 0.6788, 'learning_rate': 7.144059665146917e-06, 'epoch': 0.6}
 60%|██████    | 862/1426 [4:33:34<3:03:11, 19.49s/it] 61%|██████    | 863/1426 [4:33:48<2:47:29, 17.85s/it]                                                      {'loss': 0.2348, 'learning_rate': 7.122297366878379e-06, 'epoch': 0.6}
 61%|██████    | 863/1426 [4:33:48<2:47:29, 17.85s/it] 61%|██████    | 864/1426 [4:34:10<2:57:58, 19.00s/it]                                                      {'loss': 0.677, 'learning_rate': 7.100549917743375e-06, 'epoch': 0.61}
 61%|██████    | 864/1426 [4:34:10<2:57:58, 19.00s/it] 61%|██████    | 865/1426 [4:34:30<2:59:03, 19.15s/it]                                                      {'loss': 0.6858, 'learning_rate': 7.078817429960156e-06, 'epoch': 0.61}
 61%|██████    | 865/1426 [4:34:30<2:59:03, 19.15s/it] 61%|██████    | 866/1426 [4:34:52<3:06:35, 19.99s/it]                                                      {'loss': 0.6624, 'learning_rate': 7.057100015669776e-06, 'epoch': 0.61}
 61%|██████    | 866/1426 [4:34:52<3:06:35, 19.99s/it] 61%|██████    | 867/1426 [4:35:13<3:10:51, 20.49s/it]                                                      {'loss': 0.6797, 'learning_rate': 7.035397786935518e-06, 'epoch': 0.61}
 61%|██████    | 867/1426 [4:35:13<3:10:51, 20.49s/it] 61%|██████    | 868/1426 [4:35:34<3:10:10, 20.45s/it]                                                      {'loss': 0.6873, 'learning_rate': 7.013710855742299e-06, 'epoch': 0.61}
 61%|██████    | 868/1426 [4:35:34<3:10:10, 20.45s/it] 61%|██████    | 869/1426 [4:35:51<3:02:18, 19.64s/it]                                                      {'loss': 0.6654, 'learning_rate': 6.992039333996094e-06, 'epoch': 0.61}
 61%|██████    | 869/1426 [4:35:51<3:02:18, 19.64s/it] 61%|██████    | 870/1426 [4:36:11<3:03:11, 19.77s/it]                                                      {'loss': 0.6746, 'learning_rate': 6.970383333523376e-06, 'epoch': 0.61}
 61%|██████    | 870/1426 [4:36:11<3:03:11, 19.77s/it] 61%|██████    | 871/1426 [4:36:31<3:03:14, 19.81s/it]                                                      {'loss': 0.6785, 'learning_rate': 6.948742966070521e-06, 'epoch': 0.61}
 61%|██████    | 871/1426 [4:36:31<3:03:14, 19.81s/it] 61%|██████    | 872/1426 [4:36:51<3:03:27, 19.87s/it]                                                      {'loss': 0.6759, 'learning_rate': 6.927118343303234e-06, 'epoch': 0.61}
 61%|██████    | 872/1426 [4:36:51<3:03:27, 19.87s/it] 61%|██████    | 873/1426 [4:37:10<2:59:29, 19.47s/it]                                                      {'loss': 0.6816, 'learning_rate': 6.905509576805992e-06, 'epoch': 0.61}
 61%|██████    | 873/1426 [4:37:10<2:59:29, 19.47s/it] 61%|██████▏   | 874/1426 [4:37:27<2:54:02, 18.92s/it]                                                      {'loss': 0.6882, 'learning_rate': 6.8839167780814295e-06, 'epoch': 0.61}
 61%|██████▏   | 874/1426 [4:37:27<2:54:02, 18.92s/it] 61%|██████▏   | 875/1426 [4:37:47<2:55:31, 19.11s/it]                                                      {'loss': 0.6783, 'learning_rate': 6.8623400585498e-06, 'epoch': 0.61}
 61%|██████▏   | 875/1426 [4:37:47<2:55:31, 19.11s/it] 61%|██████▏   | 876/1426 [4:38:06<2:56:14, 19.23s/it]                                                      {'loss': 0.6905, 'learning_rate': 6.840779529548389e-06, 'epoch': 0.61}
 61%|██████▏   | 876/1426 [4:38:06<2:56:14, 19.23s/it] 62%|██████▏   | 877/1426 [4:38:26<2:56:36, 19.30s/it]                                                      {'loss': 0.6591, 'learning_rate': 6.819235302330938e-06, 'epoch': 0.61}
 62%|██████▏   | 877/1426 [4:38:26<2:56:36, 19.30s/it] 62%|██████▏   | 878/1426 [4:38:44<2:51:40, 18.80s/it]                                                      {'loss': 0.6788, 'learning_rate': 6.797707488067058e-06, 'epoch': 0.62}
 62%|██████▏   | 878/1426 [4:38:44<2:51:40, 18.80s/it] 62%|██████▏   | 879/1426 [4:39:02<2:50:54, 18.75s/it]                                                      {'loss': 0.6929, 'learning_rate': 6.77619619784168e-06, 'epoch': 0.62}
 62%|██████▏   | 879/1426 [4:39:02<2:50:54, 18.75s/it] 62%|██████▏   | 880/1426 [4:39:22<2:53:48, 19.10s/it]                                                      {'loss': 0.6907, 'learning_rate': 6.754701542654467e-06, 'epoch': 0.62}
 62%|██████▏   | 880/1426 [4:39:22<2:53:48, 19.10s/it] 62%|██████▏   | 881/1426 [4:39:45<3:03:10, 20.17s/it]                                                      {'loss': 0.661, 'learning_rate': 6.733223633419241e-06, 'epoch': 0.62}
 62%|██████▏   | 881/1426 [4:39:45<3:03:10, 20.17s/it] 62%|██████▏   | 882/1426 [4:40:03<2:57:55, 19.62s/it]                                                      {'loss': 0.6936, 'learning_rate': 6.7117625809634255e-06, 'epoch': 0.62}
 62%|██████▏   | 882/1426 [4:40:03<2:57:55, 19.62s/it] 62%|██████▏   | 883/1426 [4:40:23<2:57:06, 19.57s/it]                                                      {'loss': 0.6859, 'learning_rate': 6.690318496027439e-06, 'epoch': 0.62}
 62%|██████▏   | 883/1426 [4:40:23<2:57:06, 19.57s/it] 62%|██████▏   | 884/1426 [4:40:41<2:53:58, 19.26s/it]                                                      {'loss': 0.7141, 'learning_rate': 6.668891489264169e-06, 'epoch': 0.62}
 62%|██████▏   | 884/1426 [4:40:41<2:53:58, 19.26s/it] 62%|██████▏   | 885/1426 [4:40:59<2:49:20, 18.78s/it]                                                      {'loss': 0.6815, 'learning_rate': 6.647481671238369e-06, 'epoch': 0.62}
 62%|██████▏   | 885/1426 [4:40:59<2:49:20, 18.78s/it] 62%|██████▏   | 886/1426 [4:41:18<2:49:09, 18.80s/it]                                                      {'loss': 0.6899, 'learning_rate': 6.626089152426097e-06, 'epoch': 0.62}
 62%|██████▏   | 886/1426 [4:41:18<2:49:09, 18.80s/it] 62%|██████▏   | 887/1426 [4:41:35<2:46:14, 18.50s/it]                                                      {'loss': 0.6696, 'learning_rate': 6.60471404321415e-06, 'epoch': 0.62}
 62%|██████▏   | 887/1426 [4:41:35<2:46:14, 18.50s/it] 62%|██████▏   | 888/1426 [4:41:52<2:41:24, 18.00s/it]                                                      {'loss': 0.6817, 'learning_rate': 6.583356453899482e-06, 'epoch': 0.62}
 62%|██████▏   | 888/1426 [4:41:52<2:41:24, 18.00s/it] 62%|██████▏   | 889/1426 [4:42:11<2:42:35, 18.17s/it]                                                      {'loss': 0.6656, 'learning_rate': 6.562016494688652e-06, 'epoch': 0.62}
 62%|██████▏   | 889/1426 [4:42:11<2:42:35, 18.17s/it] 62%|██████▏   | 890/1426 [4:42:28<2:40:57, 18.02s/it]                                                      {'loss': 0.6753, 'learning_rate': 6.5406942756972415e-06, 'epoch': 0.62}
 62%|██████▏   | 890/1426 [4:42:29<2:40:57, 18.02s/it] 62%|██████▏   | 891/1426 [4:42:46<2:39:34, 17.90s/it]                                                      {'loss': 0.6946, 'learning_rate': 6.5193899069493005e-06, 'epoch': 0.62}
 62%|██████▏   | 891/1426 [4:42:46<2:39:34, 17.90s/it] 63%|██████▎   | 892/1426 [4:43:08<2:49:30, 19.05s/it]                                                      {'loss': 0.7126, 'learning_rate': 6.498103498376751e-06, 'epoch': 0.63}
 63%|██████▎   | 892/1426 [4:43:08<2:49:30, 19.05s/it] 63%|██████▎   | 893/1426 [4:43:26<2:46:34, 18.75s/it]                                                      {'loss': 0.6763, 'learning_rate': 6.4768351598188595e-06, 'epoch': 0.63}
 63%|██████▎   | 893/1426 [4:43:26<2:46:34, 18.75s/it] 63%|██████▎   | 894/1426 [4:43:46<2:51:07, 19.30s/it]                                                      {'loss': 0.685, 'learning_rate': 6.455585001021641e-06, 'epoch': 0.63}
 63%|██████▎   | 894/1426 [4:43:46<2:51:07, 19.30s/it] 63%|██████▎   | 895/1426 [4:44:01<2:36:53, 17.73s/it]                                                      {'loss': 0.234, 'learning_rate': 6.434353131637301e-06, 'epoch': 0.63}
 63%|██████▎   | 895/1426 [4:44:01<2:36:53, 17.73s/it] 63%|██████▎   | 896/1426 [4:44:19<2:39:30, 18.06s/it]                                                      {'loss': 0.6704, 'learning_rate': 6.413139661223675e-06, 'epoch': 0.63}
 63%|██████▎   | 896/1426 [4:44:19<2:39:30, 18.06s/it] 63%|██████▎   | 897/1426 [4:44:38<2:39:25, 18.08s/it]                                                      {'loss': 0.6985, 'learning_rate': 6.391944699243652e-06, 'epoch': 0.63}
 63%|██████▎   | 897/1426 [4:44:38<2:39:25, 18.08s/it] 63%|██████▎   | 898/1426 [4:44:57<2:43:41, 18.60s/it]                                                      {'loss': 0.6551, 'learning_rate': 6.3707683550646216e-06, 'epoch': 0.63}
 63%|██████▎   | 898/1426 [4:44:57<2:43:41, 18.60s/it] 63%|██████▎   | 899/1426 [4:45:17<2:46:58, 19.01s/it]                                                      {'loss': 0.6828, 'learning_rate': 6.349610737957901e-06, 'epoch': 0.63}
 63%|██████▎   | 899/1426 [4:45:17<2:46:58, 19.01s/it] 63%|██████▎   | 900/1426 [4:45:36<2:46:23, 18.98s/it]                                                      {'loss': 0.6683, 'learning_rate': 6.328471957098184e-06, 'epoch': 0.63}
 63%|██████▎   | 900/1426 [4:45:36<2:46:23, 18.98s/it] 63%|██████▎   | 901/1426 [4:45:57<2:51:12, 19.57s/it]                                                      {'loss': 0.6613, 'learning_rate': 6.307352121562949e-06, 'epoch': 0.63}
 63%|██████▎   | 901/1426 [4:45:57<2:51:12, 19.57s/it] 63%|██████▎   | 902/1426 [4:46:15<2:46:10, 19.03s/it]                                                      {'loss': 0.6874, 'learning_rate': 6.286251340331935e-06, 'epoch': 0.63}
 63%|██████▎   | 902/1426 [4:46:15<2:46:10, 19.03s/it] 63%|██████▎   | 903/1426 [4:46:35<2:48:33, 19.34s/it]                                                      {'loss': 0.6925, 'learning_rate': 6.2651697222865485e-06, 'epoch': 0.63}
 63%|██████▎   | 903/1426 [4:46:35<2:48:33, 19.34s/it] 63%|██████▎   | 904/1426 [4:46:53<2:44:13, 18.88s/it]                                                      {'loss': 0.6702, 'learning_rate': 6.244107376209315e-06, 'epoch': 0.63}
 63%|██████▎   | 904/1426 [4:46:53<2:44:13, 18.88s/it] 63%|██████▎   | 905/1426 [4:47:11<2:42:59, 18.77s/it]                                                      {'loss': 0.6705, 'learning_rate': 6.223064410783319e-06, 'epoch': 0.63}
 63%|██████▎   | 905/1426 [4:47:11<2:42:59, 18.77s/it] 64%|██████▎   | 906/1426 [4:47:31<2:44:09, 18.94s/it]                                                      {'loss': 0.6621, 'learning_rate': 6.202040934591631e-06, 'epoch': 0.64}
 64%|██████▎   | 906/1426 [4:47:31<2:44:09, 18.94s/it] 64%|██████▎   | 907/1426 [4:47:52<2:51:22, 19.81s/it]                                                      {'loss': 0.6772, 'learning_rate': 6.181037056116765e-06, 'epoch': 0.64}
 64%|██████▎   | 907/1426 [4:47:52<2:51:22, 19.81s/it] 64%|██████▎   | 908/1426 [4:48:13<2:51:58, 19.92s/it]                                                      {'loss': 0.6811, 'learning_rate': 6.160052883740102e-06, 'epoch': 0.64}
 64%|██████▎   | 908/1426 [4:48:13<2:51:58, 19.92s/it] 64%|██████▎   | 909/1426 [4:48:34<2:54:19, 20.23s/it]                                                      {'loss': 0.6518, 'learning_rate': 6.139088525741346e-06, 'epoch': 0.64}
 64%|██████▎   | 909/1426 [4:48:34<2:54:19, 20.23s/it] 64%|██████▍   | 910/1426 [4:48:54<2:55:03, 20.35s/it]                                                      {'loss': 0.7124, 'learning_rate': 6.118144090297955e-06, 'epoch': 0.64}
 64%|██████▍   | 910/1426 [4:48:54<2:55:03, 20.35s/it] 64%|██████▍   | 911/1426 [4:49:18<3:03:57, 21.43s/it]                                                      {'loss': 0.6586, 'learning_rate': 6.097219685484579e-06, 'epoch': 0.64}
 64%|██████▍   | 911/1426 [4:49:18<3:03:57, 21.43s/it] 64%|██████▍   | 912/1426 [4:49:40<3:03:35, 21.43s/it]                                                      {'loss': 0.7163, 'learning_rate': 6.076315419272514e-06, 'epoch': 0.64}
 64%|██████▍   | 912/1426 [4:49:40<3:03:35, 21.43s/it] 64%|██████▍   | 913/1426 [4:49:59<2:58:20, 20.86s/it]                                                      {'loss': 0.6742, 'learning_rate': 6.055431399529141e-06, 'epoch': 0.64}
 64%|██████▍   | 913/1426 [4:49:59<2:58:20, 20.86s/it] 64%|██████▍   | 914/1426 [4:50:18<2:53:52, 20.38s/it]                                                      {'loss': 0.66, 'learning_rate': 6.0345677340173666e-06, 'epoch': 0.64}
 64%|██████▍   | 914/1426 [4:50:18<2:53:52, 20.38s/it] 64%|██████▍   | 915/1426 [4:50:38<2:50:27, 20.02s/it]                                                      {'loss': 0.6646, 'learning_rate': 6.013724530395064e-06, 'epoch': 0.64}
 64%|██████▍   | 915/1426 [4:50:38<2:50:27, 20.02s/it] 64%|██████▍   | 916/1426 [4:50:57<2:49:00, 19.88s/it]                                                      {'loss': 0.6782, 'learning_rate': 5.9929018962145254e-06, 'epoch': 0.64}
 64%|██████▍   | 916/1426 [4:50:57<2:49:00, 19.88s/it] 64%|██████▍   | 917/1426 [4:51:14<2:40:41, 18.94s/it]                                                      {'loss': 0.6626, 'learning_rate': 5.972099938921907e-06, 'epoch': 0.64}
 64%|██████▍   | 917/1426 [4:51:14<2:40:41, 18.94s/it] 64%|██████▍   | 918/1426 [4:51:34<2:44:27, 19.42s/it]                                                      {'loss': 0.6841, 'learning_rate': 5.951318765856664e-06, 'epoch': 0.64}
 64%|██████▍   | 918/1426 [4:51:34<2:44:27, 19.42s/it] 64%|██████▍   | 919/1426 [4:51:53<2:42:01, 19.17s/it]                                                      {'loss': 0.6968, 'learning_rate': 5.930558484251007e-06, 'epoch': 0.64}
 64%|██████▍   | 919/1426 [4:51:53<2:42:01, 19.17s/it] 65%|██████▍   | 920/1426 [4:52:11<2:38:28, 18.79s/it]                                                      {'loss': 0.6776, 'learning_rate': 5.909819201229346e-06, 'epoch': 0.64}
 65%|██████▍   | 920/1426 [4:52:11<2:38:28, 18.79s/it] 65%|██████▍   | 921/1426 [4:52:31<2:41:41, 19.21s/it]                                                      {'loss': 0.692, 'learning_rate': 5.889101023807735e-06, 'epoch': 0.65}
 65%|██████▍   | 921/1426 [4:52:31<2:41:41, 19.21s/it] 65%|██████▍   | 922/1426 [4:52:51<2:42:37, 19.36s/it]                                                      {'loss': 0.6899, 'learning_rate': 5.868404058893322e-06, 'epoch': 0.65}
 65%|██████▍   | 922/1426 [4:52:51<2:42:37, 19.36s/it] 65%|██████▍   | 923/1426 [4:53:10<2:41:02, 19.21s/it]                                                      {'loss': 0.7111, 'learning_rate': 5.8477284132838e-06, 'epoch': 0.65}
 65%|██████▍   | 923/1426 [4:53:10<2:41:02, 19.21s/it] 65%|██████▍   | 924/1426 [4:53:29<2:40:59, 19.24s/it]                                                      {'loss': 0.6528, 'learning_rate': 5.8270741936668465e-06, 'epoch': 0.65}
 65%|██████▍   | 924/1426 [4:53:29<2:40:59, 19.24s/it] 65%|██████▍   | 925/1426 [4:53:48<2:41:06, 19.30s/it]                                                      {'loss': 0.6806, 'learning_rate': 5.806441506619588e-06, 'epoch': 0.65}
 65%|██████▍   | 925/1426 [4:53:48<2:41:06, 19.30s/it] 65%|██████▍   | 926/1426 [4:54:08<2:42:45, 19.53s/it]                                                      {'loss': 0.6755, 'learning_rate': 5.785830458608032e-06, 'epoch': 0.65}
 65%|██████▍   | 926/1426 [4:54:09<2:42:45, 19.53s/it] 65%|██████▌   | 927/1426 [4:54:28<2:42:58, 19.60s/it]                                                      {'loss': 0.6591, 'learning_rate': 5.7652411559865365e-06, 'epoch': 0.65}
 65%|██████▌   | 927/1426 [4:54:28<2:42:58, 19.60s/it] 65%|██████▌   | 928/1426 [4:54:49<2:44:21, 19.80s/it]                                                      {'loss': 0.6969, 'learning_rate': 5.744673704997253e-06, 'epoch': 0.65}
 65%|██████▌   | 928/1426 [4:54:49<2:44:21, 19.80s/it] 65%|██████▌   | 929/1426 [4:55:07<2:40:32, 19.38s/it]                                                      {'loss': 0.6807, 'learning_rate': 5.72412821176956e-06, 'epoch': 0.65}
 65%|██████▌   | 929/1426 [4:55:07<2:40:32, 19.38s/it] 65%|██████▌   | 930/1426 [4:55:25<2:36:38, 18.95s/it]                                                      {'loss': 0.6921, 'learning_rate': 5.703604782319549e-06, 'epoch': 0.65}
 65%|██████▌   | 930/1426 [4:55:25<2:36:38, 18.95s/it] 65%|██████▌   | 931/1426 [4:55:45<2:38:20, 19.19s/it]                                                      {'loss': 0.6615, 'learning_rate': 5.68310352254946e-06, 'epoch': 0.65}
 65%|██████▌   | 931/1426 [4:55:45<2:38:20, 19.19s/it] 65%|██████▌   | 932/1426 [4:56:02<2:34:28, 18.76s/it]                                                      {'loss': 0.6817, 'learning_rate': 5.662624538247128e-06, 'epoch': 0.65}
 65%|██████▌   | 932/1426 [4:56:02<2:34:28, 18.76s/it] 65%|██████▌   | 933/1426 [4:56:24<2:40:17, 19.51s/it]                                                      {'loss': 0.6741, 'learning_rate': 5.642167935085454e-06, 'epoch': 0.65}
 65%|██████▌   | 933/1426 [4:56:24<2:40:17, 19.51s/it] 65%|██████▌   | 934/1426 [4:56:43<2:40:07, 19.53s/it]                                                      {'loss': 0.6837, 'learning_rate': 5.621733818621844e-06, 'epoch': 0.65}
 65%|██████▌   | 934/1426 [4:56:43<2:40:07, 19.53s/it] 66%|██████▌   | 935/1426 [4:57:03<2:40:33, 19.62s/it]                                                      {'loss': 0.677, 'learning_rate': 5.60132229429767e-06, 'epoch': 0.66}
 66%|██████▌   | 935/1426 [4:57:03<2:40:33, 19.62s/it] 66%|██████▌   | 936/1426 [4:57:22<2:37:42, 19.31s/it]                                                      {'loss': 0.6887, 'learning_rate': 5.580933467437733e-06, 'epoch': 0.66}
 66%|██████▌   | 936/1426 [4:57:22<2:37:42, 19.31s/it] 66%|██████▌   | 937/1426 [4:57:42<2:39:48, 19.61s/it]                                                      {'loss': 0.6775, 'learning_rate': 5.560567443249718e-06, 'epoch': 0.66}
 66%|██████▌   | 937/1426 [4:57:42<2:39:48, 19.61s/it] 66%|██████▌   | 938/1426 [4:58:01<2:39:12, 19.58s/it]                                                      {'loss': 0.6859, 'learning_rate': 5.540224326823629e-06, 'epoch': 0.66}
 66%|██████▌   | 938/1426 [4:58:01<2:39:12, 19.58s/it] 66%|██████▌   | 939/1426 [4:58:20<2:36:04, 19.23s/it]                                                      {'loss': 0.6752, 'learning_rate': 5.519904223131278e-06, 'epoch': 0.66}
 66%|██████▌   | 939/1426 [4:58:20<2:36:04, 19.23s/it] 66%|██████▌   | 940/1426 [4:58:41<2:41:23, 19.92s/it]                                                      {'loss': 0.661, 'learning_rate': 5.4996072370257326e-06, 'epoch': 0.66}
 66%|██████▌   | 940/1426 [4:58:41<2:41:23, 19.92s/it] 66%|██████▌   | 941/1426 [4:58:56<2:27:22, 18.23s/it]                                                      {'loss': 0.2521, 'learning_rate': 5.479333473240761e-06, 'epoch': 0.66}
 66%|██████▌   | 941/1426 [4:58:56<2:27:22, 18.23s/it] 66%|██████▌   | 942/1426 [4:59:14<2:26:38, 18.18s/it]                                                      {'loss': 0.6932, 'learning_rate': 5.4590830363903135e-06, 'epoch': 0.66}
 66%|██████▌   | 942/1426 [4:59:14<2:26:38, 18.18s/it] 66%|██████▌   | 943/1426 [4:59:34<2:30:22, 18.68s/it]                                                      {'loss': 0.6711, 'learning_rate': 5.438856030967965e-06, 'epoch': 0.66}
 66%|██████▌   | 943/1426 [4:59:34<2:30:22, 18.68s/it] 66%|██████▌   | 944/1426 [4:59:51<2:27:06, 18.31s/it]                                                      {'loss': 0.6922, 'learning_rate': 5.41865256134638e-06, 'epoch': 0.66}
 66%|██████▌   | 944/1426 [4:59:51<2:27:06, 18.31s/it]this iter is wrong in something... skip...
 66%|██████▋   | 945/1426 [5:00:11<2:31:38, 18.92s/it]                                                      {'loss': 0.662, 'learning_rate': 5.398472731776784e-06, 'epoch': 0.66}
 66%|██████▋   | 945/1426 [5:00:11<2:31:38, 18.92s/it] 66%|██████▋   | 946/1426 [5:00:31<2:33:03, 19.13s/it]                                                      {'loss': 0.6881, 'learning_rate': 5.378316646388424e-06, 'epoch': 0.66}
 66%|██████▋   | 946/1426 [5:00:31<2:33:03, 19.13s/it] 66%|██████▋   | 947/1426 [5:00:51<2:33:41, 19.25s/it]                                                      {'loss': 0.6611, 'learning_rate': 5.358184409188002e-06, 'epoch': 0.66}
 66%|██████▋   | 947/1426 [5:00:51<2:33:41, 19.25s/it] 66%|██████▋   | 948/1426 [5:01:08<2:29:11, 18.73s/it]                                                      {'loss': 0.6679, 'learning_rate': 5.338076124059184e-06, 'epoch': 0.66}
 66%|██████▋   | 948/1426 [5:01:08<2:29:11, 18.73s/it] 67%|██████▋   | 949/1426 [5:01:31<2:37:52, 19.86s/it]                                                      {'loss': 0.6777, 'learning_rate': 5.317991894762038e-06, 'epoch': 0.67}
 67%|██████▋   | 949/1426 [5:01:31<2:37:52, 19.86s/it] 67%|██████▋   | 950/1426 [5:01:52<2:41:03, 20.30s/it]                                                      {'loss': 0.6858, 'learning_rate': 5.297931824932492e-06, 'epoch': 0.67}
 67%|██████▋   | 950/1426 [5:01:52<2:41:03, 20.30s/it] 67%|██████▋   | 951/1426 [5:02:11<2:37:46, 19.93s/it]                                                      {'loss': 0.6772, 'learning_rate': 5.277896018081823e-06, 'epoch': 0.67}
 67%|██████▋   | 951/1426 [5:02:11<2:37:46, 19.93s/it] 67%|██████▋   | 952/1426 [5:02:31<2:37:51, 19.98s/it]                                                      {'loss': 0.6906, 'learning_rate': 5.2578845775961015e-06, 'epoch': 0.67}
 67%|██████▋   | 952/1426 [5:02:31<2:37:51, 19.98s/it] 67%|██████▋   | 953/1426 [5:02:51<2:37:26, 19.97s/it]                                                      {'loss': 0.6437, 'learning_rate': 5.23789760673566e-06, 'epoch': 0.67}
 67%|██████▋   | 953/1426 [5:02:51<2:37:26, 19.97s/it] 67%|██████▋   | 954/1426 [5:03:08<2:30:53, 19.18s/it]                                                      {'loss': 0.6785, 'learning_rate': 5.2179352086345804e-06, 'epoch': 0.67}
 67%|██████▋   | 954/1426 [5:03:08<2:30:53, 19.18s/it] 67%|██████▋   | 955/1426 [5:03:29<2:34:17, 19.65s/it]                                                      {'loss': 0.6481, 'learning_rate': 5.197997486300142e-06, 'epoch': 0.67}
 67%|██████▋   | 955/1426 [5:03:29<2:34:17, 19.65s/it] 67%|██████▋   | 956/1426 [5:03:47<2:29:21, 19.07s/it]                                                      {'loss': 0.6764, 'learning_rate': 5.178084542612293e-06, 'epoch': 0.67}
 67%|██████▋   | 956/1426 [5:03:47<2:29:21, 19.07s/it] 67%|██████▋   | 957/1426 [5:04:06<2:29:03, 19.07s/it]                                                      {'loss': 0.6962, 'learning_rate': 5.158196480323117e-06, 'epoch': 0.67}
 67%|██████▋   | 957/1426 [5:04:06<2:29:03, 19.07s/it] 67%|██████▋   | 958/1426 [5:04:25<2:28:51, 19.08s/it]                                                      {'loss': 0.6568, 'learning_rate': 5.138333402056324e-06, 'epoch': 0.67}
 67%|██████▋   | 958/1426 [5:04:25<2:28:51, 19.08s/it] 67%|██████▋   | 959/1426 [5:04:47<2:34:40, 19.87s/it]                                                      {'loss': 0.6976, 'learning_rate': 5.1184954103066855e-06, 'epoch': 0.67}
 67%|██████▋   | 959/1426 [5:04:47<2:34:40, 19.87s/it] 67%|██████▋   | 960/1426 [5:05:04<2:27:17, 18.97s/it]                                                      {'loss': 0.2243, 'learning_rate': 5.0986826074395424e-06, 'epoch': 0.67}
 67%|██████▋   | 960/1426 [5:05:04<2:27:17, 18.97s/it] 67%|██████▋   | 961/1426 [5:05:23<2:27:42, 19.06s/it]                                                      {'loss': 0.7024, 'learning_rate': 5.078895095690249e-06, 'epoch': 0.67}
 67%|██████▋   | 961/1426 [5:05:23<2:27:42, 19.06s/it]this iter is wrong in something... skip...
 67%|██████▋   | 962/1426 [5:05:40<2:23:19, 18.53s/it]                                                      {'loss': 0.6929, 'learning_rate': 5.0591329771636545e-06, 'epoch': 0.67}
 67%|██████▋   | 962/1426 [5:05:40<2:23:19, 18.53s/it] 68%|██████▊   | 963/1426 [5:05:58<2:20:52, 18.26s/it]                                                      {'loss': 0.6812, 'learning_rate': 5.039396353833583e-06, 'epoch': 0.68}
 68%|██████▊   | 963/1426 [5:05:58<2:20:52, 18.26s/it] 68%|██████▊   | 964/1426 [5:06:19<2:27:19, 19.13s/it]                                                      {'loss': 0.6742, 'learning_rate': 5.019685327542307e-06, 'epoch': 0.68}
 68%|██████▊   | 964/1426 [5:06:19<2:27:19, 19.13s/it] 68%|██████▊   | 965/1426 [5:06:40<2:31:18, 19.69s/it]                                                      {'loss': 0.6762, 'learning_rate': 5.000000000000003e-06, 'epoch': 0.68}
 68%|██████▊   | 965/1426 [5:06:40<2:31:18, 19.69s/it] 68%|██████▊   | 966/1426 [5:06:54<2:17:41, 17.96s/it]                                                      {'loss': 0.2395, 'learning_rate': 4.980340472784242e-06, 'epoch': 0.68}
 68%|██████▊   | 966/1426 [5:06:54<2:17:41, 17.96s/it] 68%|██████▊   | 967/1426 [5:07:16<2:26:13, 19.12s/it]                                                      {'loss': 0.6799, 'learning_rate': 4.9607068473394795e-06, 'epoch': 0.68}
 68%|██████▊   | 967/1426 [5:07:16<2:26:13, 19.12s/it] 68%|██████▊   | 968/1426 [5:07:39<2:34:38, 20.26s/it]                                                      {'loss': 0.7057, 'learning_rate': 4.941099224976494e-06, 'epoch': 0.68}
 68%|██████▊   | 968/1426 [5:07:39<2:34:38, 20.26s/it] 68%|██████▊   | 969/1426 [5:07:57<2:30:58, 19.82s/it]                                                      {'loss': 0.6621, 'learning_rate': 4.921517706871908e-06, 'epoch': 0.68}
 68%|██████▊   | 969/1426 [5:07:57<2:30:58, 19.82s/it] 68%|██████▊   | 970/1426 [5:08:15<2:26:38, 19.30s/it]                                                      {'loss': 0.666, 'learning_rate': 4.9019623940676275e-06, 'epoch': 0.68}
 68%|██████▊   | 970/1426 [5:08:15<2:26:38, 19.30s/it] 68%|██████▊   | 971/1426 [5:08:35<2:27:21, 19.43s/it]                                                      {'loss': 0.7014, 'learning_rate': 4.8824333874703414e-06, 'epoch': 0.68}
 68%|██████▊   | 971/1426 [5:08:35<2:27:21, 19.43s/it] 68%|██████▊   | 972/1426 [5:08:57<2:31:52, 20.07s/it]                                                      {'loss': 0.6524, 'learning_rate': 4.862930787851003e-06, 'epoch': 0.68}
 68%|██████▊   | 972/1426 [5:08:57<2:31:52, 20.07s/it] 68%|██████▊   | 973/1426 [5:09:15<2:26:48, 19.45s/it]                                                      {'loss': 0.6675, 'learning_rate': 4.843454695844303e-06, 'epoch': 0.68}
 68%|██████▊   | 973/1426 [5:09:15<2:26:48, 19.45s/it] 68%|██████▊   | 974/1426 [5:09:36<2:30:28, 19.97s/it]                                                      {'loss': 0.6987, 'learning_rate': 4.824005211948144e-06, 'epoch': 0.68}
 68%|██████▊   | 974/1426 [5:09:36<2:30:28, 19.97s/it] 68%|██████▊   | 975/1426 [5:09:55<2:27:35, 19.64s/it]                                                      {'loss': 0.6464, 'learning_rate': 4.804582436523132e-06, 'epoch': 0.68}
 68%|██████▊   | 975/1426 [5:09:55<2:27:35, 19.64s/it]this iter is wrong in something... skip...
 68%|██████▊   | 976/1426 [5:10:13<2:23:48, 19.17s/it]                                                      {'loss': 0.6874, 'learning_rate': 4.78518646979206e-06, 'epoch': 0.68}
 68%|██████▊   | 976/1426 [5:10:13<2:23:48, 19.17s/it] 69%|██████▊   | 977/1426 [5:10:32<2:23:50, 19.22s/it]                                                      {'loss': 0.6701, 'learning_rate': 4.76581741183938e-06, 'epoch': 0.68}
 69%|██████▊   | 977/1426 [5:10:32<2:23:50, 19.22s/it] 69%|██████▊   | 978/1426 [5:10:49<2:18:57, 18.61s/it]                                                      {'loss': 0.7004, 'learning_rate': 4.746475362610696e-06, 'epoch': 0.69}
 69%|██████▊   | 978/1426 [5:10:49<2:18:57, 18.61s/it] 69%|██████▊   | 979/1426 [5:11:10<2:22:15, 19.09s/it]                                                      {'loss': 0.6788, 'learning_rate': 4.727160421912256e-06, 'epoch': 0.69}
 69%|██████▊   | 979/1426 [5:11:10<2:22:15, 19.09s/it] 69%|██████▊   | 980/1426 [5:11:29<2:22:57, 19.23s/it]                                                      {'loss': 0.667, 'learning_rate': 4.707872689410399e-06, 'epoch': 0.69}
 69%|██████▊   | 980/1426 [5:11:29<2:22:57, 19.23s/it] 69%|██████▉   | 981/1426 [5:11:49<2:22:57, 19.27s/it]                                                      {'loss': 0.6682, 'learning_rate': 4.68861226463109e-06, 'epoch': 0.69}
 69%|██████▉   | 981/1426 [5:11:49<2:22:57, 19.27s/it] 69%|██████▉   | 982/1426 [5:12:08<2:23:05, 19.34s/it]                                                      {'loss': 0.6527, 'learning_rate': 4.669379246959386e-06, 'epoch': 0.69}
 69%|██████▉   | 982/1426 [5:12:08<2:23:05, 19.34s/it] 69%|██████▉   | 983/1426 [5:12:27<2:22:30, 19.30s/it]                                                      {'loss': 0.6753, 'learning_rate': 4.650173735638908e-06, 'epoch': 0.69}
 69%|██████▉   | 983/1426 [5:12:27<2:22:30, 19.30s/it] 69%|██████▉   | 984/1426 [5:12:46<2:21:42, 19.24s/it]                                                      {'loss': 0.6727, 'learning_rate': 4.630995829771346e-06, 'epoch': 0.69}
 69%|██████▉   | 984/1426 [5:12:46<2:21:42, 19.24s/it] 69%|██████▉   | 985/1426 [5:13:05<2:20:12, 19.08s/it]                                                      {'loss': 0.6843, 'learning_rate': 4.611845628315954e-06, 'epoch': 0.69}
 69%|██████▉   | 985/1426 [5:13:05<2:20:12, 19.08s/it] 69%|██████▉   | 986/1426 [5:13:25<2:21:36, 19.31s/it]                                                      {'loss': 0.6834, 'learning_rate': 4.5927232300890146e-06, 'epoch': 0.69}
 69%|██████▉   | 986/1426 [5:13:25<2:21:36, 19.31s/it] 69%|██████▉   | 987/1426 [5:13:46<2:25:06, 19.83s/it]                                                      {'loss': 0.6549, 'learning_rate': 4.573628733763356e-06, 'epoch': 0.69}
 69%|██████▉   | 987/1426 [5:13:46<2:25:06, 19.83s/it] 69%|██████▉   | 988/1426 [5:14:06<2:24:32, 19.80s/it]                                                      {'loss': 0.688, 'learning_rate': 4.554562237867832e-06, 'epoch': 0.69}
 69%|██████▉   | 988/1426 [5:14:06<2:24:32, 19.80s/it] 69%|██████▉   | 989/1426 [5:14:24<2:20:17, 19.26s/it]                                                      {'loss': 0.6475, 'learning_rate': 4.535523840786794e-06, 'epoch': 0.69}
 69%|██████▉   | 989/1426 [5:14:24<2:20:17, 19.26s/it] 69%|██████▉   | 990/1426 [5:14:47<2:29:40, 20.60s/it]                                                      {'loss': 0.6982, 'learning_rate': 4.51651364075962e-06, 'epoch': 0.69}
 69%|██████▉   | 990/1426 [5:14:47<2:29:40, 20.60s/it] 69%|██████▉   | 991/1426 [5:15:07<2:27:52, 20.40s/it]                                                      {'loss': 0.6564, 'learning_rate': 4.4975317358801885e-06, 'epoch': 0.69}
 69%|██████▉   | 991/1426 [5:15:07<2:27:52, 20.40s/it] 70%|██████▉   | 992/1426 [5:15:25<2:22:04, 19.64s/it]                                                      {'loss': 0.6538, 'learning_rate': 4.478578224096365e-06, 'epoch': 0.7}
 70%|██████▉   | 992/1426 [5:15:25<2:22:04, 19.64s/it] 70%|██████▉   | 993/1426 [5:15:43<2:17:25, 19.04s/it]                                                      {'loss': 0.657, 'learning_rate': 4.459653203209503e-06, 'epoch': 0.7}
 70%|██████▉   | 993/1426 [5:15:43<2:17:25, 19.04s/it] 70%|██████▉   | 994/1426 [5:16:00<2:13:29, 18.54s/it]                                                      {'loss': 0.6885, 'learning_rate': 4.440756770873954e-06, 'epoch': 0.7}
 70%|██████▉   | 994/1426 [5:16:00<2:13:29, 18.54s/it] 70%|██████▉   | 995/1426 [5:16:14<2:03:34, 17.20s/it]                                                      {'loss': 0.2304, 'learning_rate': 4.421889024596537e-06, 'epoch': 0.7}
 70%|██████▉   | 995/1426 [5:16:14<2:03:34, 17.20s/it] 70%|██████▉   | 996/1426 [5:16:36<2:12:51, 18.54s/it]                                                      {'loss': 0.6577, 'learning_rate': 4.4030500617360575e-06, 'epoch': 0.7}
 70%|██████▉   | 996/1426 [5:16:36<2:12:51, 18.54s/it] 70%|██████▉   | 997/1426 [5:16:54<2:11:57, 18.45s/it]                                                      {'loss': 0.6795, 'learning_rate': 4.384239979502801e-06, 'epoch': 0.7}
 70%|██████▉   | 997/1426 [5:16:54<2:11:57, 18.45s/it] 70%|██████▉   | 998/1426 [5:17:13<2:12:25, 18.57s/it]                                                      {'loss': 0.6736, 'learning_rate': 4.3654588749580075e-06, 'epoch': 0.7}
 70%|██████▉   | 998/1426 [5:17:13<2:12:25, 18.57s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (3103 > 3072). Running this sequence through the model will result in indexing errors
 70%|███████   | 999/1426 [5:17:31<2:09:57, 18.26s/it]                                                      {'loss': 0.7101, 'learning_rate': 4.346706845013409e-06, 'epoch': 0.7}
 70%|███████   | 999/1426 [5:17:31<2:09:57, 18.26s/it] 70%|███████   | 1000/1426 [5:17:49<2:09:33, 18.25s/it]                                                       {'loss': 0.6821, 'learning_rate': 4.327983986430709e-06, 'epoch': 0.7}
 70%|███████   | 1000/1426 [5:17:49<2:09:33, 18.25s/it] 70%|███████   | 1001/1426 [5:18:08<2:11:31, 18.57s/it]                                                       {'loss': 0.676, 'learning_rate': 4.309290395821075e-06, 'epoch': 0.7}
 70%|███████   | 1001/1426 [5:18:08<2:11:31, 18.57s/it] 70%|███████   | 1002/1426 [5:18:26<2:10:36, 18.48s/it]                                                       {'loss': 0.6589, 'learning_rate': 4.2906261696446535e-06, 'epoch': 0.7}
 70%|███████   | 1002/1426 [5:18:26<2:10:36, 18.48s/it]this iter is wrong in something... skip...
 70%|███████   | 1003/1426 [5:18:45<2:11:21, 18.63s/it]                                                       {'loss': 0.6572, 'learning_rate': 4.271991404210078e-06, 'epoch': 0.7}
 70%|███████   | 1003/1426 [5:18:45<2:11:21, 18.63s/it] 70%|███████   | 1004/1426 [5:19:08<2:20:10, 19.93s/it]                                                       {'loss': 0.6457, 'learning_rate': 4.253386195673948e-06, 'epoch': 0.7}
 70%|███████   | 1004/1426 [5:19:08<2:20:10, 19.93s/it] 70%|███████   | 1005/1426 [5:19:28<2:20:18, 20.00s/it]                                                       {'loss': 0.663, 'learning_rate': 4.234810640040359e-06, 'epoch': 0.7}
 70%|███████   | 1005/1426 [5:19:28<2:20:18, 20.00s/it] 71%|███████   | 1006/1426 [5:19:50<2:22:12, 20.31s/it]                                                       {'loss': 0.6604, 'learning_rate': 4.216264833160396e-06, 'epoch': 0.71}
 71%|███████   | 1006/1426 [5:19:50<2:22:12, 20.31s/it] 71%|███████   | 1007/1426 [5:20:10<2:21:19, 20.24s/it]                                                       {'loss': 0.6796, 'learning_rate': 4.1977488707316206e-06, 'epoch': 0.71}
 71%|███████   | 1007/1426 [5:20:10<2:21:19, 20.24s/it] 71%|███████   | 1008/1426 [5:20:30<2:20:30, 20.17s/it]                                                       {'loss': 0.6743, 'learning_rate': 4.179262848297615e-06, 'epoch': 0.71}
 71%|███████   | 1008/1426 [5:20:30<2:20:30, 20.17s/it] 71%|███████   | 1009/1426 [5:20:50<2:21:40, 20.38s/it]                                                       {'loss': 0.6769, 'learning_rate': 4.160806861247467e-06, 'epoch': 0.71}
 71%|███████   | 1009/1426 [5:20:51<2:21:40, 20.38s/it] 71%|███████   | 1010/1426 [5:21:08<2:15:22, 19.53s/it]                                                       {'loss': 0.689, 'learning_rate': 4.142381004815267e-06, 'epoch': 0.71}
 71%|███████   | 1010/1426 [5:21:08<2:15:22, 19.53s/it] 71%|███████   | 1011/1426 [5:21:26<2:12:53, 19.21s/it]                                                       {'loss': 0.6799, 'learning_rate': 4.123985374079644e-06, 'epoch': 0.71}
 71%|███████   | 1011/1426 [5:21:27<2:12:53, 19.21s/it] 71%|███████   | 1012/1426 [5:21:44<2:09:39, 18.79s/it]                                                       {'loss': 0.6882, 'learning_rate': 4.105620063963252e-06, 'epoch': 0.71}
 71%|███████   | 1012/1426 [5:21:44<2:09:39, 18.79s/it] 71%|███████   | 1013/1426 [5:22:06<2:15:44, 19.72s/it]                                                       {'loss': 0.6637, 'learning_rate': 4.087285169232288e-06, 'epoch': 0.71}
 71%|███████   | 1013/1426 [5:22:06<2:15:44, 19.72s/it] 71%|███████   | 1014/1426 [5:22:24<2:11:48, 19.20s/it]                                                       {'loss': 0.6742, 'learning_rate': 4.0689807844960065e-06, 'epoch': 0.71}
 71%|███████   | 1014/1426 [5:22:24<2:11:48, 19.20s/it] 71%|███████   | 1015/1426 [5:22:47<2:19:16, 20.33s/it]                                                       {'loss': 0.6473, 'learning_rate': 4.050707004206234e-06, 'epoch': 0.71}
 71%|███████   | 1015/1426 [5:22:47<2:19:16, 20.33s/it] 71%|███████   | 1016/1426 [5:23:07<2:17:03, 20.06s/it]                                                       {'loss': 0.6812, 'learning_rate': 4.0324639226568666e-06, 'epoch': 0.71}
 71%|███████   | 1016/1426 [5:23:07<2:17:03, 20.06s/it] 71%|███████▏  | 1017/1426 [5:23:28<2:18:45, 20.36s/it]                                                       {'loss': 0.6719, 'learning_rate': 4.014251633983392e-06, 'epoch': 0.71}
 71%|███████▏  | 1017/1426 [5:23:28<2:18:45, 20.36s/it] 71%|███████▏  | 1018/1426 [5:23:50<2:23:05, 21.04s/it]                                                       {'loss': 0.6802, 'learning_rate': 3.996070232162417e-06, 'epoch': 0.71}
 71%|███████▏  | 1018/1426 [5:23:50<2:23:05, 21.04s/it] 71%|███████▏  | 1019/1426 [5:24:08<2:15:38, 20.00s/it]                                                       {'loss': 0.6634, 'learning_rate': 3.977919811011155e-06, 'epoch': 0.71}
 71%|███████▏  | 1019/1426 [5:24:08<2:15:38, 20.00s/it] 72%|███████▏  | 1020/1426 [5:24:28<2:15:53, 20.08s/it]                                                       {'loss': 0.6495, 'learning_rate': 3.959800464186972e-06, 'epoch': 0.72}
 72%|███████▏  | 1020/1426 [5:24:28<2:15:53, 20.08s/it] 72%|███████▏  | 1021/1426 [5:24:46<2:11:57, 19.55s/it]                                                       {'loss': 0.6962, 'learning_rate': 3.941712285186879e-06, 'epoch': 0.72}
 72%|███████▏  | 1021/1426 [5:24:46<2:11:57, 19.55s/it] 72%|███████▏  | 1022/1426 [5:25:06<2:11:21, 19.51s/it]                                                       {'loss': 0.6659, 'learning_rate': 3.923655367347054e-06, 'epoch': 0.72}
 72%|███████▏  | 1022/1426 [5:25:06<2:11:21, 19.51s/it] 72%|███████▏  | 1023/1426 [5:25:27<2:14:16, 19.99s/it]                                                       {'loss': 0.6604, 'learning_rate': 3.905629803842377e-06, 'epoch': 0.72}
 72%|███████▏  | 1023/1426 [5:25:27<2:14:16, 19.99s/it] 72%|███████▏  | 1024/1426 [5:25:48<2:16:58, 20.45s/it]                                                       {'loss': 0.6768, 'learning_rate': 3.887635687685938e-06, 'epoch': 0.72}
 72%|███████▏  | 1024/1426 [5:25:48<2:16:58, 20.45s/it] 72%|███████▏  | 1025/1426 [5:26:06<2:11:40, 19.70s/it]                                                       {'loss': 0.678, 'learning_rate': 3.8696731117285465e-06, 'epoch': 0.72}
 72%|███████▏  | 1025/1426 [5:26:06<2:11:40, 19.70s/it] 72%|███████▏  | 1026/1426 [5:26:27<2:12:24, 19.86s/it]                                                       {'loss': 0.6769, 'learning_rate': 3.8517421686582644e-06, 'epoch': 0.72}
 72%|███████▏  | 1026/1426 [5:26:27<2:12:24, 19.86s/it] 72%|███████▏  | 1027/1426 [5:26:51<2:20:49, 21.18s/it]                                                       {'loss': 0.6517, 'learning_rate': 3.833842950999937e-06, 'epoch': 0.72}
 72%|███████▏  | 1027/1426 [5:26:51<2:20:49, 21.18s/it] 72%|███████▏  | 1028/1426 [5:27:12<2:20:36, 21.20s/it]                                                       {'loss': 0.6643, 'learning_rate': 3.815975551114689e-06, 'epoch': 0.72}
 72%|███████▏  | 1028/1426 [5:27:12<2:20:36, 21.20s/it] 72%|███████▏  | 1029/1426 [5:27:30<2:13:47, 20.22s/it]                                                       {'loss': 0.6632, 'learning_rate': 3.7981400611994767e-06, 'epoch': 0.72}
 72%|███████▏  | 1029/1426 [5:27:30<2:13:47, 20.22s/it] 72%|███████▏  | 1030/1426 [5:27:51<2:15:14, 20.49s/it]                                                       {'loss': 0.6699, 'learning_rate': 3.7803365732865915e-06, 'epoch': 0.72}
 72%|███████▏  | 1030/1426 [5:27:51<2:15:14, 20.49s/it] 72%|███████▏  | 1031/1426 [5:28:11<2:13:45, 20.32s/it]                                                       {'loss': 0.6659, 'learning_rate': 3.762565179243187e-06, 'epoch': 0.72}
 72%|███████▏  | 1031/1426 [5:28:11<2:13:45, 20.32s/it] 72%|███████▏  | 1032/1426 [5:28:31<2:12:59, 20.25s/it]                                                       {'loss': 0.664, 'learning_rate': 3.7448259707708234e-06, 'epoch': 0.72}
 72%|███████▏  | 1032/1426 [5:28:31<2:12:59, 20.25s/it] 72%|███████▏  | 1033/1426 [5:28:49<2:07:24, 19.45s/it]                                                       {'loss': 0.6636, 'learning_rate': 3.727119039404977e-06, 'epoch': 0.72}
 72%|███████▏  | 1033/1426 [5:28:49<2:07:24, 19.45s/it] 73%|███████▎  | 1034/1426 [5:29:08<2:06:46, 19.40s/it]                                                       {'loss': 0.6932, 'learning_rate': 3.7094444765145674e-06, 'epoch': 0.72}
 73%|███████▎  | 1034/1426 [5:29:08<2:06:46, 19.40s/it] 73%|███████▎  | 1035/1426 [5:29:26<2:03:04, 18.89s/it]                                                       {'loss': 0.6675, 'learning_rate': 3.6918023733014908e-06, 'epoch': 0.73}
 73%|███████▎  | 1035/1426 [5:29:26<2:03:04, 18.89s/it] 73%|███████▎  | 1036/1426 [5:29:47<2:08:07, 19.71s/it]                                                       {'loss': 0.6898, 'learning_rate': 3.6741928208001566e-06, 'epoch': 0.73}
 73%|███████▎  | 1036/1426 [5:29:47<2:08:07, 19.71s/it] 73%|███████▎  | 1037/1426 [5:30:07<2:06:43, 19.55s/it]                                                       {'loss': 0.6665, 'learning_rate': 3.6566159098770004e-06, 'epoch': 0.73}
 73%|███████▎  | 1037/1426 [5:30:07<2:06:43, 19.55s/it] 73%|███████▎  | 1038/1426 [5:30:28<2:09:40, 20.05s/it]                                                       {'loss': 0.6997, 'learning_rate': 3.639071731230036e-06, 'epoch': 0.73}
 73%|███████▎  | 1038/1426 [5:30:28<2:09:40, 20.05s/it] 73%|███████▎  | 1039/1426 [5:30:47<2:07:55, 19.83s/it]                                                       {'loss': 0.674, 'learning_rate': 3.62156037538837e-06, 'epoch': 0.73}
 73%|███████▎  | 1039/1426 [5:30:47<2:07:55, 19.83s/it] 73%|███████▎  | 1040/1426 [5:31:06<2:06:17, 19.63s/it]                                                       {'loss': 0.6582, 'learning_rate': 3.6040819327117373e-06, 'epoch': 0.73}
 73%|███████▎  | 1040/1426 [5:31:06<2:06:17, 19.63s/it] 73%|███████▎  | 1041/1426 [5:31:26<2:05:34, 19.57s/it]                                                       {'loss': 0.6653, 'learning_rate': 3.5866364933900466e-06, 'epoch': 0.73}
 73%|███████▎  | 1041/1426 [5:31:26<2:05:34, 19.57s/it] 73%|███████▎  | 1042/1426 [5:31:45<2:03:56, 19.37s/it]                                                       {'loss': 0.6621, 'learning_rate': 3.5692241474429077e-06, 'epoch': 0.73}
 73%|███████▎  | 1042/1426 [5:31:45<2:03:56, 19.37s/it] 73%|███████▎  | 1043/1426 [5:32:05<2:05:03, 19.59s/it]                                                       {'loss': 0.6631, 'learning_rate': 3.5518449847191603e-06, 'epoch': 0.73}
 73%|███████▎  | 1043/1426 [5:32:05<2:05:03, 19.59s/it] 73%|███████▎  | 1044/1426 [5:32:25<2:06:29, 19.87s/it]                                                       {'loss': 0.6712, 'learning_rate': 3.5344990948964163e-06, 'epoch': 0.73}
 73%|███████▎  | 1044/1426 [5:32:25<2:06:29, 19.87s/it] 73%|███████▎  | 1045/1426 [5:32:44<2:05:00, 19.69s/it]                                                       {'loss': 0.6747, 'learning_rate': 3.517186567480605e-06, 'epoch': 0.73}
 73%|███████▎  | 1045/1426 [5:32:44<2:05:00, 19.69s/it]this iter is wrong in something... skip...
this iter is wrong in something... skip...
 73%|███████▎  | 1046/1426 [5:33:06<2:08:02, 20.22s/it]                                                       {'loss': 0.6685, 'learning_rate': 3.4999074918054922e-06, 'epoch': 0.73}
 73%|███████▎  | 1046/1426 [5:33:06<2:08:02, 20.22s/it] 73%|███████▎  | 1047/1426 [5:33:23<2:02:10, 19.34s/it]                                                       {'loss': 0.6688, 'learning_rate': 3.482661957032244e-06, 'epoch': 0.73}
 73%|███████▎  | 1047/1426 [5:33:23<2:02:10, 19.34s/it] 73%|███████▎  | 1048/1426 [5:33:43<2:03:18, 19.57s/it]                                                       {'loss': 0.656, 'learning_rate': 3.46545005214894e-06, 'epoch': 0.73}
 73%|███████▎  | 1048/1426 [5:33:43<2:03:18, 19.57s/it] 74%|███████▎  | 1049/1426 [5:34:01<1:59:51, 19.08s/it]                                                       {'loss': 0.6643, 'learning_rate': 3.44827186597014e-06, 'epoch': 0.74}
 74%|███████▎  | 1049/1426 [5:34:01<1:59:51, 19.08s/it] 74%|███████▎  | 1050/1426 [5:34:20<1:58:33, 18.92s/it]                                                       {'loss': 0.6616, 'learning_rate': 3.4311274871363985e-06, 'epoch': 0.74}
 74%|███████▎  | 1050/1426 [5:34:20<1:58:33, 18.92s/it] 74%|███████▎  | 1051/1426 [5:34:39<1:59:08, 19.06s/it]                                                       {'loss': 0.6807, 'learning_rate': 3.414017004113839e-06, 'epoch': 0.74}
 74%|███████▎  | 1051/1426 [5:34:39<1:59:08, 19.06s/it] 74%|███████▍  | 1052/1426 [5:34:57<1:56:56, 18.76s/it]                                                       {'loss': 0.6746, 'learning_rate': 3.3969405051936655e-06, 'epoch': 0.74}
 74%|███████▍  | 1052/1426 [5:34:57<1:56:56, 18.76s/it] 74%|███████▍  | 1053/1426 [5:35:16<1:56:03, 18.67s/it]                                                       {'loss': 0.6753, 'learning_rate': 3.3798980784917255e-06, 'epoch': 0.74}
 74%|███████▍  | 1053/1426 [5:35:16<1:56:03, 18.67s/it] 74%|███████▍  | 1054/1426 [5:35:34<1:54:15, 18.43s/it]                                                       {'loss': 0.6832, 'learning_rate': 3.362889811948061e-06, 'epoch': 0.74}
 74%|███████▍  | 1054/1426 [5:35:34<1:54:15, 18.43s/it] 74%|███████▍  | 1055/1426 [5:35:53<1:55:11, 18.63s/it]                                                       {'loss': 0.6905, 'learning_rate': 3.345915793326431e-06, 'epoch': 0.74}
 74%|███████▍  | 1055/1426 [5:35:53<1:55:11, 18.63s/it] 74%|███████▍  | 1056/1426 [5:36:11<1:53:53, 18.47s/it]                                                       {'loss': 0.6708, 'learning_rate': 3.328976110213884e-06, 'epoch': 0.74}
 74%|███████▍  | 1056/1426 [5:36:11<1:53:53, 18.47s/it] 74%|███████▍  | 1057/1426 [5:36:30<1:54:49, 18.67s/it]                                                       {'loss': 0.6618, 'learning_rate': 3.3120708500202923e-06, 'epoch': 0.74}
 74%|███████▍  | 1057/1426 [5:36:30<1:54:49, 18.67s/it] 74%|███████▍  | 1058/1426 [5:36:49<1:54:29, 18.67s/it]                                                       {'loss': 0.6903, 'learning_rate': 3.295200099977903e-06, 'epoch': 0.74}
 74%|███████▍  | 1058/1426 [5:36:49<1:54:29, 18.67s/it] 74%|███████▍  | 1059/1426 [5:37:08<1:55:48, 18.93s/it]                                                       {'loss': 0.6554, 'learning_rate': 3.2783639471408823e-06, 'epoch': 0.74}
 74%|███████▍  | 1059/1426 [5:37:08<1:55:48, 18.93s/it] 74%|███████▍  | 1060/1426 [5:37:26<1:52:56, 18.52s/it]                                                       {'loss': 0.6398, 'learning_rate': 3.2615624783848853e-06, 'epoch': 0.74}
 74%|███████▍  | 1060/1426 [5:37:26<1:52:56, 18.52s/it] 74%|███████▍  | 1061/1426 [5:37:45<1:54:48, 18.87s/it]                                                       {'loss': 0.6691, 'learning_rate': 3.2447957804065845e-06, 'epoch': 0.74}
 74%|███████▍  | 1061/1426 [5:37:45<1:54:48, 18.87s/it] 74%|███████▍  | 1062/1426 [5:38:04<1:54:02, 18.80s/it]                                                       {'loss': 0.6805, 'learning_rate': 3.228063939723235e-06, 'epoch': 0.74}
 74%|███████▍  | 1062/1426 [5:38:04<1:54:02, 18.80s/it] 75%|███████▍  | 1063/1426 [5:38:20<1:49:01, 18.02s/it]                                                       {'loss': 0.2385, 'learning_rate': 3.211367042672232e-06, 'epoch': 0.75}
 75%|███████▍  | 1063/1426 [5:38:20<1:49:01, 18.02s/it] 75%|███████▍  | 1064/1426 [5:38:39<1:49:19, 18.12s/it]                                                       {'loss': 0.6618, 'learning_rate': 3.1947051754106483e-06, 'epoch': 0.75}
 75%|███████▍  | 1064/1426 [5:38:39<1:49:19, 18.12s/it] 75%|███████▍  | 1065/1426 [5:38:57<1:49:00, 18.12s/it]                                                       {'loss': 0.6652, 'learning_rate': 3.178078423914812e-06, 'epoch': 0.75}
 75%|███████▍  | 1065/1426 [5:38:57<1:49:00, 18.12s/it] 75%|███████▍  | 1066/1426 [5:39:17<1:52:55, 18.82s/it]                                                       {'loss': 0.6761, 'learning_rate': 3.1614868739798497e-06, 'epoch': 0.75}
 75%|███████▍  | 1066/1426 [5:39:17<1:52:55, 18.82s/it] 75%|███████▍  | 1067/1426 [5:39:36<1:53:31, 18.97s/it]                                                       {'loss': 0.6795, 'learning_rate': 3.144930611219238e-06, 'epoch': 0.75}
 75%|███████▍  | 1067/1426 [5:39:36<1:53:31, 18.97s/it] 75%|███████▍  | 1068/1426 [5:39:50<1:44:15, 17.47s/it]                                                       {'loss': 0.2405, 'learning_rate': 3.1284097210643715e-06, 'epoch': 0.75}
 75%|███████▍  | 1068/1426 [5:39:50<1:44:15, 17.47s/it] 75%|███████▍  | 1069/1426 [5:40:09<1:45:49, 17.79s/it]                                                       {'loss': 0.6733, 'learning_rate': 3.1119242887641286e-06, 'epoch': 0.75}
 75%|███████▍  | 1069/1426 [5:40:09<1:45:49, 17.79s/it] 75%|███████▌  | 1070/1426 [5:40:29<1:48:45, 18.33s/it]                                                       {'loss': 0.6461, 'learning_rate': 3.095474399384414e-06, 'epoch': 0.75}
 75%|███████▌  | 1070/1426 [5:40:29<1:48:45, 18.33s/it] 75%|███████▌  | 1071/1426 [5:40:49<1:51:23, 18.83s/it]                                                       {'loss': 0.6731, 'learning_rate': 3.0790601378077277e-06, 'epoch': 0.75}
 75%|███████▌  | 1071/1426 [5:40:49<1:51:23, 18.83s/it] 75%|███████▌  | 1072/1426 [5:41:09<1:53:15, 19.20s/it]                                                       {'loss': 0.6621, 'learning_rate': 3.0626815887327378e-06, 'epoch': 0.75}
 75%|███████▌  | 1072/1426 [5:41:09<1:53:15, 19.20s/it] 75%|███████▌  | 1073/1426 [5:41:26<1:50:09, 18.72s/it]                                                       {'loss': 0.6629, 'learning_rate': 3.0463388366738213e-06, 'epoch': 0.75}
 75%|███████▌  | 1073/1426 [5:41:26<1:50:09, 18.72s/it]this iter is wrong in something... skip...
 75%|███████▌  | 1074/1426 [5:41:47<1:53:00, 19.26s/it]                                                       {'loss': 0.6466, 'learning_rate': 3.0300319659606493e-06, 'epoch': 0.75}
 75%|███████▌  | 1074/1426 [5:41:47<1:53:00, 19.26s/it] 75%|███████▌  | 1075/1426 [5:42:05<1:50:26, 18.88s/it]                                                       {'loss': 0.666, 'learning_rate': 3.013761060737741e-06, 'epoch': 0.75}
 75%|███████▌  | 1075/1426 [5:42:05<1:50:26, 18.88s/it] 75%|███████▌  | 1076/1426 [5:42:27<1:55:12, 19.75s/it]                                                       {'loss': 0.6827, 'learning_rate': 2.9975262049640297e-06, 'epoch': 0.75}
 75%|███████▌  | 1076/1426 [5:42:27<1:55:12, 19.75s/it] 76%|███████▌  | 1077/1426 [5:42:47<1:56:02, 19.95s/it]                                                       {'loss': 0.6631, 'learning_rate': 2.981327482412426e-06, 'epoch': 0.75}
 76%|███████▌  | 1077/1426 [5:42:47<1:56:02, 19.95s/it] 76%|███████▌  | 1078/1426 [5:43:06<1:53:55, 19.64s/it]                                                       {'loss': 0.689, 'learning_rate': 2.9651649766694034e-06, 'epoch': 0.76}
 76%|███████▌  | 1078/1426 [5:43:06<1:53:55, 19.64s/it] 76%|███████▌  | 1079/1426 [5:43:24<1:50:41, 19.14s/it]                                                       {'loss': 0.6934, 'learning_rate': 2.9490387711345412e-06, 'epoch': 0.76}
 76%|███████▌  | 1079/1426 [5:43:24<1:50:41, 19.14s/it] 76%|███████▌  | 1080/1426 [5:43:43<1:49:44, 19.03s/it]                                                       {'loss': 0.6894, 'learning_rate': 2.9329489490201203e-06, 'epoch': 0.76}
 76%|███████▌  | 1080/1426 [5:43:43<1:49:44, 19.03s/it] 76%|███████▌  | 1081/1426 [5:44:03<1:52:01, 19.48s/it]                                                       {'loss': 0.671, 'learning_rate': 2.916895593350665e-06, 'epoch': 0.76}
 76%|███████▌  | 1081/1426 [5:44:03<1:52:01, 19.48s/it] 76%|███████▌  | 1082/1426 [5:44:24<1:53:48, 19.85s/it]                                                       {'loss': 0.6713, 'learning_rate': 2.9008787869625466e-06, 'epoch': 0.76}
 76%|███████▌  | 1082/1426 [5:44:24<1:53:48, 19.85s/it] 76%|███████▌  | 1083/1426 [5:44:45<1:55:41, 20.24s/it]                                                       {'loss': 0.6731, 'learning_rate': 2.884898612503525e-06, 'epoch': 0.76}
 76%|███████▌  | 1083/1426 [5:44:45<1:55:41, 20.24s/it]this iter is wrong in something... skip...
 76%|███████▌  | 1084/1426 [5:45:05<1:55:41, 20.30s/it]                                                       {'loss': 0.6545, 'learning_rate': 2.8689551524323522e-06, 'epoch': 0.76}
 76%|███████▌  | 1084/1426 [5:45:05<1:55:41, 20.30s/it] 76%|███████▌  | 1085/1426 [5:45:26<1:56:36, 20.52s/it]                                                       {'loss': 0.6937, 'learning_rate': 2.8530484890183176e-06, 'epoch': 0.76}
 76%|███████▌  | 1085/1426 [5:45:26<1:56:36, 20.52s/it] 76%|███████▌  | 1086/1426 [5:45:44<1:51:27, 19.67s/it]                                                       {'loss': 0.6596, 'learning_rate': 2.8371787043408426e-06, 'epoch': 0.76}
 76%|███████▌  | 1086/1426 [5:45:44<1:51:27, 19.67s/it] 76%|███████▌  | 1087/1426 [5:46:02<1:47:23, 19.01s/it]                                                       {'loss': 0.6777, 'learning_rate': 2.821345880289058e-06, 'epoch': 0.76}
 76%|███████▌  | 1087/1426 [5:46:02<1:47:23, 19.01s/it] 76%|███████▋  | 1088/1426 [5:46:19<1:44:20, 18.52s/it]                                                       {'loss': 0.6589, 'learning_rate': 2.805550098561364e-06, 'epoch': 0.76}
 76%|███████▋  | 1088/1426 [5:46:19<1:44:20, 18.52s/it] 76%|███████▋  | 1089/1426 [5:46:37<1:43:22, 18.41s/it]                                                       {'loss': 0.6541, 'learning_rate': 2.789791440665031e-06, 'epoch': 0.76}
 76%|███████▋  | 1089/1426 [5:46:37<1:43:22, 18.41s/it] 76%|███████▋  | 1090/1426 [5:46:55<1:42:56, 18.38s/it]                                                       {'loss': 0.6767, 'learning_rate': 2.7740699879157573e-06, 'epoch': 0.76}
 76%|███████▋  | 1090/1426 [5:46:55<1:42:56, 18.38s/it] 77%|███████▋  | 1091/1426 [5:47:14<1:42:52, 18.43s/it]                                                       {'loss': 0.658, 'learning_rate': 2.7583858214372726e-06, 'epoch': 0.76}
 77%|███████▋  | 1091/1426 [5:47:14<1:42:52, 18.43s/it] 77%|███████▋  | 1092/1426 [5:47:33<1:44:03, 18.69s/it]                                                       {'loss': 0.6553, 'learning_rate': 2.742739022160893e-06, 'epoch': 0.77}
 77%|███████▋  | 1092/1426 [5:47:33<1:44:03, 18.69s/it] 77%|███████▋  | 1093/1426 [5:47:54<1:47:45, 19.42s/it]                                                       {'loss': 0.6846, 'learning_rate': 2.727129670825133e-06, 'epoch': 0.77}
 77%|███████▋  | 1093/1426 [5:47:54<1:47:45, 19.42s/it] 77%|███████▋  | 1094/1426 [5:48:13<1:46:36, 19.27s/it]                                                       {'loss': 0.663, 'learning_rate': 2.711557847975259e-06, 'epoch': 0.77}
 77%|███████▋  | 1094/1426 [5:48:13<1:46:36, 19.27s/it] 77%|███████▋  | 1095/1426 [5:48:33<1:47:13, 19.44s/it]                                                       {'loss': 0.6964, 'learning_rate': 2.6960236339628933e-06, 'epoch': 0.77}
 77%|███████▋  | 1095/1426 [5:48:33<1:47:13, 19.44s/it] 77%|███████▋  | 1096/1426 [5:48:53<1:46:58, 19.45s/it]                                                       {'loss': 0.6762, 'learning_rate': 2.6805271089455984e-06, 'epoch': 0.77}
 77%|███████▋  | 1096/1426 [5:48:53<1:46:58, 19.45s/it]this iter is wrong in something... skip...
 77%|███████▋  | 1097/1426 [5:49:10<1:43:43, 18.92s/it]                                                       {'loss': 0.679, 'learning_rate': 2.665068352886452e-06, 'epoch': 0.77}
 77%|███████▋  | 1097/1426 [5:49:10<1:43:43, 18.92s/it] 77%|███████▋  | 1098/1426 [5:49:30<1:44:32, 19.12s/it]                                                       {'loss': 0.6738, 'learning_rate': 2.649647445553649e-06, 'epoch': 0.77}
 77%|███████▋  | 1098/1426 [5:49:30<1:44:32, 19.12s/it] 77%|███████▋  | 1099/1426 [5:49:52<1:49:12, 20.04s/it]                                                       {'loss': 0.6565, 'learning_rate': 2.6342644665200736e-06, 'epoch': 0.77}
 77%|███████▋  | 1099/1426 [5:49:52<1:49:12, 20.04s/it] 77%|███████▋  | 1100/1426 [5:50:13<1:51:02, 20.44s/it]                                                       {'loss': 0.6618, 'learning_rate': 2.618919495162907e-06, 'epoch': 0.77}
 77%|███████▋  | 1100/1426 [5:50:13<1:51:02, 20.44s/it] 77%|███████▋  | 1101/1426 [5:50:33<1:49:24, 20.20s/it]                                                       {'loss': 0.6562, 'learning_rate': 2.6036126106631986e-06, 'epoch': 0.77}
 77%|███████▋  | 1101/1426 [5:50:33<1:49:24, 20.20s/it] 77%|███████▋  | 1102/1426 [5:50:51<1:44:41, 19.39s/it]                                                       {'loss': 0.6726, 'learning_rate': 2.588343892005478e-06, 'epoch': 0.77}
 77%|███████▋  | 1102/1426 [5:50:51<1:44:41, 19.39s/it] 77%|███████▋  | 1103/1426 [5:51:10<1:44:21, 19.38s/it]                                                       {'loss': 0.6989, 'learning_rate': 2.573113417977329e-06, 'epoch': 0.77}
 77%|███████▋  | 1103/1426 [5:51:10<1:44:21, 19.38s/it] 77%|███████▋  | 1104/1426 [5:51:30<1:44:23, 19.45s/it]                                                       {'loss': 0.6505, 'learning_rate': 2.557921267168986e-06, 'epoch': 0.77}
 77%|███████▋  | 1104/1426 [5:51:30<1:44:23, 19.45s/it] 77%|███████▋  | 1105/1426 [5:51:47<1:41:31, 18.98s/it]                                                       {'loss': 0.6616, 'learning_rate': 2.542767517972945e-06, 'epoch': 0.77}
 77%|███████▋  | 1105/1426 [5:51:47<1:41:31, 18.98s/it] 78%|███████▊  | 1106/1426 [5:52:04<1:37:53, 18.35s/it]                                                       {'loss': 0.6939, 'learning_rate': 2.527652248583543e-06, 'epoch': 0.78}
 78%|███████▊  | 1106/1426 [5:52:04<1:37:53, 18.35s/it] 78%|███████▊  | 1107/1426 [5:52:22<1:36:35, 18.17s/it]                                                       {'loss': 0.6461, 'learning_rate': 2.5125755369965555e-06, 'epoch': 0.78}
 78%|███████▊  | 1107/1426 [5:52:22<1:36:35, 18.17s/it] 78%|███████▊  | 1108/1426 [5:52:46<1:45:24, 19.89s/it]                                                       {'loss': 0.667, 'learning_rate': 2.4975374610087957e-06, 'epoch': 0.78}
 78%|███████▊  | 1108/1426 [5:52:46<1:45:24, 19.89s/it] 78%|███████▊  | 1109/1426 [5:53:05<1:43:54, 19.67s/it]                                                       {'loss': 0.6748, 'learning_rate': 2.482538098217723e-06, 'epoch': 0.78}
 78%|███████▊  | 1109/1426 [5:53:05<1:43:54, 19.67s/it] 78%|███████▊  | 1110/1426 [5:53:26<1:44:43, 19.88s/it]                                                       {'loss': 0.675, 'learning_rate': 2.4675775260210232e-06, 'epoch': 0.78}
 78%|███████▊  | 1110/1426 [5:53:26<1:44:43, 19.88s/it] 78%|███████▊  | 1111/1426 [5:53:43<1:41:04, 19.25s/it]                                                       {'loss': 0.6398, 'learning_rate': 2.452655821616232e-06, 'epoch': 0.78}
 78%|███████▊  | 1111/1426 [5:53:43<1:41:04, 19.25s/it] 78%|███████▊  | 1112/1426 [5:54:01<1:37:41, 18.67s/it]                                                       {'loss': 0.653, 'learning_rate': 2.437773062000317e-06, 'epoch': 0.78}
 78%|███████▊  | 1112/1426 [5:54:01<1:37:41, 18.67s/it] 78%|███████▊  | 1113/1426 [5:54:19<1:36:35, 18.52s/it]                                                       {'loss': 0.674, 'learning_rate': 2.422929323969285e-06, 'epoch': 0.78}
 78%|███████▊  | 1113/1426 [5:54:19<1:36:35, 18.52s/it] 78%|███████▊  | 1114/1426 [5:54:38<1:37:22, 18.73s/it]                                                       {'loss': 0.7012, 'learning_rate': 2.4081246841177973e-06, 'epoch': 0.78}
 78%|███████▊  | 1114/1426 [5:54:38<1:37:22, 18.73s/it] 78%|███████▊  | 1115/1426 [5:54:57<1:38:03, 18.92s/it]                                                       {'loss': 0.6575, 'learning_rate': 2.3933592188387654e-06, 'epoch': 0.78}
 78%|███████▊  | 1115/1426 [5:54:57<1:38:03, 18.92s/it] 78%|███████▊  | 1116/1426 [5:55:18<1:39:45, 19.31s/it]                                                       {'loss': 0.6688, 'learning_rate': 2.3786330043229523e-06, 'epoch': 0.78}
 78%|███████▊  | 1116/1426 [5:55:18<1:39:45, 19.31s/it] 78%|███████▊  | 1117/1426 [5:55:40<1:43:48, 20.16s/it]                                                       {'loss': 0.6657, 'learning_rate': 2.363946116558583e-06, 'epoch': 0.78}
 78%|███████▊  | 1117/1426 [5:55:40<1:43:48, 20.16s/it] 78%|███████▊  | 1118/1426 [5:56:00<1:43:41, 20.20s/it]                                                       {'loss': 0.6649, 'learning_rate': 2.3492986313309628e-06, 'epoch': 0.78}
 78%|███████▊  | 1118/1426 [5:56:00<1:43:41, 20.20s/it] 78%|███████▊  | 1119/1426 [5:56:14<1:33:54, 18.35s/it]                                                       {'loss': 0.2232, 'learning_rate': 2.3346906242220668e-06, 'epoch': 0.78}
 78%|███████▊  | 1119/1426 [5:56:14<1:33:54, 18.35s/it] 79%|███████▊  | 1120/1426 [5:56:34<1:35:21, 18.70s/it]                                                       {'loss': 0.6603, 'learning_rate': 2.320122170610172e-06, 'epoch': 0.79}
 79%|███████▊  | 1120/1426 [5:56:34<1:35:21, 18.70s/it] 79%|███████▊  | 1121/1426 [5:56:52<1:35:07, 18.71s/it]                                                       {'loss': 0.6479, 'learning_rate': 2.305593345669447e-06, 'epoch': 0.79}
 79%|███████▊  | 1121/1426 [5:56:52<1:35:07, 18.71s/it] 79%|███████▊  | 1122/1426 [5:57:12<1:35:48, 18.91s/it]                                                       {'loss': 0.6729, 'learning_rate': 2.2911042243695745e-06, 'epoch': 0.79}
 79%|███████▊  | 1122/1426 [5:57:12<1:35:48, 18.91s/it] 79%|███████▉  | 1123/1426 [5:57:33<1:39:44, 19.75s/it]                                                       {'loss': 0.6906, 'learning_rate': 2.2766548814753696e-06, 'epoch': 0.79}
 79%|███████▉  | 1123/1426 [5:57:33<1:39:44, 19.75s/it] 79%|███████▉  | 1124/1426 [5:57:52<1:37:31, 19.38s/it]                                                       {'loss': 0.6586, 'learning_rate': 2.2622453915463893e-06, 'epoch': 0.79}
 79%|███████▉  | 1124/1426 [5:57:52<1:37:31, 19.38s/it] 79%|███████▉  | 1125/1426 [5:58:10<1:35:37, 19.06s/it]                                                       {'loss': 0.6728, 'learning_rate': 2.2478758289365356e-06, 'epoch': 0.79}
 79%|███████▉  | 1125/1426 [5:58:10<1:35:37, 19.06s/it] 79%|███████▉  | 1126/1426 [5:58:31<1:37:55, 19.59s/it]                                                       {'loss': 0.6634, 'learning_rate': 2.233546267793696e-06, 'epoch': 0.79}
 79%|███████▉  | 1126/1426 [5:58:31<1:37:55, 19.59s/it] 79%|███████▉  | 1127/1426 [5:58:53<1:40:31, 20.17s/it]                                                       {'loss': 0.6425, 'learning_rate': 2.219256782059339e-06, 'epoch': 0.79}
 79%|███████▉  | 1127/1426 [5:58:53<1:40:31, 20.17s/it] 79%|███████▉  | 1128/1426 [5:59:11<1:37:18, 19.59s/it]                                                       {'loss': 0.665, 'learning_rate': 2.2050074454681427e-06, 'epoch': 0.79}
 79%|███████▉  | 1128/1426 [5:59:11<1:37:18, 19.59s/it] 79%|███████▉  | 1129/1426 [5:59:32<1:38:46, 19.95s/it]                                                       {'loss': 0.6655, 'learning_rate': 2.1907983315476176e-06, 'epoch': 0.79}
 79%|███████▉  | 1129/1426 [5:59:32<1:38:46, 19.95s/it] 79%|███████▉  | 1130/1426 [5:59:51<1:38:16, 19.92s/it]                                                       {'loss': 0.6315, 'learning_rate': 2.1766295136177185e-06, 'epoch': 0.79}
 79%|███████▉  | 1130/1426 [5:59:51<1:38:16, 19.92s/it] 79%|███████▉  | 1131/1426 [6:00:12<1:38:34, 20.05s/it]                                                       {'loss': 0.6426, 'learning_rate': 2.1625010647904686e-06, 'epoch': 0.79}
 79%|███████▉  | 1131/1426 [6:00:12<1:38:34, 20.05s/it] 79%|███████▉  | 1132/1426 [6:00:34<1:40:47, 20.57s/it]                                                       {'loss': 0.6756, 'learning_rate': 2.1484130579695883e-06, 'epoch': 0.79}
 79%|███████▉  | 1132/1426 [6:00:34<1:40:47, 20.57s/it] 79%|███████▉  | 1133/1426 [6:00:52<1:36:57, 19.86s/it]                                                       {'loss': 0.6511, 'learning_rate': 2.134365565850117e-06, 'epoch': 0.79}
 79%|███████▉  | 1133/1426 [6:00:52<1:36:57, 19.86s/it] 80%|███████▉  | 1134/1426 [6:01:10<1:33:49, 19.28s/it]                                                       {'loss': 0.6549, 'learning_rate': 2.120358660918025e-06, 'epoch': 0.79}
 80%|███████▉  | 1134/1426 [6:01:10<1:33:49, 19.28s/it] 80%|███████▉  | 1135/1426 [6:01:32<1:38:25, 20.29s/it]                                                       {'loss': 0.6736, 'learning_rate': 2.1063924154498626e-06, 'epoch': 0.8}
 80%|███████▉  | 1135/1426 [6:01:32<1:38:25, 20.29s/it] 80%|███████▉  | 1136/1426 [6:01:53<1:38:45, 20.43s/it]                                                       {'loss': 0.6614, 'learning_rate': 2.0924669015123643e-06, 'epoch': 0.8}
 80%|███████▉  | 1136/1426 [6:01:53<1:38:45, 20.43s/it] 80%|███████▉  | 1137/1426 [6:02:14<1:39:45, 20.71s/it]                                                       {'loss': 0.6693, 'learning_rate': 2.078582190962091e-06, 'epoch': 0.8}
 80%|███████▉  | 1137/1426 [6:02:14<1:39:45, 20.71s/it] 80%|███████▉  | 1138/1426 [6:02:35<1:39:26, 20.72s/it]                                                       {'loss': 0.658, 'learning_rate': 2.064738355445057e-06, 'epoch': 0.8}
 80%|███████▉  | 1138/1426 [6:02:35<1:39:26, 20.72s/it] 80%|███████▉  | 1139/1426 [6:02:56<1:39:50, 20.87s/it]                                                       {'loss': 0.6596, 'learning_rate': 2.0509354663963642e-06, 'epoch': 0.8}
 80%|███████▉  | 1139/1426 [6:02:56<1:39:50, 20.87s/it] 80%|███████▉  | 1140/1426 [6:03:17<1:38:48, 20.73s/it]                                                       {'loss': 0.6533, 'learning_rate': 2.03717359503981e-06, 'epoch': 0.8}
 80%|███████▉  | 1140/1426 [6:03:17<1:38:48, 20.73s/it] 80%|████████  | 1141/1426 [6:03:34<1:33:32, 19.69s/it]                                                       {'loss': 0.6741, 'learning_rate': 2.0234528123875554e-06, 'epoch': 0.8}
 80%|████████  | 1141/1426 [6:03:34<1:33:32, 19.69s/it] 80%|████████  | 1142/1426 [6:03:55<1:34:16, 19.92s/it]                                                       {'loss': 0.6528, 'learning_rate': 2.0097731892397364e-06, 'epoch': 0.8}
 80%|████████  | 1142/1426 [6:03:55<1:34:16, 19.92s/it] 80%|████████  | 1143/1426 [6:04:13<1:31:42, 19.44s/it]                                                       {'loss': 0.6683, 'learning_rate': 1.9961347961840982e-06, 'epoch': 0.8}
 80%|████████  | 1143/1426 [6:04:13<1:31:42, 19.44s/it] 80%|████████  | 1144/1426 [6:04:33<1:32:37, 19.71s/it]                                                       {'loss': 0.6916, 'learning_rate': 1.982537703595644e-06, 'epoch': 0.8}
 80%|████████  | 1144/1426 [6:04:33<1:32:37, 19.71s/it] 80%|████████  | 1145/1426 [6:04:54<1:33:09, 19.89s/it]                                                       {'loss': 0.6735, 'learning_rate': 1.9689819816362567e-06, 'epoch': 0.8}
 80%|████████  | 1145/1426 [6:04:54<1:33:09, 19.89s/it] 80%|████████  | 1146/1426 [6:05:09<1:26:42, 18.58s/it]                                                       {'loss': 0.2219, 'learning_rate': 1.9554677002543452e-06, 'epoch': 0.8}
 80%|████████  | 1146/1426 [6:05:09<1:26:42, 18.58s/it]WARNING: tokenization mismatch: 1 vs. 624. (ignored)
 80%|████████  | 1147/1426 [6:05:27<1:24:53, 18.26s/it]                                                       {'loss': 0.6708, 'learning_rate': 1.9419949291844862e-06, 'epoch': 0.8}
 80%|████████  | 1147/1426 [6:05:27<1:24:53, 18.26s/it] 81%|████████  | 1148/1426 [6:05:46<1:26:04, 18.58s/it]                                                       {'loss': 0.645, 'learning_rate': 1.9285637379470613e-06, 'epoch': 0.8}
 81%|████████  | 1148/1426 [6:05:46<1:26:04, 18.58s/it] 81%|████████  | 1149/1426 [6:06:00<1:19:47, 17.28s/it]                                                       {'loss': 0.2284, 'learning_rate': 1.915174195847891e-06, 'epoch': 0.81}
 81%|████████  | 1149/1426 [6:06:00<1:19:47, 17.28s/it] 81%|████████  | 1150/1426 [6:06:20<1:22:41, 17.98s/it]                                                       {'loss': 0.6706, 'learning_rate': 1.9018263719778851e-06, 'epoch': 0.81}
 81%|████████  | 1150/1426 [6:06:20<1:22:41, 17.98s/it] 81%|████████  | 1151/1426 [6:06:34<1:16:54, 16.78s/it]                                                       {'loss': 0.237, 'learning_rate': 1.8885203352126912e-06, 'epoch': 0.81}
 81%|████████  | 1151/1426 [6:06:34<1:16:54, 16.78s/it] 81%|████████  | 1152/1426 [6:06:53<1:20:16, 17.58s/it]                                                       {'loss': 0.7021, 'learning_rate': 1.8752561542123226e-06, 'epoch': 0.81}
 81%|████████  | 1152/1426 [6:06:53<1:20:16, 17.58s/it] 81%|████████  | 1153/1426 [6:07:13<1:23:13, 18.29s/it]                                                       {'loss': 0.6688, 'learning_rate': 1.8620338974208263e-06, 'epoch': 0.81}
 81%|████████  | 1153/1426 [6:07:13<1:23:13, 18.29s/it] 81%|████████  | 1154/1426 [6:07:32<1:23:54, 18.51s/it]                                                       {'loss': 0.686, 'learning_rate': 1.8488536330659068e-06, 'epoch': 0.81}
 81%|████████  | 1154/1426 [6:07:32<1:23:54, 18.51s/it] 81%|████████  | 1155/1426 [6:07:54<1:28:23, 19.57s/it]                                                       {'loss': 0.671, 'learning_rate': 1.8357154291585877e-06, 'epoch': 0.81}
 81%|████████  | 1155/1426 [6:07:54<1:28:23, 19.57s/it] 81%|████████  | 1156/1426 [6:08:14<1:27:46, 19.51s/it]                                                       {'loss': 0.6544, 'learning_rate': 1.8226193534928604e-06, 'epoch': 0.81}
 81%|████████  | 1156/1426 [6:08:14<1:27:46, 19.51s/it] 81%|████████  | 1157/1426 [6:08:28<1:20:08, 17.87s/it]                                                       {'loss': 0.2222, 'learning_rate': 1.8095654736453328e-06, 'epoch': 0.81}
 81%|████████  | 1157/1426 [6:08:28<1:20:08, 17.87s/it] 81%|████████  | 1158/1426 [6:08:48<1:22:46, 18.53s/it]                                                       {'loss': 0.661, 'learning_rate': 1.7965538569748775e-06, 'epoch': 0.81}
 81%|████████  | 1158/1426 [6:08:48<1:22:46, 18.53s/it] 81%|████████▏ | 1159/1426 [6:09:05<1:21:11, 18.25s/it]                                                       {'loss': 0.6884, 'learning_rate': 1.783584570622281e-06, 'epoch': 0.81}
 81%|████████▏ | 1159/1426 [6:09:05<1:21:11, 18.25s/it] 81%|████████▏ | 1160/1426 [6:09:25<1:23:01, 18.73s/it]                                                       {'loss': 0.6702, 'learning_rate': 1.7706576815099142e-06, 'epoch': 0.81}
 81%|████████▏ | 1160/1426 [6:09:25<1:23:01, 18.73s/it] 81%|████████▏ | 1161/1426 [6:09:46<1:25:19, 19.32s/it]                                                       {'loss': 0.6649, 'learning_rate': 1.7577732563413641e-06, 'epoch': 0.81}
 81%|████████▏ | 1161/1426 [6:09:46<1:25:19, 19.32s/it] 81%|████████▏ | 1162/1426 [6:10:05<1:24:18, 19.16s/it]                                                       {'loss': 0.6677, 'learning_rate': 1.744931361601111e-06, 'epoch': 0.81}
 81%|████████▏ | 1162/1426 [6:10:05<1:24:18, 19.16s/it] 82%|████████▏ | 1163/1426 [6:10:24<1:24:11, 19.21s/it]                                                       {'loss': 0.6469, 'learning_rate': 1.7321320635541684e-06, 'epoch': 0.82}
 82%|████████▏ | 1163/1426 [6:10:24<1:24:11, 19.21s/it] 82%|████████▏ | 1164/1426 [6:10:44<1:24:55, 19.45s/it]                                                       {'loss': 0.6397, 'learning_rate': 1.7193754282457465e-06, 'epoch': 0.82}
 82%|████████▏ | 1164/1426 [6:10:44<1:24:55, 19.45s/it] 82%|████████▏ | 1165/1426 [6:11:04<1:25:05, 19.56s/it]                                                       {'loss': 0.684, 'learning_rate': 1.7066615215009196e-06, 'epoch': 0.82}
 82%|████████▏ | 1165/1426 [6:11:04<1:25:05, 19.56s/it] 82%|████████▏ | 1166/1426 [6:11:22<1:23:05, 19.17s/it]                                                       {'loss': 0.6785, 'learning_rate': 1.6939904089242797e-06, 'epoch': 0.82}
 82%|████████▏ | 1166/1426 [6:11:22<1:23:05, 19.17s/it] 82%|████████▏ | 1167/1426 [6:11:42<1:23:43, 19.40s/it]                                                       {'loss': 0.6624, 'learning_rate': 1.681362155899593e-06, 'epoch': 0.82}
 82%|████████▏ | 1167/1426 [6:11:42<1:23:43, 19.40s/it] 82%|████████▏ | 1168/1426 [6:12:00<1:21:06, 18.86s/it]                                                       {'loss': 0.6805, 'learning_rate': 1.6687768275894667e-06, 'epoch': 0.82}
 82%|████████▏ | 1168/1426 [6:12:00<1:21:06, 18.86s/it] 82%|████████▏ | 1169/1426 [6:12:18<1:20:15, 18.74s/it]                                                       {'loss': 0.6762, 'learning_rate': 1.6562344889350224e-06, 'epoch': 0.82}
 82%|████████▏ | 1169/1426 [6:12:18<1:20:15, 18.74s/it] 82%|████████▏ | 1170/1426 [6:12:38<1:21:25, 19.09s/it]                                                       {'loss': 0.6472, 'learning_rate': 1.643735204655541e-06, 'epoch': 0.82}
 82%|████████▏ | 1170/1426 [6:12:38<1:21:25, 19.09s/it] 82%|████████▏ | 1171/1426 [6:12:58<1:22:19, 19.37s/it]                                                       {'loss': 0.6757, 'learning_rate': 1.6312790392481504e-06, 'epoch': 0.82}
 82%|████████▏ | 1171/1426 [6:12:58<1:22:19, 19.37s/it] 82%|████████▏ | 1172/1426 [6:13:18<1:22:18, 19.44s/it]                                                       {'loss': 0.6286, 'learning_rate': 1.618866056987477e-06, 'epoch': 0.82}
 82%|████████▏ | 1172/1426 [6:13:18<1:22:18, 19.44s/it] 82%|████████▏ | 1173/1426 [6:13:32<1:15:21, 17.87s/it]                                                       {'loss': 0.2348, 'learning_rate': 1.606496321925315e-06, 'epoch': 0.82}
 82%|████████▏ | 1173/1426 [6:13:32<1:15:21, 17.87s/it] 82%|████████▏ | 1174/1426 [6:13:52<1:17:40, 18.49s/it]                                                       {'loss': 0.6963, 'learning_rate': 1.5941698978903096e-06, 'epoch': 0.82}
 82%|████████▏ | 1174/1426 [6:13:52<1:17:40, 18.49s/it] 82%|████████▏ | 1175/1426 [6:14:11<1:18:58, 18.88s/it]                                                       {'loss': 0.6724, 'learning_rate': 1.5818868484876159e-06, 'epoch': 0.82}
 82%|████████▏ | 1175/1426 [6:14:11<1:18:58, 18.88s/it] 82%|████████▏ | 1176/1426 [6:14:30<1:18:31, 18.85s/it]                                                       {'loss': 0.6574, 'learning_rate': 1.5696472370985672e-06, 'epoch': 0.82}
 82%|████████▏ | 1176/1426 [6:14:30<1:18:31, 18.85s/it] 83%|████████▎ | 1177/1426 [6:14:53<1:23:11, 20.05s/it]                                                       {'loss': 0.678, 'learning_rate': 1.5574511268803572e-06, 'epoch': 0.83}
 83%|████████▎ | 1177/1426 [6:14:53<1:23:11, 20.05s/it] 83%|████████▎ | 1178/1426 [6:15:13<1:22:45, 20.02s/it]                                                       {'loss': 0.6849, 'learning_rate': 1.545298580765715e-06, 'epoch': 0.83}
 83%|████████▎ | 1178/1426 [6:15:13<1:22:45, 20.02s/it] 83%|████████▎ | 1179/1426 [6:15:34<1:23:12, 20.21s/it]                                                       {'loss': 0.7005, 'learning_rate': 1.5331896614625675e-06, 'epoch': 0.83}
 83%|████████▎ | 1179/1426 [6:15:34<1:23:12, 20.21s/it] 83%|████████▎ | 1180/1426 [6:15:48<1:15:22, 18.38s/it]                                                       {'loss': 0.2225, 'learning_rate': 1.521124431453731e-06, 'epoch': 0.83}
 83%|████████▎ | 1180/1426 [6:15:48<1:15:22, 18.38s/it] 83%|████████▎ | 1181/1426 [6:16:08<1:17:01, 18.86s/it]                                                       {'loss': 0.6704, 'learning_rate': 1.5091029529965861e-06, 'epoch': 0.83}
 83%|████████▎ | 1181/1426 [6:16:08<1:17:01, 18.86s/it] 83%|████████▎ | 1182/1426 [6:16:29<1:19:06, 19.45s/it]                                                       {'loss': 0.6854, 'learning_rate': 1.4971252881227373e-06, 'epoch': 0.83}
 83%|████████▎ | 1182/1426 [6:16:29<1:19:06, 19.45s/it] 83%|████████▎ | 1183/1426 [6:16:48<1:18:44, 19.44s/it]                                                       {'loss': 0.6748, 'learning_rate': 1.4851914986377204e-06, 'epoch': 0.83}
 83%|████████▎ | 1183/1426 [6:16:48<1:18:44, 19.44s/it] 83%|████████▎ | 1184/1426 [6:17:08<1:18:45, 19.53s/it]                                                       {'loss': 0.6458, 'learning_rate': 1.4733016461206727e-06, 'epoch': 0.83}
 83%|████████▎ | 1184/1426 [6:17:08<1:18:45, 19.53s/it] 83%|████████▎ | 1185/1426 [6:17:27<1:18:11, 19.47s/it]                                                       {'loss': 0.6627, 'learning_rate': 1.461455791924008e-06, 'epoch': 0.83}
 83%|████████▎ | 1185/1426 [6:17:27<1:18:11, 19.47s/it] 83%|████████▎ | 1186/1426 [6:17:44<1:14:37, 18.66s/it]                                                       {'loss': 0.6607, 'learning_rate': 1.4496539971731028e-06, 'epoch': 0.83}
 83%|████████▎ | 1186/1426 [6:17:44<1:14:37, 18.66s/it] 83%|████████▎ | 1187/1426 [6:18:03<1:14:53, 18.80s/it]                                                       {'loss': 0.6602, 'learning_rate': 1.4378963227659959e-06, 'epoch': 0.83}
 83%|████████▎ | 1187/1426 [6:18:03<1:14:53, 18.80s/it] 83%|████████▎ | 1188/1426 [6:18:25<1:18:45, 19.85s/it]                                                       {'loss': 0.6634, 'learning_rate': 1.4261828293730495e-06, 'epoch': 0.83}
 83%|████████▎ | 1188/1426 [6:18:25<1:18:45, 19.85s/it] 83%|████████▎ | 1189/1426 [6:18:44<1:17:12, 19.54s/it]                                                       {'loss': 0.6649, 'learning_rate': 1.4145135774366558e-06, 'epoch': 0.83}
 83%|████████▎ | 1189/1426 [6:18:44<1:17:12, 19.54s/it] 83%|████████▎ | 1190/1426 [6:19:04<1:17:22, 19.67s/it]                                                       {'loss': 0.7216, 'learning_rate': 1.402888627170923e-06, 'epoch': 0.83}
 83%|████████▎ | 1190/1426 [6:19:04<1:17:22, 19.67s/it] 84%|████████▎ | 1191/1426 [6:19:25<1:18:59, 20.17s/it]                                                       {'loss': 0.6407, 'learning_rate': 1.3913080385613443e-06, 'epoch': 0.83}
 84%|████████▎ | 1191/1426 [6:19:25<1:18:59, 20.17s/it] 84%|████████▎ | 1192/1426 [6:19:45<1:18:23, 20.10s/it]                                                       {'loss': 0.6624, 'learning_rate': 1.3797718713645169e-06, 'epoch': 0.84}
 84%|████████▎ | 1192/1426 [6:19:45<1:18:23, 20.10s/it] 84%|████████▎ | 1193/1426 [6:20:04<1:16:34, 19.72s/it]                                                       {'loss': 0.6682, 'learning_rate': 1.3682801851078208e-06, 'epoch': 0.84}
 84%|████████▎ | 1193/1426 [6:20:04<1:16:34, 19.72s/it] 84%|████████▎ | 1194/1426 [6:20:25<1:17:04, 19.93s/it]                                                       {'loss': 0.7077, 'learning_rate': 1.3568330390891038e-06, 'epoch': 0.84}
 84%|████████▎ | 1194/1426 [6:20:25<1:17:04, 19.93s/it] 84%|████████▍ | 1195/1426 [6:20:47<1:19:05, 20.54s/it]                                                       {'loss': 0.6688, 'learning_rate': 1.345430492376385e-06, 'epoch': 0.84}
 84%|████████▍ | 1195/1426 [6:20:47<1:19:05, 20.54s/it] 84%|████████▍ | 1196/1426 [6:21:07<1:18:17, 20.42s/it]                                                       {'loss': 0.6437, 'learning_rate': 1.3340726038075558e-06, 'epoch': 0.84}
 84%|████████▍ | 1196/1426 [6:21:07<1:18:17, 20.42s/it] 84%|████████▍ | 1197/1426 [6:21:27<1:18:02, 20.45s/it]                                                       {'loss': 0.6599, 'learning_rate': 1.3227594319900572e-06, 'epoch': 0.84}
 84%|████████▍ | 1197/1426 [6:21:27<1:18:02, 20.45s/it] 84%|████████▍ | 1198/1426 [6:21:41<1:10:18, 18.50s/it]                                                       {'loss': 0.233, 'learning_rate': 1.3114910353005972e-06, 'epoch': 0.84}
 84%|████████▍ | 1198/1426 [6:21:41<1:10:18, 18.50s/it] 84%|████████▍ | 1199/1426 [6:22:03<1:13:28, 19.42s/it]                                                       {'loss': 0.6747, 'learning_rate': 1.300267471884843e-06, 'epoch': 0.84}
 84%|████████▍ | 1199/1426 [6:22:03<1:13:28, 19.42s/it] 84%|████████▍ | 1200/1426 [6:22:22<1:13:10, 19.43s/it]                                                       {'loss': 0.6501, 'learning_rate': 1.2890887996571066e-06, 'epoch': 0.84}
 84%|████████▍ | 1200/1426 [6:22:22<1:13:10, 19.43s/it] 84%|████████▍ | 1201/1426 [6:22:42<1:13:34, 19.62s/it]                                                       {'loss': 0.6595, 'learning_rate': 1.2779550763000703e-06, 'epoch': 0.84}
 84%|████████▍ | 1201/1426 [6:22:42<1:13:34, 19.62s/it] 84%|████████▍ | 1202/1426 [6:23:00<1:11:20, 19.11s/it]                                                       {'loss': 0.7056, 'learning_rate': 1.266866359264477e-06, 'epoch': 0.84}
 84%|████████▍ | 1202/1426 [6:23:00<1:11:20, 19.11s/it] 84%|████████▍ | 1203/1426 [6:23:16<1:07:28, 18.16s/it]                                                       {'loss': 0.2304, 'learning_rate': 1.2558227057688255e-06, 'epoch': 0.84}
 84%|████████▍ | 1203/1426 [6:23:16<1:07:28, 18.16s/it] 84%|████████▍ | 1204/1426 [6:23:34<1:07:03, 18.12s/it]                                                       {'loss': 0.6569, 'learning_rate': 1.244824172799094e-06, 'epoch': 0.84}
 84%|████████▍ | 1204/1426 [6:23:34<1:07:03, 18.12s/it] 85%|████████▍ | 1205/1426 [6:23:52<1:06:34, 18.08s/it]                                                       {'loss': 0.6734, 'learning_rate': 1.233870817108429e-06, 'epoch': 0.84}
 85%|████████▍ | 1205/1426 [6:23:52<1:06:34, 18.08s/it] 85%|████████▍ | 1206/1426 [6:24:12<1:08:39, 18.73s/it]                                                       {'loss': 0.6921, 'learning_rate': 1.2229626952168583e-06, 'epoch': 0.85}
 85%|████████▍ | 1206/1426 [6:24:12<1:08:39, 18.73s/it] 85%|████████▍ | 1207/1426 [6:24:34<1:11:26, 19.57s/it]                                                       {'loss': 0.6883, 'learning_rate': 1.2120998634110048e-06, 'epoch': 0.85}
 85%|████████▍ | 1207/1426 [6:24:34<1:11:26, 19.57s/it] 85%|████████▍ | 1208/1426 [6:24:53<1:11:04, 19.56s/it]                                                       {'loss': 0.6609, 'learning_rate': 1.2012823777437965e-06, 'epoch': 0.85}
 85%|████████▍ | 1208/1426 [6:24:53<1:11:04, 19.56s/it] 85%|████████▍ | 1209/1426 [6:25:11<1:08:57, 19.07s/it]                                                       {'loss': 0.6695, 'learning_rate': 1.1905102940341561e-06, 'epoch': 0.85}
 85%|████████▍ | 1209/1426 [6:25:11<1:08:57, 19.07s/it] 85%|████████▍ | 1210/1426 [6:25:29<1:07:12, 18.67s/it]                                                       {'loss': 0.6943, 'learning_rate': 1.1797836678667428e-06, 'epoch': 0.85}
 85%|████████▍ | 1210/1426 [6:25:29<1:07:12, 18.67s/it] 85%|████████▍ | 1211/1426 [6:25:49<1:07:59, 18.98s/it]                                                       {'loss': 0.6698, 'learning_rate': 1.1691025545916511e-06, 'epoch': 0.85}
 85%|████████▍ | 1211/1426 [6:25:49<1:07:59, 18.98s/it] 85%|████████▍ | 1212/1426 [6:26:07<1:06:39, 18.69s/it]                                                       {'loss': 0.6544, 'learning_rate': 1.1584670093241202e-06, 'epoch': 0.85}
 85%|████████▍ | 1212/1426 [6:26:07<1:06:39, 18.69s/it] 85%|████████▌ | 1213/1426 [6:26:28<1:09:07, 19.47s/it]                                                       {'loss': 0.6774, 'learning_rate': 1.147877086944259e-06, 'epoch': 0.85}
 85%|████████▌ | 1213/1426 [6:26:28<1:09:07, 19.47s/it] 85%|████████▌ | 1214/1426 [6:26:49<1:10:45, 20.03s/it]                                                       {'loss': 0.6829, 'learning_rate': 1.137332842096759e-06, 'epoch': 0.85}
 85%|████████▌ | 1214/1426 [6:26:49<1:10:45, 20.03s/it] 85%|████████▌ | 1215/1426 [6:27:09<1:10:20, 20.00s/it]                                                       {'loss': 0.6199, 'learning_rate': 1.1268343291906102e-06, 'epoch': 0.85}
 85%|████████▌ | 1215/1426 [6:27:09<1:10:20, 20.00s/it] 85%|████████▌ | 1216/1426 [6:27:30<1:10:53, 20.25s/it]                                                       {'loss': 0.6765, 'learning_rate': 1.116381602398826e-06, 'epoch': 0.85}
 85%|████████▌ | 1216/1426 [6:27:30<1:10:53, 20.25s/it] 85%|████████▌ | 1217/1426 [6:27:50<1:09:53, 20.06s/it]                                                       {'loss': 0.672, 'learning_rate': 1.1059747156581636e-06, 'epoch': 0.85}
 85%|████████▌ | 1217/1426 [6:27:50<1:09:53, 20.06s/it] 85%|████████▌ | 1218/1426 [6:28:08<1:07:17, 19.41s/it]                                                       {'loss': 0.6783, 'learning_rate': 1.09561372266883e-06, 'epoch': 0.85}
 85%|████████▌ | 1218/1426 [6:28:08<1:07:17, 19.41s/it]this iter is wrong in something... skip...
 85%|████████▌ | 1219/1426 [6:28:27<1:07:07, 19.46s/it]                                                       {'loss': 0.6703, 'learning_rate': 1.0852986768942308e-06, 'epoch': 0.85}
 85%|████████▌ | 1219/1426 [6:28:27<1:07:07, 19.46s/it] 86%|████████▌ | 1220/1426 [6:28:46<1:05:39, 19.13s/it]                                                       {'loss': 0.6542, 'learning_rate': 1.0750296315606768e-06, 'epoch': 0.86}
 86%|████████▌ | 1220/1426 [6:28:46<1:05:39, 19.13s/it] 86%|████████▌ | 1221/1426 [6:29:06<1:06:30, 19.47s/it]                                                       {'loss': 0.6637, 'learning_rate': 1.0648066396571089e-06, 'epoch': 0.86}
 86%|████████▌ | 1221/1426 [6:29:06<1:06:30, 19.47s/it] 86%|████████▌ | 1222/1426 [6:29:26<1:07:04, 19.73s/it]                                                       {'loss': 0.6611, 'learning_rate': 1.054629753934837e-06, 'epoch': 0.86}
 86%|████████▌ | 1222/1426 [6:29:26<1:07:04, 19.73s/it] 86%|████████▌ | 1223/1426 [6:29:44<1:04:26, 19.05s/it]                                                       {'loss': 0.6743, 'learning_rate': 1.0444990269072542e-06, 'epoch': 0.86}
 86%|████████▌ | 1223/1426 [6:29:44<1:04:26, 19.05s/it] 86%|████████▌ | 1224/1426 [6:30:04<1:04:55, 19.28s/it]                                                       {'loss': 0.6668, 'learning_rate': 1.0344145108495718e-06, 'epoch': 0.86}
 86%|████████▌ | 1224/1426 [6:30:04<1:04:55, 19.28s/it] 86%|████████▌ | 1225/1426 [6:30:23<1:04:43, 19.32s/it]                                                       {'loss': 0.6614, 'learning_rate': 1.0243762577985516e-06, 'epoch': 0.86}
 86%|████████▌ | 1225/1426 [6:30:23<1:04:43, 19.32s/it] 86%|████████▌ | 1226/1426 [6:30:41<1:02:40, 18.80s/it]                                                       {'loss': 0.659, 'learning_rate': 1.0143843195522396e-06, 'epoch': 0.86}
 86%|████████▌ | 1226/1426 [6:30:41<1:02:40, 18.80s/it] 86%|████████▌ | 1227/1426 [6:31:01<1:03:30, 19.15s/it]                                                       {'loss': 0.6837, 'learning_rate': 1.004438747669686e-06, 'epoch': 0.86}
 86%|████████▌ | 1227/1426 [6:31:01<1:03:30, 19.15s/it] 86%|████████▌ | 1228/1426 [6:31:18<1:01:54, 18.76s/it]                                                       {'loss': 0.6476, 'learning_rate': 9.945395934706891e-07, 'epoch': 0.86}
 86%|████████▌ | 1228/1426 [6:31:18<1:01:54, 18.76s/it]this iter is wrong in something... skip...
 86%|████████▌ | 1229/1426 [6:31:38<1:02:28, 19.03s/it]                                                       {'loss': 0.6439, 'learning_rate': 9.846869080355358e-07, 'epoch': 0.86}
 86%|████████▌ | 1229/1426 [6:31:38<1:02:28, 19.03s/it] 86%|████████▋ | 1230/1426 [6:32:03<1:07:36, 20.70s/it]                                                       {'loss': 0.661, 'learning_rate': 9.748807422047224e-07, 'epoch': 0.86}
 86%|████████▋ | 1230/1426 [6:32:03<1:07:36, 20.70s/it] 86%|████████▋ | 1231/1426 [6:32:24<1:08:11, 20.98s/it]                                                       {'loss': 0.6814, 'learning_rate': 9.651211465787091e-07, 'epoch': 0.86}
 86%|████████▋ | 1231/1426 [6:32:24<1:08:11, 20.98s/it] 86%|████████▋ | 1232/1426 [6:32:42<1:04:47, 20.04s/it]                                                       {'loss': 0.6607, 'learning_rate': 9.554081715176444e-07, 'epoch': 0.86}
 86%|████████▋ | 1232/1426 [6:32:42<1:04:47, 20.04s/it] 86%|████████▋ | 1233/1426 [6:33:01<1:03:24, 19.71s/it]                                                       {'loss': 0.69, 'learning_rate': 9.457418671411122e-07, 'epoch': 0.86}
 86%|████████▋ | 1233/1426 [6:33:01<1:03:24, 19.71s/it] 87%|████████▋ | 1234/1426 [6:33:22<1:04:27, 20.14s/it]                                                       {'loss': 0.6578, 'learning_rate': 9.36122283327876e-07, 'epoch': 0.87}
 87%|████████▋ | 1234/1426 [6:33:22<1:04:27, 20.14s/it] 87%|████████▋ | 1235/1426 [6:33:42<1:03:39, 20.00s/it]                                                       {'loss': 0.6733, 'learning_rate': 9.265494697156186e-07, 'epoch': 0.87}
 87%|████████▋ | 1235/1426 [6:33:42<1:03:39, 20.00s/it] 87%|████████▋ | 1236/1426 [6:34:04<1:05:46, 20.77s/it]                                                       {'loss': 0.6735, 'learning_rate': 9.170234757006824e-07, 'epoch': 0.87}
 87%|████████▋ | 1236/1426 [6:34:04<1:05:46, 20.77s/it] 87%|████████▋ | 1237/1426 [6:34:23<1:02:56, 19.98s/it]                                                       {'loss': 0.6655, 'learning_rate': 9.075443504378156e-07, 'epoch': 0.87}
 87%|████████▋ | 1237/1426 [6:34:23<1:02:56, 19.98s/it] 87%|████████▋ | 1238/1426 [6:34:45<1:04:38, 20.63s/it]                                                       {'loss': 0.6686, 'learning_rate': 8.981121428399275e-07, 'epoch': 0.87}
 87%|████████▋ | 1238/1426 [6:34:45<1:04:38, 20.63s/it] 87%|████████▋ | 1239/1426 [6:35:03<1:02:05, 19.92s/it]                                                       {'loss': 0.6673, 'learning_rate': 8.88726901577821e-07, 'epoch': 0.87}
 87%|████████▋ | 1239/1426 [6:35:03<1:02:05, 19.92s/it] 87%|████████▋ | 1240/1426 [6:35:24<1:03:09, 20.37s/it]                                                       {'loss': 0.6545, 'learning_rate': 8.793886750799596e-07, 'epoch': 0.87}
 87%|████████▋ | 1240/1426 [6:35:24<1:03:09, 20.37s/it] 87%|████████▋ | 1241/1426 [6:35:44<1:02:25, 20.25s/it]                                                       {'loss': 0.6731, 'learning_rate': 8.700975115321985e-07, 'epoch': 0.87}
 87%|████████▋ | 1241/1426 [6:35:44<1:02:25, 20.25s/it] 87%|████████▋ | 1242/1426 [6:36:03<1:00:20, 19.68s/it]                                                       {'loss': 0.658, 'learning_rate': 8.608534588775474e-07, 'epoch': 0.87}
 87%|████████▋ | 1242/1426 [6:36:03<1:00:20, 19.68s/it] 87%|████████▋ | 1243/1426 [6:36:25<1:01:59, 20.32s/it]                                                       {'loss': 0.6661, 'learning_rate': 8.516565648159237e-07, 'epoch': 0.87}
 87%|████████▋ | 1243/1426 [6:36:25<1:01:59, 20.32s/it] 87%|████████▋ | 1244/1426 [6:36:45<1:01:31, 20.28s/it]                                                       {'loss': 0.6602, 'learning_rate': 8.425068768039024e-07, 'epoch': 0.87}
 87%|████████▋ | 1244/1426 [6:36:45<1:01:31, 20.28s/it] 87%|████████▋ | 1245/1426 [6:37:03<59:03, 19.58s/it]                                                       {'loss': 0.6782, 'learning_rate': 8.334044420544696e-07, 'epoch': 0.87}
 87%|████████▋ | 1245/1426 [6:37:03<59:03, 19.58s/it] 87%|████████▋ | 1246/1426 [6:37:24<1:00:02, 20.01s/it]                                                       {'loss': 0.64, 'learning_rate': 8.243493075367814e-07, 'epoch': 0.87}
 87%|████████▋ | 1246/1426 [6:37:24<1:00:02, 20.01s/it] 87%|████████▋ | 1247/1426 [6:37:43<59:24, 19.91s/it]                                                       {'loss': 0.6484, 'learning_rate': 8.153415199759263e-07, 'epoch': 0.87}
 87%|████████▋ | 1247/1426 [6:37:43<59:24, 19.91s/it]this iter is wrong in something... skip...
 88%|████████▊ | 1248/1426 [6:38:01<57:21, 19.33s/it]                                                     {'loss': 0.6684, 'learning_rate': 8.063811258526743e-07, 'epoch': 0.87}
 88%|████████▊ | 1248/1426 [6:38:01<57:21, 19.33s/it] 88%|████████▊ | 1249/1426 [6:38:20<56:44, 19.24s/it]                                                     {'loss': 0.6626, 'learning_rate': 7.974681714032451e-07, 'epoch': 0.88}
 88%|████████▊ | 1249/1426 [6:38:20<56:44, 19.24s/it] 88%|████████▊ | 1250/1426 [6:38:42<58:24, 19.91s/it]                                                     {'loss': 0.679, 'learning_rate': 7.88602702619069e-07, 'epoch': 0.88}
 88%|████████▊ | 1250/1426 [6:38:42<58:24, 19.91s/it] 88%|████████▊ | 1251/1426 [6:39:04<59:53, 20.54s/it]                                                     {'loss': 0.6475, 'learning_rate': 7.797847652465373e-07, 'epoch': 0.88}
 88%|████████▊ | 1251/1426 [6:39:04<59:53, 20.54s/it] 88%|████████▊ | 1252/1426 [6:39:23<58:26, 20.15s/it]                                                     {'loss': 0.6573, 'learning_rate': 7.710144047867862e-07, 'epoch': 0.88}
 88%|████████▊ | 1252/1426 [6:39:23<58:26, 20.15s/it] 88%|████████▊ | 1253/1426 [6:39:43<58:06, 20.15s/it]                                                     {'loss': 0.6769, 'learning_rate': 7.622916664954505e-07, 'epoch': 0.88}
 88%|████████▊ | 1253/1426 [6:39:43<58:06, 20.15s/it] 88%|████████▊ | 1254/1426 [6:40:03<57:18, 19.99s/it]                                                     {'loss': 0.6683, 'learning_rate': 7.536165953824281e-07, 'epoch': 0.88}
 88%|████████▊ | 1254/1426 [6:40:03<57:18, 19.99s/it] 88%|████████▊ | 1255/1426 [6:40:23<57:10, 20.06s/it]                                                     {'loss': 0.6724, 'learning_rate': 7.449892362116484e-07, 'epoch': 0.88}
 88%|████████▊ | 1255/1426 [6:40:23<57:10, 20.06s/it] 88%|████████▊ | 1256/1426 [6:40:43<56:34, 19.97s/it]                                                     {'loss': 0.6544, 'learning_rate': 7.364096335008553e-07, 'epoch': 0.88}
 88%|████████▊ | 1256/1426 [6:40:43<56:34, 19.97s/it] 88%|████████▊ | 1257/1426 [6:41:01<54:43, 19.43s/it]                                                     {'loss': 0.6788, 'learning_rate': 7.27877831521352e-07, 'epoch': 0.88}
 88%|████████▊ | 1257/1426 [6:41:01<54:43, 19.43s/it] 88%|████████▊ | 1258/1426 [6:41:20<54:16, 19.38s/it]                                                     {'loss': 0.6646, 'learning_rate': 7.193938742977991e-07, 'epoch': 0.88}
 88%|████████▊ | 1258/1426 [6:41:20<54:16, 19.38s/it] 88%|████████▊ | 1259/1426 [6:41:40<54:08, 19.45s/it]                                                     {'loss': 0.6662, 'learning_rate': 7.109578056079703e-07, 'epoch': 0.88}
 88%|████████▊ | 1259/1426 [6:41:40<54:08, 19.45s/it] 88%|████████▊ | 1260/1426 [6:42:00<54:19, 19.64s/it]                                                     {'loss': 0.6773, 'learning_rate': 7.025696689825256e-07, 'epoch': 0.88}
 88%|████████▊ | 1260/1426 [6:42:00<54:19, 19.64s/it] 88%|████████▊ | 1261/1426 [6:42:19<53:22, 19.41s/it]                                                     {'loss': 0.6403, 'learning_rate': 6.942295077048011e-07, 'epoch': 0.88}
 88%|████████▊ | 1261/1426 [6:42:19<53:22, 19.41s/it]this iter is wrong in something... skip...
 88%|████████▊ | 1262/1426 [6:42:37<52:10, 19.09s/it]                                                     {'loss': 0.6414, 'learning_rate': 6.85937364810576e-07, 'epoch': 0.88}
 88%|████████▊ | 1262/1426 [6:42:37<52:10, 19.09s/it] 89%|████████▊ | 1263/1426 [6:42:57<52:37, 19.37s/it]                                                     {'loss': 0.6653, 'learning_rate': 6.776932830878469e-07, 'epoch': 0.89}
 89%|████████▊ | 1263/1426 [6:42:57<52:37, 19.37s/it] 89%|████████▊ | 1264/1426 [6:43:15<51:02, 18.91s/it]                                                     {'loss': 0.6927, 'learning_rate': 6.694973050766118e-07, 'epoch': 0.89}
 89%|████████▊ | 1264/1426 [6:43:15<51:02, 18.91s/it] 89%|████████▊ | 1265/1426 [6:43:34<50:47, 18.93s/it]                                                     {'loss': 0.6696, 'learning_rate': 6.613494730686565e-07, 'epoch': 0.89}
 89%|████████▊ | 1265/1426 [6:43:34<50:47, 18.93s/it] 89%|████████▉ | 1266/1426 [6:43:52<50:02, 18.77s/it]                                                     {'loss': 0.6292, 'learning_rate': 6.532498291073231e-07, 'epoch': 0.89}
 89%|████████▉ | 1266/1426 [6:43:52<50:02, 18.77s/it] 89%|████████▉ | 1267/1426 [6:44:12<50:25, 19.03s/it]                                                     {'loss': 0.6363, 'learning_rate': 6.451984149873047e-07, 'epoch': 0.89}
 89%|████████▉ | 1267/1426 [6:44:12<50:25, 19.03s/it] 89%|████████▉ | 1268/1426 [6:44:31<49:42, 18.88s/it]                                                     {'loss': 0.655, 'learning_rate': 6.37195272254425e-07, 'epoch': 0.89}
 89%|████████▉ | 1268/1426 [6:44:31<49:42, 18.88s/it] 89%|████████▉ | 1269/1426 [6:44:53<51:50, 19.81s/it]                                                     {'loss': 0.6384, 'learning_rate': 6.292404422054233e-07, 'epoch': 0.89}
 89%|████████▉ | 1269/1426 [6:44:53<51:50, 19.81s/it] 89%|████████▉ | 1270/1426 [6:45:10<49:53, 19.19s/it]                                                     {'loss': 0.672, 'learning_rate': 6.213339658877393e-07, 'epoch': 0.89}
 89%|████████▉ | 1270/1426 [6:45:10<49:53, 19.19s/it] 89%|████████▉ | 1271/1426 [6:45:30<49:51, 19.30s/it]                                                     {'loss': 0.6482, 'learning_rate': 6.134758840993093e-07, 'epoch': 0.89}
 89%|████████▉ | 1271/1426 [6:45:30<49:51, 19.30s/it] 89%|████████▉ | 1272/1426 [6:45:50<49:57, 19.46s/it]                                                     {'loss': 0.6752, 'learning_rate': 6.056662373883482e-07, 'epoch': 0.89}
 89%|████████▉ | 1272/1426 [6:45:50<49:57, 19.46s/it] 89%|████████▉ | 1273/1426 [6:46:10<50:34, 19.83s/it]                                                     {'loss': 0.6625, 'learning_rate': 5.979050660531438e-07, 'epoch': 0.89}
 89%|████████▉ | 1273/1426 [6:46:10<50:34, 19.83s/it] 89%|████████▉ | 1274/1426 [6:46:31<50:40, 20.00s/it]                                                     {'loss': 0.6923, 'learning_rate': 5.901924101418465e-07, 'epoch': 0.89}
 89%|████████▉ | 1274/1426 [6:46:31<50:40, 20.00s/it] 89%|████████▉ | 1275/1426 [6:46:49<49:14, 19.57s/it]                                                     {'loss': 0.6643, 'learning_rate': 5.825283094522627e-07, 'epoch': 0.89}
 89%|████████▉ | 1275/1426 [6:46:49<49:14, 19.57s/it] 89%|████████▉ | 1276/1426 [6:47:08<47:56, 19.18s/it]                                                     {'loss': 0.6746, 'learning_rate': 5.749128035316553e-07, 'epoch': 0.89}
 89%|████████▉ | 1276/1426 [6:47:08<47:56, 19.18s/it] 90%|████████▉ | 1277/1426 [6:47:27<48:01, 19.34s/it]                                                     {'loss': 0.6459, 'learning_rate': 5.673459316765317e-07, 'epoch': 0.9}
 90%|████████▉ | 1277/1426 [6:47:27<48:01, 19.34s/it] 90%|████████▉ | 1278/1426 [6:47:47<48:08, 19.52s/it]                                                     {'loss': 0.6598, 'learning_rate': 5.598277329324442e-07, 'epoch': 0.9}
 90%|████████▉ | 1278/1426 [6:47:47<48:08, 19.52s/it] 90%|████████▉ | 1279/1426 [6:48:06<47:08, 19.24s/it]                                                     {'loss': 0.646, 'learning_rate': 5.52358246093787e-07, 'epoch': 0.9}
 90%|████████▉ | 1279/1426 [6:48:06<47:08, 19.24s/it] 90%|████████▉ | 1280/1426 [6:48:23<45:34, 18.73s/it]                                                     {'loss': 0.675, 'learning_rate': 5.449375097036058e-07, 'epoch': 0.9}
 90%|████████▉ | 1280/1426 [6:48:23<45:34, 18.73s/it] 90%|████████▉ | 1281/1426 [6:48:41<44:35, 18.46s/it]                                                     {'loss': 0.6651, 'learning_rate': 5.375655620533793e-07, 'epoch': 0.9}
 90%|████████▉ | 1281/1426 [6:48:41<44:35, 18.46s/it] 90%|████████▉ | 1282/1426 [6:49:03<46:45, 19.49s/it]                                                     {'loss': 0.6578, 'learning_rate': 5.302424411828421e-07, 'epoch': 0.9}
 90%|████████▉ | 1282/1426 [6:49:03<46:45, 19.49s/it]this iter is wrong in something... skip...
 90%|████████▉ | 1283/1426 [6:49:21<45:15, 18.99s/it]                                                     {'loss': 0.6689, 'learning_rate': 5.229681848797752e-07, 'epoch': 0.9}
 90%|████████▉ | 1283/1426 [6:49:21<45:15, 18.99s/it] 90%|█████████ | 1284/1426 [6:49:35<41:36, 17.58s/it]                                                     {'loss': 0.2381, 'learning_rate': 5.157428306798162e-07, 'epoch': 0.9}
 90%|█████████ | 1284/1426 [6:49:35<41:36, 17.58s/it] 90%|█████████ | 1285/1426 [6:49:54<42:25, 18.06s/it]                                                     {'loss': 0.6522, 'learning_rate': 5.085664158662628e-07, 'epoch': 0.9}
 90%|█████████ | 1285/1426 [6:49:54<42:25, 18.06s/it] 90%|█████████ | 1286/1426 [6:50:12<41:55, 17.97s/it]                                                     {'loss': 0.6531, 'learning_rate': 5.014389774698891e-07, 'epoch': 0.9}
 90%|█████████ | 1286/1426 [6:50:12<41:55, 17.97s/it] 90%|█████████ | 1287/1426 [6:50:32<43:06, 18.61s/it]                                                     {'loss': 0.6615, 'learning_rate': 4.943605522687389e-07, 'epoch': 0.9}
 90%|█████████ | 1287/1426 [6:50:32<43:06, 18.61s/it] 90%|█████████ | 1288/1426 [6:50:51<43:05, 18.74s/it]                                                     {'loss': 0.664, 'learning_rate': 4.873311767879474e-07, 'epoch': 0.9}
 90%|█████████ | 1288/1426 [6:50:51<43:05, 18.74s/it] 90%|█████████ | 1289/1426 [6:51:05<39:33, 17.33s/it]                                                     {'loss': 0.2384, 'learning_rate': 4.803508872995554e-07, 'epoch': 0.9}
 90%|█████████ | 1289/1426 [6:51:05<39:33, 17.33s/it] 90%|█████████ | 1290/1426 [6:51:25<40:37, 17.92s/it]                                                     {'loss': 0.6698, 'learning_rate': 4.7341971982230784e-07, 'epoch': 0.9}
 90%|█████████ | 1290/1426 [6:51:25<40:37, 17.92s/it] 91%|█████████ | 1291/1426 [6:51:44<41:36, 18.49s/it]                                                     {'loss': 0.6488, 'learning_rate': 4.665377101214863e-07, 'epoch': 0.91}
 91%|█████████ | 1291/1426 [6:51:44<41:36, 18.49s/it] 91%|█████████ | 1292/1426 [6:51:59<38:22, 17.18s/it]                                                     {'loss': 0.2503, 'learning_rate': 4.597048937087056e-07, 'epoch': 0.91}
 91%|█████████ | 1292/1426 [6:51:59<38:22, 17.18s/it] 91%|█████████ | 1293/1426 [6:52:19<39:55, 18.01s/it]                                                     {'loss': 0.6466, 'learning_rate': 4.529213058417481e-07, 'epoch': 0.91}
 91%|█████████ | 1293/1426 [6:52:19<39:55, 18.01s/it] 91%|█████████ | 1294/1426 [6:52:38<40:42, 18.51s/it]                                                     {'loss': 0.6597, 'learning_rate': 4.4618698152436915e-07, 'epoch': 0.91}
 91%|█████████ | 1294/1426 [6:52:38<40:42, 18.51s/it] 91%|█████████ | 1295/1426 [6:52:57<40:47, 18.68s/it]                                                     {'loss': 0.6629, 'learning_rate': 4.3950195550612197e-07, 'epoch': 0.91}
 91%|█████████ | 1295/1426 [6:52:57<40:47, 18.68s/it] 91%|█████████ | 1296/1426 [6:53:14<39:17, 18.13s/it]                                                     {'loss': 0.6693, 'learning_rate': 4.328662622821778e-07, 'epoch': 0.91}
 91%|█████████ | 1296/1426 [6:53:14<39:17, 18.13s/it] 91%|█████████ | 1297/1426 [6:53:34<39:51, 18.54s/it]                                                     {'loss': 0.6955, 'learning_rate': 4.262799360931458e-07, 'epoch': 0.91}
 91%|█████████ | 1297/1426 [6:53:34<39:51, 18.54s/it] 91%|█████████ | 1298/1426 [6:53:53<40:19, 18.91s/it]                                                     {'loss': 0.6658, 'learning_rate': 4.197430109249012e-07, 'epoch': 0.91}
 91%|█████████ | 1298/1426 [6:53:53<40:19, 18.91s/it] 91%|█████████ | 1299/1426 [6:54:15<41:49, 19.76s/it]                                                     {'loss': 0.6553, 'learning_rate': 4.1325552050840213e-07, 'epoch': 0.91}
 91%|█████████ | 1299/1426 [6:54:15<41:49, 19.76s/it] 91%|█████████ | 1300/1426 [6:54:36<41:58, 19.99s/it]                                                     {'loss': 0.6569, 'learning_rate': 4.068174983195261e-07, 'epoch': 0.91}
 91%|█████████ | 1300/1426 [6:54:36<41:58, 19.99s/it] 91%|█████████ | 1301/1426 [6:54:55<41:07, 19.74s/it]                                                     {'loss': 0.6569, 'learning_rate': 4.00428977578885e-07, 'epoch': 0.91}
 91%|█████████ | 1301/1426 [6:54:55<41:07, 19.74s/it] 91%|█████████▏| 1302/1426 [6:55:19<43:41, 21.14s/it]                                                     {'loss': 0.6672, 'learning_rate': 3.94089991251666e-07, 'epoch': 0.91}
 91%|█████████▏| 1302/1426 [6:55:19<43:41, 21.14s/it] 91%|█████████▏| 1303/1426 [6:55:39<42:30, 20.73s/it]                                                     {'loss': 0.6806, 'learning_rate': 3.878005720474498e-07, 'epoch': 0.91}
 91%|█████████▏| 1303/1426 [6:55:39<42:30, 20.73s/it] 91%|█████████▏| 1304/1426 [6:55:59<41:55, 20.62s/it]                                                     {'loss': 0.6636, 'learning_rate': 3.8156075242005265e-07, 'epoch': 0.91}
 91%|█████████▏| 1304/1426 [6:55:59<41:55, 20.62s/it] 92%|█████████▏| 1305/1426 [6:56:20<41:43, 20.69s/it]                                                     {'loss': 0.6743, 'learning_rate': 3.7537056456735e-07, 'epoch': 0.91}
 92%|█████████▏| 1305/1426 [6:56:20<41:43, 20.69s/it] 92%|█████████▏| 1306/1426 [6:56:39<40:20, 20.17s/it]                                                     {'loss': 0.6535, 'learning_rate': 3.6923004043111443e-07, 'epoch': 0.92}
 92%|█████████▏| 1306/1426 [6:56:39<40:20, 20.17s/it] 92%|█████████▏| 1307/1426 [6:57:01<40:44, 20.54s/it]                                                     {'loss': 0.6849, 'learning_rate': 3.6313921169685353e-07, 'epoch': 0.92}
 92%|█████████▏| 1307/1426 [6:57:01<40:44, 20.54s/it] 92%|█████████▏| 1308/1426 [6:57:20<39:38, 20.16s/it]                                                     {'loss': 0.6644, 'learning_rate': 3.570981097936399e-07, 'epoch': 0.92}
 92%|█████████▏| 1308/1426 [6:57:20<39:38, 20.16s/it] 92%|█████████▏| 1309/1426 [6:57:40<39:21, 20.18s/it]                                                     {'loss': 0.6489, 'learning_rate': 3.5110676589395597e-07, 'epoch': 0.92}
 92%|█████████▏| 1309/1426 [6:57:40<39:21, 20.18s/it] 92%|█████████▏| 1310/1426 [6:58:02<39:44, 20.55s/it]                                                     {'loss': 0.6561, 'learning_rate': 3.45165210913524e-07, 'epoch': 0.92}
 92%|█████████▏| 1310/1426 [6:58:02<39:44, 20.55s/it] 92%|█████████▏| 1311/1426 [6:58:20<38:02, 19.85s/it]                                                     {'loss': 0.6775, 'learning_rate': 3.392734755111604e-07, 'epoch': 0.92}
 92%|█████████▏| 1311/1426 [6:58:20<38:02, 19.85s/it] 92%|█████████▏| 1312/1426 [6:58:37<36:14, 19.08s/it]                                                     {'loss': 0.673, 'learning_rate': 3.3343159008859984e-07, 'epoch': 0.92}
 92%|█████████▏| 1312/1426 [6:58:37<36:14, 19.08s/it] 92%|█████████▏| 1313/1426 [6:58:54<34:52, 18.52s/it]                                                     {'loss': 0.6745, 'learning_rate': 3.276395847903568e-07, 'epoch': 0.92}
 92%|█████████▏| 1313/1426 [6:58:54<34:52, 18.52s/it] 92%|█████████▏| 1314/1426 [6:59:12<34:12, 18.33s/it]                                                     {'loss': 0.6388, 'learning_rate': 3.2189748950355425e-07, 'epoch': 0.92}
 92%|█████████▏| 1314/1426 [6:59:12<34:12, 18.33s/it] 92%|█████████▏| 1315/1426 [6:59:32<34:35, 18.70s/it]                                                     {'loss': 0.6561, 'learning_rate': 3.1620533385777863e-07, 'epoch': 0.92}
 92%|█████████▏| 1315/1426 [6:59:32<34:35, 18.70s/it] 92%|█████████▏| 1316/1426 [6:59:50<34:17, 18.70s/it]                                                     {'loss': 0.6864, 'learning_rate': 3.105631472249282e-07, 'epoch': 0.92}
 92%|█████████▏| 1316/1426 [6:59:50<34:17, 18.70s/it] 92%|█████████▏| 1317/1426 [7:00:11<34:46, 19.14s/it]                                                     {'loss': 0.6869, 'learning_rate': 3.049709587190497e-07, 'epoch': 0.92}
 92%|█████████▏| 1317/1426 [7:00:11<34:46, 19.14s/it] 92%|█████████▏| 1318/1426 [7:00:25<31:43, 17.63s/it]                                                     {'loss': 0.2283, 'learning_rate': 2.9942879719620396e-07, 'epoch': 0.92}
 92%|█████████▏| 1318/1426 [7:00:25<31:43, 17.63s/it] 92%|█████████▏| 1319/1426 [7:00:43<31:55, 17.90s/it]                                                     {'loss': 0.6627, 'learning_rate': 2.9393669125430625e-07, 'epoch': 0.92}
 92%|█████████▏| 1319/1426 [7:00:43<31:55, 17.90s/it] 93%|█████████▎| 1320/1426 [7:01:01<31:32, 17.86s/it]                                                     {'loss': 0.6706, 'learning_rate': 2.884946692329826e-07, 'epoch': 0.93}
 93%|█████████▎| 1320/1426 [7:01:01<31:32, 17.86s/it] 93%|█████████▎| 1321/1426 [7:01:19<31:20, 17.91s/it]                                                     {'loss': 0.6911, 'learning_rate': 2.8310275921341947e-07, 'epoch': 0.93}
 93%|█████████▎| 1321/1426 [7:01:19<31:20, 17.91s/it] 93%|█████████▎| 1322/1426 [7:01:39<32:11, 18.57s/it]                                                     {'loss': 0.6691, 'learning_rate': 2.7776098901822646e-07, 'epoch': 0.93}
 93%|█████████▎| 1322/1426 [7:01:39<32:11, 18.57s/it] 93%|█████████▎| 1323/1426 [7:02:00<33:13, 19.36s/it]                                                     {'loss': 0.6485, 'learning_rate': 2.7246938621128813e-07, 'epoch': 0.93}
 93%|█████████▎| 1323/1426 [7:02:00<33:13, 19.36s/it] 93%|█████████▎| 1324/1426 [7:02:20<33:18, 19.59s/it]                                                     {'loss': 0.6566, 'learning_rate': 2.672279780976161e-07, 'epoch': 0.93}
 93%|█████████▎| 1324/1426 [7:02:20<33:18, 19.59s/it] 93%|█████████▎| 1325/1426 [7:02:39<32:22, 19.23s/it]                                                     {'loss': 0.6557, 'learning_rate': 2.620367917232225e-07, 'epoch': 0.93}
 93%|█████████▎| 1325/1426 [7:02:39<32:22, 19.23s/it] 93%|█████████▎| 1326/1426 [7:02:56<31:04, 18.64s/it]                                                     {'loss': 0.6701, 'learning_rate': 2.5689585387496775e-07, 'epoch': 0.93}
 93%|█████████▎| 1326/1426 [7:02:56<31:04, 18.64s/it] 93%|█████████▎| 1327/1426 [7:03:17<31:46, 19.26s/it]                                                     {'loss': 0.6742, 'learning_rate': 2.5180519108042424e-07, 'epoch': 0.93}
 93%|█████████▎| 1327/1426 [7:03:17<31:46, 19.26s/it] 93%|█████████▎| 1328/1426 [7:03:35<31:00, 18.98s/it]                                                     {'loss': 0.6532, 'learning_rate': 2.4676482960774737e-07, 'epoch': 0.93}
 93%|█████████▎| 1328/1426 [7:03:35<31:00, 18.98s/it] 93%|█████████▎| 1329/1426 [7:03:57<31:51, 19.71s/it]                                                     {'loss': 0.6936, 'learning_rate': 2.417747954655314e-07, 'epoch': 0.93}
 93%|█████████▎| 1329/1426 [7:03:57<31:51, 19.71s/it] 93%|█████████▎| 1330/1426 [7:04:14<30:31, 19.07s/it]                                                     {'loss': 0.6562, 'learning_rate': 2.3683511440267592e-07, 'epoch': 0.93}
 93%|█████████▎| 1330/1426 [7:04:14<30:31, 19.07s/it] 93%|█████████▎| 1331/1426 [7:04:30<28:37, 18.08s/it]                                                     {'loss': 0.2349, 'learning_rate': 2.319458119082596e-07, 'epoch': 0.93}
 93%|█████████▎| 1331/1426 [7:04:30<28:37, 18.08s/it] 93%|█████████▎| 1332/1426 [7:04:49<28:54, 18.45s/it]                                                     {'loss': 0.6504, 'learning_rate': 2.2710691321140343e-07, 'epoch': 0.93}
 93%|█████████▎| 1332/1426 [7:04:49<28:54, 18.45s/it] 93%|█████████▎| 1333/1426 [7:05:08<28:57, 18.68s/it]                                                     {'loss': 0.6782, 'learning_rate': 2.223184432811376e-07, 'epoch': 0.93}
 93%|█████████▎| 1333/1426 [7:05:08<28:57, 18.68s/it] 94%|█████████▎| 1334/1426 [7:05:26<28:06, 18.33s/it]                                                     {'loss': 0.6525, 'learning_rate': 2.1758042682628154e-07, 'epoch': 0.94}
 94%|█████████▎| 1334/1426 [7:05:26<28:06, 18.33s/it] 94%|█████████▎| 1335/1426 [7:05:47<29:10, 19.24s/it]                                                     {'loss': 0.6536, 'learning_rate': 2.1289288829531186e-07, 'epoch': 0.94}
 94%|█████████▎| 1335/1426 [7:05:47<29:10, 19.24s/it] 94%|█████████▎| 1336/1426 [7:06:06<28:37, 19.09s/it]                                                     {'loss': 0.653, 'learning_rate': 2.082558518762301e-07, 'epoch': 0.94}
 94%|█████████▎| 1336/1426 [7:06:06<28:37, 19.09s/it] 94%|█████████▍| 1337/1426 [7:06:26<28:40, 19.33s/it]                                                     {'loss': 0.6436, 'learning_rate': 2.0366934149644857e-07, 'epoch': 0.94}
 94%|█████████▍| 1337/1426 [7:06:26<28:40, 19.33s/it] 94%|█████████▍| 1338/1426 [7:06:46<28:48, 19.65s/it]                                                     {'loss': 0.6873, 'learning_rate': 1.991333808226603e-07, 'epoch': 0.94}
 94%|█████████▍| 1338/1426 [7:06:46<28:48, 19.65s/it] 94%|█████████▍| 1339/1426 [7:07:07<28:51, 19.90s/it]                                                     {'loss': 0.6738, 'learning_rate': 1.9464799326071816e-07, 'epoch': 0.94}
 94%|█████████▍| 1339/1426 [7:07:07<28:51, 19.90s/it] 94%|█████████▍| 1340/1426 [7:07:26<28:25, 19.83s/it]                                                     {'loss': 0.6489, 'learning_rate': 1.9021320195551584e-07, 'epoch': 0.94}
 94%|█████████▍| 1340/1426 [7:07:26<28:25, 19.83s/it] 94%|█████████▍| 1341/1426 [7:07:45<27:22, 19.32s/it]                                                     {'loss': 0.6672, 'learning_rate': 1.858290297908649e-07, 'epoch': 0.94}
 94%|█████████▍| 1341/1426 [7:07:45<27:22, 19.32s/it] 94%|█████████▍| 1342/1426 [7:08:05<27:34, 19.69s/it]                                                     {'loss': 0.6343, 'learning_rate': 1.8149549938938026e-07, 'epoch': 0.94}
 94%|█████████▍| 1342/1426 [7:08:05<27:34, 19.69s/it] 94%|█████████▍| 1343/1426 [7:08:29<28:48, 20.82s/it]                                                     {'loss': 0.6847, 'learning_rate': 1.7721263311236247e-07, 'epoch': 0.94}
 94%|█████████▍| 1343/1426 [7:08:29<28:48, 20.82s/it] 94%|█████████▍| 1344/1426 [7:08:48<27:55, 20.43s/it]                                                     {'loss': 0.6658, 'learning_rate': 1.7298045305968126e-07, 'epoch': 0.94}
 94%|█████████▍| 1344/1426 [7:08:48<27:55, 20.43s/it] 94%|█████████▍| 1345/1426 [7:09:09<27:33, 20.41s/it]                                                     {'loss': 0.6715, 'learning_rate': 1.687989810696611e-07, 'epoch': 0.94}
 94%|█████████▍| 1345/1426 [7:09:09<27:33, 20.41s/it] 94%|█████████▍| 1346/1426 [7:09:28<26:51, 20.15s/it]                                                     {'loss': 0.6677, 'learning_rate': 1.646682387189713e-07, 'epoch': 0.94}
 94%|█████████▍| 1346/1426 [7:09:28<26:51, 20.15s/it] 94%|█████████▍| 1347/1426 [7:09:47<26:08, 19.86s/it]                                                     {'loss': 0.6637, 'learning_rate': 1.6058824732251288e-07, 'epoch': 0.94}
 94%|█████████▍| 1347/1426 [7:09:47<26:08, 19.86s/it] 95%|█████████▍| 1348/1426 [7:10:01<23:32, 18.11s/it]                                                     {'loss': 0.2259, 'learning_rate': 1.5655902793330623e-07, 'epoch': 0.94}
 95%|█████████▍| 1348/1426 [7:10:01<23:32, 18.11s/it] 95%|█████████▍| 1349/1426 [7:10:19<23:11, 18.08s/it]                                                     {'loss': 0.6457, 'learning_rate': 1.5258060134238696e-07, 'epoch': 0.95}
 95%|█████████▍| 1349/1426 [7:10:19<23:11, 18.08s/it] 95%|█████████▍| 1350/1426 [7:10:39<23:32, 18.58s/it]                                                     {'loss': 0.6887, 'learning_rate': 1.48652988078698e-07, 'epoch': 0.95}
 95%|█████████▍| 1350/1426 [7:10:39<23:32, 18.58s/it] 95%|█████████▍| 1351/1426 [7:10:59<23:48, 19.04s/it]                                                     {'loss': 0.6675, 'learning_rate': 1.4477620840897767e-07, 'epoch': 0.95}
 95%|█████████▍| 1351/1426 [7:10:59<23:48, 19.04s/it] 95%|█████████▍| 1352/1426 [7:11:20<24:19, 19.72s/it]                                                     {'loss': 0.6545, 'learning_rate': 1.4095028233766406e-07, 'epoch': 0.95}
 95%|█████████▍| 1352/1426 [7:11:20<24:19, 19.72s/it] 95%|█████████▍| 1353/1426 [7:11:43<24:52, 20.44s/it]                                                     {'loss': 0.6579, 'learning_rate': 1.371752296067852e-07, 'epoch': 0.95}
 95%|█████████▍| 1353/1426 [7:11:43<24:52, 20.44s/it] 95%|█████████▍| 1354/1426 [7:12:02<24:18, 20.26s/it]                                                     {'loss': 0.6742, 'learning_rate': 1.334510696958591e-07, 'epoch': 0.95}
 95%|█████████▍| 1354/1426 [7:12:02<24:18, 20.26s/it] 95%|█████████▌| 1355/1426 [7:12:22<23:45, 20.08s/it]                                                     {'loss': 0.657, 'learning_rate': 1.2977782182179488e-07, 'epoch': 0.95}
 95%|█████████▌| 1355/1426 [7:12:22<23:45, 20.08s/it] 95%|█████████▌| 1356/1426 [7:12:42<23:13, 19.90s/it]                                                     {'loss': 0.6586, 'learning_rate': 1.2615550493879082e-07, 'epoch': 0.95}
 95%|█████████▌| 1356/1426 [7:12:42<23:13, 19.90s/it] 95%|█████████▌| 1357/1426 [7:13:01<22:52, 19.89s/it]                                                     {'loss': 0.6447, 'learning_rate': 1.2258413773823863e-07, 'epoch': 0.95}
 95%|█████████▌| 1357/1426 [7:13:01<22:52, 19.89s/it] 95%|█████████▌| 1358/1426 [7:13:21<22:17, 19.67s/it]                                                     {'loss': 0.6606, 'learning_rate': 1.1906373864862598e-07, 'epoch': 0.95}
 95%|█████████▌| 1358/1426 [7:13:21<22:17, 19.67s/it] 95%|█████████▌| 1359/1426 [7:13:41<22:04, 19.77s/it]                                                     {'loss': 0.6709, 'learning_rate': 1.1559432583544417e-07, 'epoch': 0.95}
 95%|█████████▌| 1359/1426 [7:13:41<22:04, 19.77s/it] 95%|█████████▌| 1360/1426 [7:14:03<22:34, 20.52s/it]                                                     {'loss': 0.6254, 'learning_rate': 1.1217591720108834e-07, 'epoch': 0.95}
 95%|█████████▌| 1360/1426 [7:14:03<22:34, 20.52s/it] 95%|█████████▌| 1361/1426 [7:14:21<21:29, 19.84s/it]                                                     {'loss': 0.6894, 'learning_rate': 1.0880853038476858e-07, 'epoch': 0.95}
 95%|█████████▌| 1361/1426 [7:14:21<21:29, 19.84s/it] 96%|█████████▌| 1362/1426 [7:14:41<21:03, 19.74s/it]                                                     {'loss': 0.645, 'learning_rate': 1.0549218276242446e-07, 'epoch': 0.95}
 96%|█████████▌| 1362/1426 [7:14:41<21:03, 19.74s/it] 96%|█████████▌| 1363/1426 [7:15:00<20:44, 19.75s/it]                                                     {'loss': 0.6641, 'learning_rate': 1.022268914466229e-07, 'epoch': 0.96}
 96%|█████████▌| 1363/1426 [7:15:00<20:44, 19.75s/it] 96%|█████████▌| 1364/1426 [7:15:21<20:35, 19.93s/it]                                                     {'loss': 0.6842, 'learning_rate': 9.901267328648378e-08, 'epoch': 0.96}
 96%|█████████▌| 1364/1426 [7:15:21<20:35, 19.93s/it] 96%|█████████▌| 1365/1426 [7:15:38<19:31, 19.20s/it]                                                     {'loss': 0.6437, 'learning_rate': 9.584954486758002e-08, 'epoch': 0.96}
 96%|█████████▌| 1365/1426 [7:15:38<19:31, 19.20s/it] 96%|█████████▌| 1366/1426 [7:15:59<19:45, 19.76s/it]                                                     {'loss': 0.6558, 'learning_rate': 9.273752251186097e-08, 'epoch': 0.96}
 96%|█████████▌| 1366/1426 [7:15:59<19:45, 19.76s/it] 96%|█████████▌| 1367/1426 [7:16:13<17:43, 18.03s/it]                                                     {'loss': 0.2293, 'learning_rate': 8.96766222775658e-08, 'epoch': 0.96}
 96%|█████████▌| 1367/1426 [7:16:13<17:43, 18.03s/it] 96%|█████████▌| 1368/1426 [7:16:33<18:02, 18.67s/it]                                                     {'loss': 0.6537, 'learning_rate': 8.666685995914026e-08, 'epoch': 0.96}
 96%|█████████▌| 1368/1426 [7:16:33<18:02, 18.67s/it] 96%|█████████▌| 1369/1426 [7:16:53<17:50, 18.78s/it]                                                     {'loss': 0.6731, 'learning_rate': 8.370825108715341e-08, 'epoch': 0.96}
 96%|█████████▌| 1369/1426 [7:16:53<17:50, 18.78s/it] 96%|█████████▌| 1370/1426 [7:17:11<17:28, 18.73s/it]                                                     {'loss': 0.6667, 'learning_rate': 8.080081092822101e-08, 'epoch': 0.96}
 96%|█████████▌| 1370/1426 [7:17:11<17:28, 18.73s/it] 96%|█████████▌| 1371/1426 [7:17:28<16:45, 18.27s/it]                                                     {'loss': 0.6675, 'learning_rate': 7.794455448492444e-08, 'epoch': 0.96}
 96%|█████████▌| 1371/1426 [7:17:28<16:45, 18.27s/it] 96%|█████████▌| 1372/1426 [7:17:46<16:20, 18.15s/it]                                                     {'loss': 0.6678, 'learning_rate': 7.513949649573304e-08, 'epoch': 0.96}
 96%|█████████▌| 1372/1426 [7:17:46<16:20, 18.15s/it] 96%|█████████▋| 1373/1426 [7:18:07<16:50, 19.06s/it]                                                     {'loss': 0.6991, 'learning_rate': 7.238565143492859e-08, 'epoch': 0.96}
 96%|█████████▋| 1373/1426 [7:18:07<16:50, 19.06s/it] 96%|█████████▋| 1374/1426 [7:18:25<16:14, 18.74s/it]                                                     {'loss': 0.6557, 'learning_rate': 6.968303351253203e-08, 'epoch': 0.96}
 96%|█████████▋| 1374/1426 [7:18:25<16:14, 18.74s/it]this iter is wrong in something... skip...
 96%|█████████▋| 1375/1426 [7:18:47<16:44, 19.70s/it]                                                     {'loss': 0.6632, 'learning_rate': 6.703165667422906e-08, 'epoch': 0.96}
 96%|█████████▋| 1375/1426 [7:18:47<16:44, 19.70s/it] 96%|█████████▋| 1376/1426 [7:19:06<16:11, 19.43s/it]                                                     {'loss': 0.6903, 'learning_rate': 6.44315346012958e-08, 'epoch': 0.96}
 96%|█████████▋| 1376/1426 [7:19:06<16:11, 19.43s/it] 97%|█████████▋| 1377/1426 [7:19:24<15:28, 18.95s/it]                                                     {'loss': 0.6773, 'learning_rate': 6.188268071053327e-08, 'epoch': 0.97}
 97%|█████████▋| 1377/1426 [7:19:24<15:28, 18.95s/it]this iter is wrong in something... skip...
 97%|█████████▋| 1378/1426 [7:19:46<15:48, 19.75s/it]                                                     {'loss': 0.656, 'learning_rate': 5.938510815419296e-08, 'epoch': 0.97}
 97%|█████████▋| 1378/1426 [7:19:46<15:48, 19.75s/it] 97%|█████████▋| 1379/1426 [7:20:05<15:28, 19.76s/it]                                                     {'loss': 0.6565, 'learning_rate': 5.693882981991361e-08, 'epoch': 0.97}
 97%|█████████▋| 1379/1426 [7:20:05<15:28, 19.76s/it] 97%|█████████▋| 1380/1426 [7:20:23<14:44, 19.22s/it]                                                     {'loss': 0.6708, 'learning_rate': 5.454385833065012e-08, 'epoch': 0.97}
 97%|█████████▋| 1380/1426 [7:20:23<14:44, 19.22s/it] 97%|█████████▋| 1381/1426 [7:20:41<14:02, 18.72s/it]                                                     {'loss': 0.6427, 'learning_rate': 5.2200206044612514e-08, 'epoch': 0.97}
 97%|█████████▋| 1381/1426 [7:20:41<14:02, 18.72s/it] 97%|█████████▋| 1382/1426 [7:20:59<13:42, 18.70s/it]                                                     {'loss': 0.6441, 'learning_rate': 4.990788505519928e-08, 'epoch': 0.97}
 97%|█████████▋| 1382/1426 [7:21:00<13:42, 18.70s/it] 97%|█████████▋| 1383/1426 [7:21:22<14:13, 19.85s/it]                                                     {'loss': 0.6533, 'learning_rate': 4.766690719093636e-08, 'epoch': 0.97}
 97%|█████████▋| 1383/1426 [7:21:22<14:13, 19.85s/it] 97%|█████████▋| 1384/1426 [7:21:36<12:40, 18.10s/it]                                                     {'loss': 0.2331, 'learning_rate': 4.547728401541607e-08, 'epoch': 0.97}
 97%|█████████▋| 1384/1426 [7:21:36<12:40, 18.10s/it] 97%|█████████▋| 1385/1426 [7:21:54<12:19, 18.04s/it]                                                     {'loss': 0.6642, 'learning_rate': 4.3339026827237116e-08, 'epoch': 0.97}
 97%|█████████▋| 1385/1426 [7:21:54<12:19, 18.04s/it] 97%|█████████▋| 1386/1426 [7:22:15<12:36, 18.91s/it]                                                     {'loss': 0.6649, 'learning_rate': 4.1252146659945814e-08, 'epoch': 0.97}
 97%|█████████▋| 1386/1426 [7:22:15<12:36, 18.91s/it] 97%|█████████▋| 1387/1426 [7:22:33<12:06, 18.63s/it]                                                     {'loss': 0.6567, 'learning_rate': 3.921665428198051e-08, 'epoch': 0.97}
 97%|█████████▋| 1387/1426 [7:22:33<12:06, 18.63s/it] 97%|█████████▋| 1388/1426 [7:22:52<11:58, 18.92s/it]                                                     {'loss': 0.6764, 'learning_rate': 3.723256019661392e-08, 'epoch': 0.97}
 97%|█████████▋| 1388/1426 [7:22:52<11:58, 18.92s/it] 97%|█████████▋| 1389/1426 [7:23:12<11:47, 19.12s/it]                                                     {'loss': 0.6642, 'learning_rate': 3.52998746419031e-08, 'epoch': 0.97}
 97%|█████████▋| 1389/1426 [7:23:12<11:47, 19.12s/it] 97%|█████████▋| 1390/1426 [7:23:32<11:42, 19.50s/it]                                                     {'loss': 0.6785, 'learning_rate': 3.3418607590629536e-08, 'epoch': 0.97}
 97%|█████████▋| 1390/1426 [7:23:33<11:42, 19.50s/it] 98%|█████████▊| 1391/1426 [7:23:52<11:18, 19.38s/it]                                                     {'loss': 0.6676, 'learning_rate': 3.158876875025474e-08, 'epoch': 0.98}
 98%|█████████▊| 1391/1426 [7:23:52<11:18, 19.38s/it] 98%|█████████▊| 1392/1426 [7:24:09<10:37, 18.76s/it]                                                     {'loss': 0.6674, 'learning_rate': 2.9810367562869144e-08, 'epoch': 0.98}
 98%|█████████▊| 1392/1426 [7:24:09<10:37, 18.76s/it] 98%|█████████▊| 1393/1426 [7:24:27<10:11, 18.55s/it]                                                     {'loss': 0.6576, 'learning_rate': 2.8083413205134413e-08, 'epoch': 0.98}
 98%|█████████▊| 1393/1426 [7:24:27<10:11, 18.55s/it] 98%|█████████▊| 1394/1426 [7:24:43<09:29, 17.78s/it]                                                     {'loss': 0.2244, 'learning_rate': 2.6407914588251203e-08, 'epoch': 0.98}
 98%|█████████▊| 1394/1426 [7:24:43<09:29, 17.78s/it] 98%|█████████▊| 1395/1426 [7:25:01<09:14, 17.89s/it]                                                     {'loss': 0.6757, 'learning_rate': 2.478388035790036e-08, 'epoch': 0.98}
 98%|█████████▊| 1395/1426 [7:25:01<09:14, 17.89s/it] 98%|█████████▊| 1396/1426 [7:25:22<09:25, 18.85s/it]                                                     {'loss': 0.6588, 'learning_rate': 2.3211318894205136e-08, 'epoch': 0.98}
 98%|█████████▊| 1396/1426 [7:25:22<09:25, 18.85s/it] 98%|█████████▊| 1397/1426 [7:25:41<09:04, 18.77s/it]                                                     {'loss': 0.668, 'learning_rate': 2.169023831168571e-08, 'epoch': 0.98}
 98%|█████████▊| 1397/1426 [7:25:41<09:04, 18.77s/it] 98%|█████████▊| 1398/1426 [7:25:58<08:32, 18.29s/it]                                                     {'loss': 0.6728, 'learning_rate': 2.0220646459216953e-08, 'epoch': 0.98}
 98%|█████████▊| 1398/1426 [7:25:58<08:32, 18.29s/it] 98%|█████████▊| 1399/1426 [7:26:19<08:33, 19.02s/it]                                                     {'loss': 0.6443, 'learning_rate': 1.8802550919988506e-08, 'epoch': 0.98}
 98%|█████████▊| 1399/1426 [7:26:19<08:33, 19.02s/it] 98%|█████████▊| 1400/1426 [7:26:38<08:14, 19.02s/it]                                                     {'loss': 0.661, 'learning_rate': 1.7435959011465886e-08, 'epoch': 0.98}
 98%|█████████▊| 1400/1426 [7:26:38<08:14, 19.02s/it] 98%|█████████▊| 1401/1426 [7:26:58<08:02, 19.31s/it]                                                     {'loss': 0.6884, 'learning_rate': 1.6120877785352762e-08, 'epoch': 0.98}
 98%|█████████▊| 1401/1426 [7:26:58<08:02, 19.31s/it] 98%|█████████▊| 1402/1426 [7:27:18<07:49, 19.58s/it]                                                     {'loss': 0.6796, 'learning_rate': 1.4857314027554304e-08, 'epoch': 0.98}
 98%|█████████▊| 1402/1426 [7:27:18<07:49, 19.58s/it] 98%|█████████▊| 1403/1426 [7:27:40<07:49, 20.43s/it]                                                     {'loss': 0.6559, 'learning_rate': 1.364527425814166e-08, 'epoch': 0.98}
 98%|█████████▊| 1403/1426 [7:27:40<07:49, 20.43s/it] 98%|█████████▊| 1404/1426 [7:28:00<07:23, 20.17s/it]                                                     {'loss': 0.6491, 'learning_rate': 1.2484764731319765e-08, 'epoch': 0.98}
 98%|█████████▊| 1404/1426 [7:28:00<07:23, 20.17s/it] 99%|█████████▊| 1405/1426 [7:28:17<06:44, 19.28s/it]                                                     {'loss': 0.6561, 'learning_rate': 1.1375791435392913e-08, 'epoch': 0.98}
 99%|█████████▊| 1405/1426 [7:28:17<06:44, 19.28s/it] 99%|█████████▊| 1406/1426 [7:28:36<06:23, 19.20s/it]                                                     {'loss': 0.6754, 'learning_rate': 1.0318360092737013e-08, 'epoch': 0.99}
 99%|█████████▊| 1406/1426 [7:28:36<06:23, 19.20s/it] 99%|█████████▊| 1407/1426 [7:28:57<06:14, 19.73s/it]                                                     {'loss': 0.661, 'learning_rate': 9.31247615976516e-09, 'epoch': 0.99}
 99%|█████████▊| 1407/1426 [7:28:57<06:14, 19.73s/it] 99%|█████████▊| 1408/1426 [7:29:15<05:44, 19.13s/it]                                                     {'loss': 0.6732, 'learning_rate': 8.358144826904335e-09, 'epoch': 0.99}
 99%|█████████▊| 1408/1426 [7:29:15<05:44, 19.13s/it] 99%|█████████▉| 1409/1426 [7:29:38<05:45, 20.32s/it]                                                     {'loss': 0.6395, 'learning_rate': 7.455371018568746e-09, 'epoch': 0.99}
 99%|█████████▉| 1409/1426 [7:29:38<05:45, 20.32s/it] 99%|█████████▉| 1410/1426 [7:29:58<05:22, 20.18s/it]                                                     {'loss': 0.6511, 'learning_rate': 6.604159393127641e-09, 'epoch': 0.99}
 99%|█████████▉| 1410/1426 [7:29:58<05:22, 20.18s/it] 99%|█████████▉| 1411/1426 [7:30:15<04:51, 19.40s/it]                                                     {'loss': 0.6525, 'learning_rate': 5.804514342889755e-09, 'epoch': 0.99}
 99%|█████████▉| 1411/1426 [7:30:15<04:51, 19.40s/it] 99%|█████████▉| 1412/1426 [7:30:34<04:28, 19.15s/it]                                                     {'loss': 0.6793, 'learning_rate': 5.056439994074458e-09, 'epoch': 0.99}
 99%|█████████▉| 1412/1426 [7:30:34<04:28, 19.15s/it] 99%|█████████▉| 1413/1426 [7:30:54<04:11, 19.36s/it]                                                     {'loss': 0.6748, 'learning_rate': 4.3599402067973085e-09, 'epoch': 0.99}
 99%|█████████▉| 1413/1426 [7:30:54<04:11, 19.36s/it] 99%|█████████▉| 1414/1426 [7:31:11<03:46, 18.90s/it]                                                     {'loss': 0.677, 'learning_rate': 3.7150185750389757e-09, 'epoch': 0.99}
 99%|█████████▉| 1414/1426 [7:31:11<03:46, 18.90s/it] 99%|█████████▉| 1415/1426 [7:31:32<03:32, 19.30s/it]                                                     {'loss': 0.6552, 'learning_rate': 3.121678426639685e-09, 'epoch': 0.99}
 99%|█████████▉| 1415/1426 [7:31:32<03:32, 19.30s/it] 99%|█████████▉| 1416/1426 [7:31:53<03:19, 19.98s/it]                                                     {'loss': 0.6632, 'learning_rate': 2.579922823272574e-09, 'epoch': 0.99}
 99%|█████████▉| 1416/1426 [7:31:53<03:19, 19.98s/it] 99%|█████████▉| 1417/1426 [7:32:12<02:57, 19.71s/it]                                                     {'loss': 0.6558, 'learning_rate': 2.0897545604325888e-09, 'epoch': 0.99}
 99%|█████████▉| 1417/1426 [7:32:12<02:57, 19.71s/it] 99%|█████████▉| 1418/1426 [7:32:33<02:40, 20.09s/it]                                                     {'loss': 0.6671, 'learning_rate': 1.651176167418722e-09, 'epoch': 0.99}
 99%|█████████▉| 1418/1426 [7:32:33<02:40, 20.09s/it]100%|█████████▉| 1419/1426 [7:32:53<02:20, 20.01s/it]                                                     {'loss': 0.6785, 'learning_rate': 1.2641899073251307e-09, 'epoch': 0.99}
100%|█████████▉| 1419/1426 [7:32:53<02:20, 20.01s/it]100%|█████████▉| 1420/1426 [7:33:10<01:54, 19.12s/it]                                                     {'loss': 0.6619, 'learning_rate': 9.287977770255918e-10, 'epoch': 1.0}
100%|█████████▉| 1420/1426 [7:33:10<01:54, 19.12s/it]100%|█████████▉| 1421/1426 [7:33:29<01:35, 19.18s/it]                                                     {'loss': 0.6639, 'learning_rate': 6.450015071657323e-10, 'epoch': 1.0}
100%|█████████▉| 1421/1426 [7:33:30<01:35, 19.18s/it]100%|█████████▉| 1422/1426 [7:33:49<01:16, 19.20s/it]                                                     {'loss': 0.6572, 'learning_rate': 4.1280256215192607e-10, 'epoch': 1.0}
100%|█████████▉| 1422/1426 [7:33:49<01:16, 19.20s/it]100%|█████████▉| 1423/1426 [7:34:03<00:52, 17.61s/it]                                                     {'loss': 0.2255, 'learning_rate': 2.322021401479635e-10, 'epoch': 1.0}
100%|█████████▉| 1423/1426 [7:34:03<00:52, 17.61s/it]100%|█████████▉| 1424/1426 [7:34:20<00:34, 17.47s/it]                                                     {'loss': 0.6511, 'learning_rate': 1.0320117306172883e-10, 'epoch': 1.0}
100%|█████████▉| 1424/1426 [7:34:20<00:34, 17.47s/it]100%|█████████▉| 1425/1426 [7:34:41<00:18, 18.55s/it]                                                     {'loss': 0.6848, 'learning_rate': 2.5800326548530708e-11, 'epoch': 1.0}
100%|█████████▉| 1425/1426 [7:34:41<00:18, 18.55s/it]100%|██████████| 1426/1426 [7:35:00<00:00, 18.85s/it]                                                     {'loss': 0.4969, 'learning_rate': 0.0, 'epoch': 1.0}
100%|██████████| 1426/1426 [7:35:00<00:00, 18.85s/it]                                                     {'train_runtime': 27305.4615, 'train_samples_per_second': 35.106, 'train_steps_per_second': 0.052, 'train_loss': 0.6876807338329551, 'epoch': 1.0}
100%|██████████| 1426/1426 [7:35:01<00:00, 18.85s/it]100%|██████████| 1426/1426 [7:35:01<00:00, 19.15s/it]
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.
wandb: 
wandb: Run history:
wandb:                    train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:              train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:            train/learning_rate ▄███████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:                     train/loss █▅▄▄▃▃▄▃▃▃▃▂▃▃▃▃▂▂▁▂▂▂▁▁▂▃▂▂▂▁▁▁▂▁▁▂▂▁▂▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                    train/epoch 1.0
wandb:              train/global_step 1426
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.4969
wandb:               train/total_flos 4.554774480959413e+19
wandb:               train/train_loss 0.68768
wandb:            train/train_runtime 27305.4615
wandb: train/train_samples_per_second 35.106
wandb:   train/train_steps_per_second 0.052
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/test/test08/gzh/LLaVA-UHD/wandb/offline-run-20240901_012153-irbvwe1o
wandb: Find logs at: ./wandb/offline-run-20240901_012153-irbvwe1o/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
