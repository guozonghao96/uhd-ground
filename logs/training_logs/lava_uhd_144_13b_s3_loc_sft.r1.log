nohup: ignoring input
./checkpoints_new/llava-uhd-144-13b-loc-unpad
[2024-08-26 21:50:38,815] torch.distributed.run: [WARNING] 
[2024-08-26 21:50:38,815] torch.distributed.run: [WARNING] *****************************************
[2024-08-26 21:50:38,815] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-08-26 21:50:38,815] torch.distributed.run: [WARNING] *****************************************
[2024-08-26 21:50:42,652] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-26 21:50:42,672] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-26 21:50:42,703] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-26 21:50:42,704] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-26 21:50:42,708] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-26 21:50:42,721] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-26 21:50:42,726] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-26 21:50:42,727] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-26 21:50:44,488] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-26 21:50:44,488] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-26 21:50:44,562] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-26 21:50:44,564] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-26 21:50:44,564] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-08-26 21:50:44,579] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-26 21:50:44,580] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-26 21:50:44,639] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  33%|███▎      | 1/3 [00:24<00:48, 24.41s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:26<00:52, 26.27s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:26<00:53, 26.78s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:27<00:55, 27.85s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:28<00:56, 28.22s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:28<00:56, 28.24s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:28<00:56, 28.30s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:28<00:56, 28.45s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:37<00:17, 17.67s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:41<00:00, 11.29s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:41<00:00, 13.69s/it]
---------init adapt_vision_model---------
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:43<00:20, 20.96s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:44<00:21, 21.69s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:46<00:22, 22.58s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:47<00:00, 13.11s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:47<00:00, 15.76s/it]
---------init adapt_vision_model---------
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:47<00:22, 22.91s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:47<00:22, 22.96s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:47<00:23, 23.00s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:47<00:23, 23.09s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:49<00:00, 14.05s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:49<00:00, 16.62s/it]
---------init adapt_vision_model---------
Loading checkpoint shards: 100%|██████████| 3/3 [00:52<00:00, 14.86s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:52<00:00, 17.47s/it]
---------init adapt_vision_model---------
Loading checkpoint shards: 100%|██████████| 3/3 [00:53<00:00, 15.12s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:53<00:00, 17.77s/it]
---------init adapt_vision_model---------
Loading checkpoint shards: 100%|██████████| 3/3 [00:53<00:00, 15.18s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:53<00:00, 17.82s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:53<00:00, 15.16s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:53<00:00, 17.82s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:53<00:00, 15.19s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:53<00:00, 17.82s/it]
---------init adapt_vision_model---------
---------init adapt_vision_model---------
---------init adapt_vision_model---------
embed_tokens.weight
mm_projector.pos_embed
mm_projector.query
mm_projector.proj
mm_projector.kv_proj.weight
mm_projector.attn.in_proj_weight
mm_projector.attn.in_proj_bias
mm_projector.attn.out_proj.weight
mm_projector.attn.out_proj.bias
mm_projector.ln_q.weight
mm_projector.ln_q.bias
mm_projector.ln_kv.weight
mm_projector.ln_kv.bias
mm_projector.ln_post.weight
mm_projector.ln_post.bias
lm_head.weight
embed_tokens.weight
mm_projector.pos_embed
mm_projector.query
mm_projector.proj
mm_projector.kv_proj.weight
mm_projector.attn.in_proj_weight
mm_projector.attn.in_proj_bias
mm_projector.attn.out_proj.weight
mm_projector.attn.out_proj.bias
mm_projector.ln_q.weight
mm_projector.ln_q.bias
mm_projector.ln_kv.weight
mm_projector.ln_kv.bias
mm_projector.ln_post.weight
mm_projector.ln_post.bias
lm_head.weight
embed_tokens.weight
mm_projector.pos_embed
mm_projector.query
mm_projector.proj
mm_projector.kv_proj.weight
mm_projector.attn.in_proj_weight
mm_projector.attn.in_proj_bias
mm_projector.attn.out_proj.weight
mm_projector.attn.out_proj.bias
mm_projector.ln_q.weight
mm_projector.ln_q.bias
mm_projector.ln_kv.weight
mm_projector.ln_kv.bias
mm_projector.ln_post.weight
mm_projector.ln_post.bias
lm_head.weight
Formatting inputs...Skip in lazy mode
embed_tokens.weight
mm_projector.pos_embed
mm_projector.query
mm_projector.proj
mm_projector.kv_proj.weight
mm_projector.attn.in_proj_weight
mm_projector.attn.in_proj_bias
mm_projector.attn.out_proj.weight
mm_projector.attn.out_proj.bias
mm_projector.ln_q.weight
mm_projector.ln_q.bias
mm_projector.ln_kv.weight
mm_projector.ln_kv.bias
mm_projector.ln_post.weight
mm_projector.ln_post.bias
lm_head.weight
embed_tokens.weight
mm_projector.pos_embed
mm_projector.query
mm_projector.proj
mm_projector.kv_proj.weight
mm_projector.attn.in_proj_weight
mm_projector.attn.in_proj_bias
mm_projector.attn.out_proj.weight
mm_projector.attn.out_proj.bias
mm_projector.ln_q.weight
mm_projector.ln_q.bias
mm_projector.ln_kv.weight
mm_projector.ln_kv.bias
mm_projector.ln_post.weight
mm_projector.ln_post.bias
lm_head.weight
embed_tokens.weight
mm_projector.pos_embed
mm_projector.query
mm_projector.proj
mm_projector.kv_proj.weight
mm_projector.attn.in_proj_weight
mm_projector.attn.in_proj_bias
mm_projector.attn.out_proj.weight
mm_projector.attn.out_proj.bias
mm_projector.ln_q.weight
mm_projector.ln_q.bias
mm_projector.ln_kv.weight
mm_projector.ln_kv.bias
mm_projector.ln_post.weight
mm_projector.ln_post.bias
lm_head.weight
embed_tokens.weight
mm_projector.pos_embed
mm_projector.query
mm_projector.proj
mm_projector.kv_proj.weight
mm_projector.attn.in_proj_weight
mm_projector.attn.in_proj_bias
mm_projector.attn.out_proj.weight
mm_projector.attn.out_proj.bias
mm_projector.ln_q.weight
mm_projector.ln_q.bias
mm_projector.ln_kv.weight
mm_projector.ln_kv.bias
mm_projector.ln_post.weight
mm_projector.ln_post.bias
lm_head.weight
embed_tokens.weight
mm_projector.pos_embed
mm_projector.query
mm_projector.proj
mm_projector.kv_proj.weight
mm_projector.attn.in_proj_weight
mm_projector.attn.in_proj_bias
mm_projector.attn.out_proj.weight
mm_projector.attn.out_proj.bias
mm_projector.ln_q.weight
mm_projector.ln_q.bias
mm_projector.ln_kv.weight
mm_projector.ln_kv.bias
mm_projector.ln_post.weight
mm_projector.ln_post.bias
lm_head.weight
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
this iter is wrong in something... skip...
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
this iter is wrong in something... skip...
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/test/test08/anaconda3/envs/llava_uhd/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
WARNING: tokenization mismatch: 1 vs. 789. (ignored)
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
WARNING: tokenization mismatch: 1 vs. 64. (ignored)
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
WARNING: tokenization mismatch: 1 vs. 1590. (ignored)
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
WARNING: tokenization mismatch: 1 vs. 1440. (ignored)
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
this iter is wrong in something... skip...
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
